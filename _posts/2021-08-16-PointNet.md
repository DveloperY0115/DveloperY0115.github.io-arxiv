---
layout: post
title: "Summary of 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation'"
subtitle: "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (CVPR 2018)"
background: '/assets/post-images/PointNet/fig1.png'
---

# Motivations

- Point cloud is an important type of geometric data structure which can easily be encountered in real-world problems. However, these data have been transformed to voluminous 3D voxel grids or collections of images due to its irregular format and such converted data came with some issues.

# Key Contributions

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig1.png">
</center>
<span class="caption text-muted">Figure 1. <b>Applications of PointNet</b>.</span>

- Designs a novel deep neural network architecture called *PointNet* suitable for consuming unordered point sets in 3D.
- Demonstrates how such architecture can be trained to perform 3D shape classification, shape part segmentation and scene semantic parsing tasks.
- Provides thorough empirical and theoretical analysis on the stability and efficiency of the proposed method.
- Visualizes the 3D features computed by the selected layers in the network and develops intuitive explanations for its performance.

---

# TL;DR

This work presents PointNet along with the idea for extracting useful features from unordered set of 3D points. This novel architecture is capable of various tasks such as 3D shape classification, shape part segmentation and scene semantic parsing tasks, and achieves both best performance and efficiency when compared to previous methods. Furthermore, it provides detailed justification on the internal behavior of PointNet using visualizations of its layers helping us to understand the reason behind its performance intuitively. 

# Methods

## Problem Statement

The primary goal of this work is to design a neural network which can directly consume unordered point sets as inputs.

A point cloud is represented as a set of 3D points $\\{ P_{i} \vert i = 1, \dots, n \\}$, where each point $P_{i}$ is a vector of its $(x, y, z)$ coordinate and possibly with extra feature channels such as color, normal etc.

For the object classification task, the input point cloud is either directly sampled from a shape or pre-segmented from a scene point cloud. Given such point cloud as input, PointNet outputs $k$ scores for all the $k$ candidate classes.

For semantic segmentation, the input can be a single object for part region segmentation, or a sub-volume from a 3D scene for object region segmentation. Then PointNet will output $n \times m$ scores for each of the $n$ points and each of the $m$ semantic subcategories.

## Deep Learning on Point Sets

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig2.png">
</center>
<span class="caption text-muted">Figure 2. <b>Qualitative results for part segmentation</b>.</span>

The structure of PointNet is heavily influenced by the properties of point sets in $\mathbb{R}^{n}$.

### Properties of Point Sets in $\mathbb{R}^{n}$

A point cloud is in fact a subset of points from an Euclidean space $\mathbb{R}^{3}$. And it has three notable properties:

1. **Unordered.** Unlike pixel arrays in images or voxel arrays in volumetric grids, point cloud is just a collection of points without specific order. That is, a network which takes point clouds as input must be invariant to all possible permutations of them (e.g. if a point cloud contains $N$ number of points, then the network must work consistently across its $N!$ permutations).
2. **Interaction among points.** The points are from a space with a distance metric. This implies that points are not isolated, and in fact neighboring points form a meaningful subset. Thus, the model should be able to capture local structures from points inside some local region.
3. **Invariance under transformations.** Since the point clouds of our concern are geometric representations of objects, the learned representation of point sets should be invariant to certain transformations. For example, rotating or translating a point cloud should not alter its predicted category. 

### PointNet Architecture

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig3.png">
</center>
<span class="caption text-muted">Figure 3. <b>PointNet Architecture</b>.</span>

The entire PointNet architecture is illustrated above. Note that the classification network and the segmentation network share a great portion of structures.

The network has three key modules:

1. The max pooling layer as a symmetric function to aggregate information from all points in the input point cloud.
2. A local and global information combination structure.
3. Two joint alignment networks (denoted "T-Net"s) that align both input points and point features.

The intention behind these structure will be discussed in the following sections.

< **Symmetry Function for Unordered Input >**

There are three possible strategies to make a model invariant to input permutation:

1. Sort input into a canonical order
2. Treat the input as asequence to train an RNN, but augment the training data by all kinds of permutations 
3. **Use a simple symmetric function to aggregate the information from each point**

Here, a symmetric function takes $n$ vectors as input and outputs a new vector which is invariant to the input order. + and * operators are examples of such function.

While the idea of using sorting sounds simple, but in fact there does not exist a stable ordering in high dimensional space. Furthermore, one **might hope RNN to be invariant to input order when trained with randomly permuted sequence, but a study shows that it's still hard to neglect the effect of different ordering. Moreover, RNN is hard to scale to thousands of input elements.

Therefore, it turns out that the reasonable way to cope with unordered set of points is to use a symmetric function.

The idea of the paper is to approximate a general function defined on a point set by applying a symmetric function on transformed elements in the set:

$$f(\{ x_1, \dots, x_n\}) \approx g(h(x_1), \dots, h(x_n)),$$

where $f: 2^{\mathbb{R}^{N}} \rightarrow \mathbb{R}$, $h: \mathbb{R}^{N} \rightarrow \mathbb{R}^{K}$ and $g: \mathbb{R}^{K} \times \cdots \times \mathbb{R}^{K} \rightarrow \mathbb{R}$ is a symmetric funciton. In practice, the basic module of PointNet is very simple. The function $h$ is approximated by a multi-layer perceptron and $g$ is approximated by a composition of a single variable function and a max pooling. Apart from the simplexity of module, it has interesting properties and achieves high performance in a few different applications.

< **Local and Global Information Aggregation >**

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig4.png">
</center>
<span class="caption text-muted">Figure 4. <b>Qualitative results for semantic segmentation</b>.</span>

The output from the above section has a form of a vector $[f_{1}, \dots, f_{K}]$, which is a global signature of the input set. One can simply construct a SVM or multi-layer perceptron classifier using such global features representing shape for classification. However, compared with shape classification, point segmentation requires not only global features but also local features that vary over different regions in the given shape (or scene). This issue can be easily addressed by simply concatenating the obtained global features to local features. After that, one can extract new per point features based on the "combined" point features which now contain both global and local information.

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig5.png">
</center>
<span class="caption text-muted">Figure 5. <b>Network architecture for part segmentation</b>.</span>

With this modification, PointNet is now able to predict per point quantities that depend on both local geometry and global semantics. For example, a experimental result shows that the network can carry out accurate normal prediction which requires understanding of a point's local neighborhood.

**< Joint Alignment Network >**

It's trivial that the semantic labeling of a point cloud must be invariant if the point cloud is transformed by certain geometric transformations - rotation, translation, etc. Thus, it is desirable that the learnt representation of the point set is invariant to such transformations.

One straightforward solution for this is to align all input set to a canonical space before feature extraction. Then our question would be: "How can we find an appropriate transformation which will bring a point cloud of interest to such well-aligned, canonical space?"

The answer is simple - predict an affine transformation matrix using a mini, PointNet-like network and apply this predicted transformation to the coordinates of input points right away.

This idea - aligning the points in Euclidean space - can be extended to the alignment of feature space as well. Thus, we append additional alignment network on point features and predict a feature transformation matrix to align features from different input point clouds. However, due to the high dimensionality of feature space, finding the optimal transformation matrix is difficult. Therefore, we constrain the feature transformation matrix to be close to orthogonal matrix:

$$L_{\text{reg}} = \Vert I - AA^{T} \Vert^{2}_{F},$$

where $A$ is the feature alignment matrix predicted by T-Net. Thanks to its property of having the determinant of 1, applying such transformation will not cause any information loss.

### Theoretical Analysis

**< Universal approximation >**

PointNet can well approximate continuous set functions. By the continuity of set functions, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores.

Formally, let $\mathcal{X} = \{ S: S \subseteq [0, 1]^{m} \,\, \text{and} \,\, \vert S \vert = n\}$, $f: \mathcal{X} \rightarrow \mathbb{R}$ be a continuous set function on $\mathcal{X}$ w.r.t to Hausdorff distance $d_{H}(\cdot, \cdot)$, i.e., $\forall \epsilon > 0$, $\exists \delta > 0$, for any $S, S^{\prime} \in \mathcal{X}$, if $d_{H}(S, S^{\prime}) < \delta$, then $\vert f(S) - f(S^{\prime}) \vert < \epsilon$. Then the proposed theorem states that $f$ can be arbitrarily approximated by the neural network given enough neurons at the max pooling layer.

---

**Theorem 1.** *Suppose $f: \mathcal{X} \rightarrow \mathbb{R}$  is a continuous set function w.r.t Hausdorff distance $d_{H}(\cdot, \cdot)$. That is, $\forall \epsilon > 0$ , $\exists$ a continuous function $h$ and a symmetric function* $g(x_1, \dots, x_n) = \gamma \circ \text{MAX}$, *such that for any* $S \in \mathcal{X}$,

$$ \Big\vert \, f(S) - \gamma \Big(  \underset{x_{i} \in S}{\text{MAX}} \{ h(x_{i})\} \Big) \Big\vert < \epsilon$$

*where $x_{1}, \dots, x_{n}$ is the full list of elements in $S$ ordered arbitrarily, $\gamma$ is a continuous function, and $\text{MAX}$ is a vector max operator that takes $n$ vectors as input and returns a new vector of the element-wise maximum.*

---

**< Bottleneck dimension and stability >**

Both theoretically and experimentally, the authors discovered that the expressiveness of PointNet is greatly affected by the dimension of the max pooling layer. Let's define $\textbf{u} = \underset{x_{i} \in S}{\text{MAX}} \\{ h(x_{i})\\}$ to be the sub-network of $f$ which maps a point set in $[0, 1]^{m}$ to a $K$-dimensional vector. Then the following theorem says that small corruptions or extra noise points in the input set are not likely to alter the output of PointNet.

---

**Theorem 2.** *Suppose $\textbf{u} : \mathcal{X} \rightarrow \mathbb{R}^{K}$ such that $\textbf{u} = \underset{x_{i} \in S}{\text{MAX}} \\{ h(x_{i}) \\}$ and $f = \gamma \circ \textbf{u}$. Then,*

1. $\forall S$,  $\exists \,\mathcal{C}\_{S}$,  $\mathcal{N}\_{S} \subseteq \mathcal{X}$,  $f(T) = f(S)$  if  $\mathcal{C}\_{S} \subseteq T \subseteq \mathcal{N}\_{S}$
2. $\vert \mathcal{C}\_{S} \vert \leq K$

---

The first says that $f(S)$ is unchanged up to the input corruption if all points in $\mathcal{C}\_{S}$ are preserved. Also, it is also unchanged with extra noise added to the input up to $\mathcal{N}\_{S}$ (i.e. less or additional number of points do not change the function value).

The second tells us that $\mathcal{C}\_{S}$ contains only a bounded number of points, determined by $K$ which is the dimensionality of the output of the function $h : \mathbb{R}^{N} \rightarrow \mathbb{R}^{K}$. That is, $f(S)$ **is in fact totally determined by a finite subset $\mathcal{C}\_{S} \subseteq S$ of less or equal to $K$ elements.** Thus, we call $\mathcal{C}\_{S} \subseteq S$ the *critical point set* of $S$ and $K$ the *bottleneck dimension* of $f$.

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig6.png">
</center>
<span class="caption text-muted">Figure 6. <b>Critical points and upper bound shape</b>.</span>

Conditioned on the continuity of $h$, this theorem explains the robustness of PointNet w.r.t point perturbation, corruption and extra noise points. This can be summarized as the following intuition:

> **"PointNet learns to summarize a shape by a sparse set of key points."**

# Conclusion

This paper proposes a novel deep neural network *PointNet* which directly consumes unordered set of points, aggregates both local and global features. *PointNet* paves the way of utilizing unstructured point cloud data by providing a unified approach to a number of 3D recognition tasks including object classification, part segmentation and semantic segmentation. The experimental results show that *PointNet* achieves on par or better performance than state of the arts on standard benchmarks.
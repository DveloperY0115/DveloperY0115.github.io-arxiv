---
layout: post
title: "Summary of 'GANSpace: Discovering Interpretable GAN Controls'"
subtitle: "GANSpace: Discovering Interpretable GAN Controls (NeurIPS 2020)"
background: '/assets/post-images/GANSpace/fig6.png'
---

# Motivations

- While generative adversarial networks (GANs) such as BigGAN and StyleGAN are powerful image synthesis models that are able to generate a wide variety of high-quality images, they provide little direct control over image contents.
- Current approaches suggested for allowing more control over GANs mostly focus on supervised learning of latent directions or training GAN with labeled images both require tremendous, expensive manual supervision for each new control to be learned.

# Key Contributions

- This paper shows the way how to identify new interpretable control directions for existing GANs without requiring supervision or expensive optimization.
- This paper show that semantically meaningful directions in GAN latent spaces can be discovered by applying Principal Component Analysis (PCA) in latent space for StyleGAN, and feature space for BigGAN.
- Additionally, the authors show how BigGAN can be modified to allow StyleGAN-like layer-wise style mixing and control without retraining.
- Last but not least, the authors show that layer-wise decomposition of PCA edit directions leads to many interpretable controls. Note that they do not explicitly target an attribute and the direction in latent space associated with its modification. Rather, it's up to the user to identify useful control directions and annotate them properly.

---

# TL;DR

This paper suggests to use PCA for the task of analyzing & exploring the latent spaces of widely used GAN models such as StyleGAN, StyleGAN2, and BigGAN. The authors show that the principal directions obtained from PCA are related to different attributes of generated images, and further can be used for image editing.

# Methods

<img class="img-fluid" src="/assets/post-images/GANSpace/fig1.png">
<span class="caption text-muted">Figure 1. <b>Sequences of image edits performed using control discovered with the proposed method</b>.</span>

## Discovering GAN Controls

This paper suggests new techniques for augmenting existing GANs with new control variables. These techniques are very simple, thus possesses an advantage over previous methods: these methods enable a range of powerful tools for analysis and control of GANs with very little effort.

ðŸ’¡ **IMPORTANT NOTE: In this paper, the authors worked exclusively with pretrained GANs.**

### Background

Let us briefly review GAN representation first. The most basic GAN works on a probability distribution $p(\textbf{z})$, from where a latent vector $\textbf{z}$ is sampled. And importantly, there is a neural network $G(\textbf{z})$ which maps such latent vector to an output image $I: \textbf{z} \sim p(\textbf{z}), I = G(\textbf{z})$. Furthermore, the network can be decomposed into a series of $L$ intermediate layers $G_{1}, \dots, G_{L}$, where the first layer $G_{1}$ takes a latent vector as an input and the each of following layers $G_{i}$ take an intermediate feature from the previous layer $G_{i-1}$ and outputs another intermediate feature for its follower. Note that the output of the last layer $G_{L}$ is an RGB image.

In the case of BigGAN model, the intermediate layers also take the latent vector as input:

$$\textbf{y}\_{i} = G_{i} (\textbf{y}\_{i-1}, \textbf{z}),$$

which are called Skip-$z$ inputs.

In a StyleGAN model, however, the first layer takes a constant feature map input $\textbf{y}_{0}$. And then, the output is controlled by a non-linear function of $\textbf{z}$ as input to intermediate layers:

$$\textbf{y}\_{i} = G(\textbf{y}\_{i-1}, \textbf{w}) \quad \text{with } \textbf{w} = M(\textbf{z}),$$

where $M$ is an 8-layer multi-layer perceptron. StyleGAN applies different (learned) affine transformations to $\textbf{w}$ at each layer $G_{i}$, turning it into a style code $\textbf{w}_{i}$, which varies over different layers of the synthesis network $G$. It is empirically shown that each of style codes affects different semantic element(s) of generated images.

### Principal Components and Principal Feature Directions

<img class="img-fluid" src="/assets/post-images/GANSpace/fig2.png">
<span class="caption text-muted">Figure 2. <b>2D illustration of identifying a principal activation direction for BigGAN</b>.</span>

... And we return to the fundamental question:

> "How can we find useful directions in $\textbf{z}$ space then?"

In fact, the isotropic prior distribution $p(\textbf{z})$ provides almost zero information about directions associated with meaningful modifications on images. What makes it even worse is that the distribution of outputs in the high-dimensional pixel space is extremely complex, thus difficult to reason about. The main observation is that **the principal components of feature tensors on the early layers of GANs represent important factors of variation**.

---

**StyleGAN**

Analysis of the latent space of StyleGAN is the simpliest example we can start with (actually there are only two GANs discussed here). In the context of StyleGAN, the goal is now finding the principal axes of $p(\text{w})$. To this end, the authors sampled $N$ random vectors $\textbf{z}\_{1:N}$, and computed the corresponding $\textbf{w}\_{i} = M(\textbf{z}\_{i})$ values. Then they carried out PCA with these $\textbf{w}\_{1:N}$ values. The result of PCA is a basis $\textbf{V}$ for $\mathcal{W}$. More generally, given a new image decoded from $\textbf{w}$, one can edit it by varying PCA coordinates $\textbf{x}$ before feeding it to the synthesis network:

$$\textbf{w}^{\prime} = \textbf{w} + \textbf{V} \textbf{x},$$

where each entry $x_{k}$ of $\textbf{x}$ is a separate control parameter. The entries $x_{k}$ are initialized to zero until modified by a user.

---

**BigGAN**

When it comes to BigGAN, the procedure becomes more complicated, since the $\textbf{z}$ distribution is not learned, and there is no $\textbf{w}$ latent that parameterizes the output image. The authors instead performed PCA at an intermediate network layer $i$, and then transferred these directions back to the $\textbf{z}$ latent space, as follows:

Specifically, the authors first sampled $N$ random latent vectors $\textbf{z}\_{1:N}$. These are processed through the model to produce $N$ feature tensors $\textbf{y}\_{1:N}$ at the $i$-th layer, where $\textbf{y}\_{j} = \hat{G}\_{i}(\textbf{z}\_{j})$. Then the authors performed PCA on the $N$ feature tensors $\textbf{y}\_{1:N}$ resulting in a low-rank basis matrix $\textbf{V}$, and the data mean $\boldsymbol{\mu}$. The PCA coordinates $\textbf{x}\_{j}$ of each feature tensor are then computed by projection: $\textbf{x}\_{j} = \textbf{V}^{T} (\textbf{y}\_{j} - \boldsymbol{\mu})$.

This basis is then transferred to the latent space by linear regression, which is done as follows:

The authors started with an individual basis vector $\textbf{v}\_{k}$ (i.e., a column of $\textbf{V}$), and the corresponding PCA coordinates $x\_{1:N}^{k}$, where $x\_{j}^{k}$ denotes the $k$-th element of $\textbf{x}\_{j}$. Finally, the corresponding latent basis vector $\textbf{u}\_{k}$ is computed as:

$$\textbf{u}\_{k} = \text{argmin} \sum\_{j} \Vert \textbf{u}\_{k} x\_{j}^{k} - \textbf{z}\_{j} \Vert^{2}$$

and this basis vector is a latent direction corresponding to this principal component. That being said, the entire basis can be computed at once with:

$$\textbf{U} = \text{argmin} \sum\_{j} \Vert \textbf{U} \textbf{x}\_{j} - \textbf{z}\_{j} \Vert^{2}$$

computing $\textbf{U}$ can be done using a standard least-squares solver, without any additional orthogonality constraints. Each column of $\textbf{U}$ aligns to the variation along the corresponding column of $\textbf{V}$. The columns $\textbf{u}_{k}$'s are called *principal directions*. Knowing principal directions, one can edit images using similar (actually, almost the same) method as in StyleGAN:

$$\textbf{z}^{\prime} = \textbf{z} + \textbf{U} \textbf{x},$$

where $x_{k}$, $k$-th element of $\textbf{x}$, specifies the offset along the column $\textbf{u}_{k}$ of the principal dimension matrix.

---

### Layer-wise Edits

Given the directions found with PCA, this paper shows that these can be further decomposed into interpretable edits by applying them only to certain layers.

---

**StyleGAN**

<img class="img-fluid" src="/assets/post-images/GANSpace/fig3.png">
<span class="caption text-muted">Figure 3. <b>(Rows 1-3) illustrate the three largest principal components in the intermediate latent space of StyleGAN2. (Rows 4-5) demonstrate the effect of constraining the variation to a subset of the layers</b>.</span>


As its name implies, StyleGAN allows us to control different *"styles"* of the image by manipulating the $\textbf{w}\_{i}$'s, intermediate latent vectors fed to each layer of the synthesis network. Given an image with latent vector $\textbf{w}$, layerwise edits can be done by modifying only the $\textbf{w}$ inputs to a range of layers, leaving the other layers' inputs fixed. In this paper, the notation $\text{E}(\textbf{v}\_{i}, \text{j}-\text{k})$ was used to denote edit directions. For example, $\text{E}(\textbf{v}\_{1}, 0-3)$ means moving along the direction of (PCA) component $\textbf{v}\_{1}$ at the first four layers only. $\text{E}(\textbf{v}\_{2}, \text{all})$ means moving along the direction of component $\textbf{v}\_{2}$ at all layers. Meanwhile, edits in the $\mathcal{Z}$ latent space are denoted $\text{E} (\textbf{u}\_{i}, \text{j-k})$. The effects of such modifications are illustrated in the figure 3.

---

**BigGAN**

<img class="img-fluid" src="/assets/post-images/GANSpace/fig4.png">
<span class="caption text-muted">Figure 4. <b>Style variation in BigGAN</b>. Experiments reveal the fact that modifications of the latent vectors in the middle of the network affect the style of the generated image in semantically meaningful ways.</span>

In contrast to StyleGAN, BigGAN does not have any layer-wise control mechanism. However, the authors discovered that **BigGAN can be modified to produce behavior similar to StyleGAN, by varying the intermediate Skip-$z$ inputs $\textbf{z}\_{i}$ separately from the latent** $\textbf{z}$: $G(\textbf{y}\_{i-1}, \textbf{z}\_{i})$. In this setting, the latent inputs $\textbf{z}\_{i}$ can vary individually across different layers of BigGAN, similar to style mixing of StyleGAN. In the beginning, all inputs are determined by an initially sampled or estimated $\textbf{z}$, but those can be edited independently yielding different effects to generated images. Surprisingly, while BigGAN is not trained with style mixing regularization, it models images in a form of style & content hierarchy.

Similar to StyleGAN, changing the latent vectors at lower layers is related to lower-level style edits. However, the latent space of BigGAN is more entangled than that of StyleGAN due to the difference between their structures and training schemes. Note that the Skip-$z$ connections in BigGAN were not originally intended for style resampling, thus such entanglement is inevitable. Still, there is no doubt that they are useful for layer-wise editing.

---

## Findings and Results

This paper presents a number of discoveries from the PCA analysis, as well as comparisons to the baseline. The authors show edits discovered on state-of-the-art pretrained GANs, including:

- BigGAN512-deep
- StyleGAN (Bedrooms, Landscapes, WikiArt training sets)
- StyleGAN2 (FFHQ, Cars, Cats, Church, Horse training sets)

The analysis on these renowned generative model reveals interesting properties of StyleGAN and BigGAN models.

### GAN and PCA Properties

One important observation obtained from all trained models is that:

> **"Large-scale changes to geometric configuration and viewpoint are limited to the first 20 principal components ($\textbf{v}\_{0} - \textbf{v}\_{20}$). Modifying the rest of them leaves layout unchanged, and instead controls object appearance & background and other details."**

Another interesting property discovered from PCA is that **StyleGAN2's latent distribution $p(\textbf{w})$ has a relatively simple structure.** Specifically, the principal coordinates are nearly independent variables with non-Gaussian unimodal distributions. The authors also found that **the first 100 principal components are sufficient to describe overall image appearance,** and other 412 dimensions control subtle, but noticable changes in appearance.

Turning our attention to BigGAN, the authors found that BigGAN components appear to be class-independent (e.g., PCA components for one class were identical to PCA components for another class in the cases tested by the authors). That is, an editing direction found in one class can easily be transferred to other classes.

### Model entanglements and disallowed combinations

<img class="img-fluid" src="/assets/post-images/GANSpace/fig5.png">
<span class="caption text-muted">Figure 5. <b>Illustration of the significance of the principal components as compared to random directions in the intermediate latent space of StyleGAN2</b>.</span>

**It is suspected that some properties of GAN principal components were inherited from GANs' training sets**. These properties may be desirable in some cases, or prevent the complete disentanglement we all desire. Some of these can also be thought as undesirable biases of the trained GAN. For example, in the case of StyleGAN2 trained on the FFHQ dataset, geometric changes are limited to rotations in the first 3 components. One possible reason for this behavior is the carefully aligned training set, since none of the modifications affected the translation at all.

In the models on human faces, the authors observed "disallowed combinations", the attributes that the model will not apply to certain faces. For example, the "wrinkles" edit will age and add wrinkles to adult faces, while having no significant effect on a child's face. Similarily, "makeup and lipstick" edits add or remove makeup to female faces, but have little or no effect on male faces. This is another evidence that editability of GANs are inherently limited by the dataset they were trained on.

# Conclusion

<img class="img-fluid" src="/assets/post-images/GANSpace/fig6.png">
<span class="caption text-muted">Figure 6. <b>A selection of interpretable edits discovered by selective application of latent edits across the layers of several pretrained GAN models</b>.</span>

This paper shows simple but powerful ways to modify images using pretrained GANs. Instead of training a new model which is computationally expensive and time consuming, the authors investigated existing general-purpose image representations and discovered techniques for controlling them. This work presents a new way of analyzing image representations, discovering sophiscated control techniques within GAN spaces by leveraging PCA in unsupervised manner.
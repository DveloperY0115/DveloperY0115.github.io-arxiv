---
layout: post
title: "Summary of 'NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections'"
background: '/assets/post-images/NeRFW/NeRFW_fig1.png'
---

![](https://www.youtube.com/watch?v=mRAKVQj5LRA)
<span class="caption text-muted">Video 1. <b>Video from authors which briefly introduces key ideas, methods, and experimental results</b>.</span>

# Motivation

- NeRF works well on images of static subjects captured under *controlled* settings â†’ Incapable of modeling real-world photos in *uncontrolled* settings (e.g. images under varying illumination or transient occluders).
- Specifically, NeRF assumes that the scene is *static*, meaning that the scene is captured within a short time, thus all contents in the scene - including lighting - are fixed. â†’ NeRF's performance degrades significantly as scene elements vary
- We need **NeRF which can be directly applied on large-scale real world scenarios** where the input images may be taken hours or years apart, and contain moving elements - pedestrians and vehicles.

# Key Contributions

- Introduces NeRF-W, an extension of NeRF that relaxes its strict consistency assumptions. The relaxation is done by two novel ideas: (1) modeling per-image appearance variations in a learned low-dimensional latent space, (2) modeling the scene as the union of *shared* and *image-dependent* elements. For instance, if there are multiple photos of Washington Monument, the monument itself appears consistently across all exemplars while people in front of it may vary in different photos.
- The first idea enables **smooth transition of scene elements, thereby allowing us to re-render the scene**. Lighting, time of day, etc, changes in a plausible way as one explores the learned latent space.
- The result of second idea is a **novel, secondary volumetric radiance field combined with a data-dependent uncertainty field, where the latter captures variable observation noise and reduces the effect of transient objects** on the static scene representation. Since the network learns to distinguish static & transient components by itself, we can obtain only the static scene representation (without noises due to moving objects) after training.
- **NeRF-W successfully produce detailed, high-fidelity renderings from novel viewpoints even in challenging in-the-wild photo collections**, outperforming previous state-of-the-art both in qualitative and quantitative (PSNR, MS-SSIM) comparisons, while enabling smooth appearance interpolation, keeping temporal consistency in different viewpoints.

---

# TL;DR

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig1.png">
<span class="caption text-muted">Figure 1. <b>NeRF-W synthesizing images from novel viewpoints, given only an unconstrained, in-the-wild internet photo collection</b>. Flickr users dbowie78, vasnic64, punch / CC BY.</span>

NeRF-W learns to extract per-image features and to disentangle transient scene elements from the static elements. By doing so, it can generate photorealistic, yet temporally consistent images from novel viewpoints unlike its predecessors.

# Background

For fundamental ideas and concepts of neural rendering, especially the one involving neural radiance fields and volumetric rendering, please refer to my [post](https://dvelopery0115.github.io/2021/06/27/NeRF.html) or the [paper](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjn86SE2tXxAhXEA94KHclBCYgQFjAAegQIAxAD&url=https%3A%2F%2Farxiv.org%2Fabs%2F2003.08934&usg=AOvVaw2VydXvj4xqoYNkoedgHRag) itself.

# NeRF in the Wild

NeRF-W is built on NeRF with two enhancements that explicitly designed to handle the challenges of unconstrained imagery. Still, the problem setting remains almost identical.

Just like NeRF, NeRF-W aims to learn a volumetric density representation $F_{\theta}$ from an unstructured photo collection $ \\{ \mathcal{I}\_{i} \\}_{i=1}^{N} $ for which camera parameters are known. However, one critical difference is that **NeRF assumes consistency (both in spatial and temporal) in its input views, which can easily broken in internet photos** taken from dramatically different viewpoints, in different time of day or even seasons, inevitably including non-static, transient components (pedestrians, objects, etc) as shown in the figure below.

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig2.png">
<span class="caption text-muted">Figure 2. <b>Examples of in-the-wild photographs from the Phototourism dataset</b>. Photos by Flickr users paradasos, itia4u, jblesa, joshheumann, ojotes, chyauchentravelworld / CC BY.</span>

More concretely, factors that alienates the direct application of NeRF to real world photographs can be summarized as follows:

1. **Photometric variation**: For photos taken outdoor, time of day and atmospheric conditions directly impact the illumination (and consequently, the emitted radiance) of objects in the scene. Furthermore, not only the semantic components, but also different imaging pipelines, including varying exposure settings, white balance, and tone-mapping applied differently by either cameras or photographers (or both) may give rise to additional inconsistencies.
2. **Transient objects**: Real-world landmarks in photos are often occluded by moving objects or occluders around them (e.g. tourists). Especially the photos taken & uploaded by tourists are challenging, since they often contain posing human subjects and other pedestrians.

To address these issues when dealing with in-the-wild photographs, the authors built additional components on top of NeRF. One is used to capture image-dependent appearance and illumination variations, while the other is designed to recognize and disentangle transient objects from static objects (e.g. landmark, buildings) in the scene.

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig3.png">
<span class="caption text-muted">Figure 3. <b>NeRF-W model architecture</b>. Given a 3D position, viewing direction, and learned appearance and transient embeddings, NeRF-W produces static and transient colors and densities as well as a measure of uncertainty. Note that the static opacity is inferred before the model is conditioned on the appearance embedding. This ensures that static geometry is shared independently across all images.</span>

## Latent Appearance Modeling

To make NeRF be aware of variable lighting and photometric post-processing, the authors adopted the approach of Generative Latent Optimization (GLO) where each image $\mathcal{I}\_{i}$ is assigned a corresponding real-valued appearance embedding vector $\ell_{i}^{(a)}$ of length $n^{(a)}$. And the image-*independent* radiance $\textbf{c}(t)$ in the original NeRF has been replaced by an image-*dependent* radiance $\textbf{c}\_{i}(t)$, which then implies that pixel colors $\hat{C}\_{i}$ of $i$-th image $\mathcal{I}\_{i}$ are also have per-image dependency:

$$ \begin{gather} \hat{C}_{i}(\textbf{r}) = \mathcal{R} (\textbf{r}, \textbf{c}_{i}, \sigma), \\
\textbf{c}_{i}(t) = \text{MLP}_{\theta_{2}} (\textbf{z}(t), \gamma_{\textbf{d}} (\textbf{d}), \ell_{i}^{(a)}) \end{gather} $$

where $\mathcal{R}$ is a volumetric rendering function for computing pixel values by making use of given ray, radiance values sampled along each ray, as well as densities sampled in the same way, and $ \\{ \ell\_{i}^{(a)} \\} \_{i=1}^{N} $ are embeddings that are optimized alongside $\theta$.

One thing you might notice here is that the authors regarded the original NeRF architecture as a compound of two different networks, instead of seeing it as a whole:

$$ \begin{gather} [\sigma(t), \textbf{z}(t)] = \text{MLP}_{\theta_{1}} (\gamma_{\textbf{x}} (\textbf{r}(t))), \\
\textbf{c}(t) = \text{MLP}_{\theta_{2}} (\textbf{z}(t), \gamma_{\textbf{d}}(\textbf{d})), \end{gather} $$

with parameters $\theta = [\theta_{1}, \theta_{2}]$ and fixed (i.e. deterministic) encoding functions $\gamma_{\textbf{x}}$ (for position) and $\gamma_{\textbf{d}}$ (for viewing direction). What this notation emphasizes is that **the volume density $\sigma$  has nothing to do with viewing direction** $\textbf{d}$.

By providing appearance embeddings as input to only the subnetwork of the system that outputs color, we can ensure that our model now has additional degree of freedom to vary the radiance for different *images* while sharing the same inferred 3D geometry across *the scene*, which was predicted earlier by another subnetwork $\text{MLP}\_{\theta_{1}}$.

By setting the dimension $n^{(a)}$ of $\ell^{(a)}$ to be small which encodes different images, we can encourage our optimization process to find **a continuous space where illumination conditions can be embedded**. This results in the smooth interpolations between different lighting conditions as shown in the figure below.

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig4.png">
<span class="caption text-muted">Figure 4. <b>Interpolations between the appearance embeddings of two training images (leftmost and rightmost) which generates images conditioned on unknown lightings (middle)</b>. Photos by Flickr users mightyohm, blatez / CC BY.</span>

## Transient Objects

This paper addresses transient phenomena using two distinct design decisions:

1. Designate the color-emitting MLP used in NeRF as the *static* head of the model and add an additional *transient* head that emits its own color and density. Note that this (transient) density is allowed to vary across training images (since this is for non-static objects that appear or disappear across images). As a consequence, the model can separately learn static objects and non-static objects appearing differently in various images.
2. Instead of assuming that all observed pixel colors are equally reliable, the transient head of NeRF-W emits *a field of uncertainty*. This allows the model to adapt its reconstruction loss **to ignore unreliable pixels and 3D locations that are likely to contain occluders** (i.e. where transient scene element is). To this end, the authors modeled each pixel's color as an isotropic normal distribution whose likelihood should be maximized, while the variance of that distribution is "rendered" using the same volume rendering approach used by NeRF.

To construct the transient head, we first start with the volume rendering formulation from NeRF and augment the static density $\sigma(t)$ and radiance $\textbf{c}\_{i}(t)$ with transient counterparts $\sigma_{i}^{(\tau)}(t)$ and $\textbf{c}_{i}^{(\tau)}(t)$, â†’ Similar to image compositing!

$$ \begin{gather} \hat{C}_{i} (\textbf{r}) = \sum_{k=1}^{K} T_{i} (t_{k}) \Bigg( \alpha \Big( \sigma (t_{k}) \delta_{k} \Big) \textbf{c}_{i} (t_{k}) + \alpha \Big( \sigma_{i}^{(\tau)}(t_{k}) \delta_{k} \Big) \textbf{c}_{i}^{(\tau)} (t_{k}) \Bigg), \\
\text{where} \,\, T_{i}(t_{k}) = \exp \bigg( - \sum_{k^{\prime}=1}^{k-1} \Big( \sigma (t_{k^{\prime}}) + \sigma_{i}^{(\tau)} (t_{k^{\prime}}) \Big) \delta_{k^{\prime}} \bigg).
 \end{gather} $$

Here, $\alpha(x) = 1 - \exp (-x)$. As you guessed, the expected color of $\textbf{r}(t)$ then becomes the **alpha composite** of both the static and the transient components. One important thing is that the dependency on image $i$ in transparency $T_{i}$ is solely from the transient density $\sigma_{i}^{(\tau)}$ **NOT** the static density $\sigma$ shared across all images.

Next, let's move on to the uncertainty modeling. Specifically, authors employed Bayesian learning framework to model the uncertainty of the observed color. The assumption here is that the observed pixel intensities are inherently noisy and it's input dependent. Thus, the authors modeled the observed color $\textbf{C}\_{i}(\textbf{r})$ with an isotropic normal distribution (whose $\Sigma = \sigma^{2}I$) with image- and ray-dependent variance $\beta_{i}(\textbf{r})^{2}$ and mean $\hat{\textbf{C}}\_{i} (\textbf{r})$. **Variance $\beta_{i}(\textbf{r})$ is rendered analogously to color via alpha-compositing according to the transient density** $\sigma_{i}^{(\tau)}(t)$ â†’ Similar to alpha channel in typical imagery:

$$ \begin{gather} \hat{\beta}\_{i} (\textbf{r}) = \mathcal{R} (\textbf{r}, \beta_{i}, \sigma_{i}^{(\tau)}). \end{gather} $$

Note that $\hat{\beta_{i}}(\textbf{r})$ is the predicted uncertainty *along a ray*, while the $\beta_{i}$, one of the arguments of $\mathcal{R}$ stands for uncertainty *per each point* (or sample) on a ray, namely $\beta_{i} (t)$.

Therefore, to allow the transient component of the scene to vary across images, NeRF-W assigns each training image $\mathcal{I}\_{i}$ a second embedding $\ell_{i}^{(\tau)} \in \mathbb{R}^{n^{(\tau)}}$which is then given as input to the transient MLP,

$$ \begin{gather} \Big[ \sigma_{i}^{(\tau)}(t), \textbf{c}_{i}^{(\tau)}, \widetilde{\beta}_{i}(t) \Big] = \text{MLP}_{\theta_{3}} \big( \textbf{z}(t), \ell_{i}^{(\tau)} \big), \\
\beta_{i}(t) = \beta_{\text{min}} + \log \big( 1 + \exp \big( \widetilde{\beta}_{i}(t)\big) \big), \end{gather} $$

ReLU and sigmoid activations are used for $\sigma_{i}^{(\tau)} (t)$ and $\textbf{c}\_{i}^{(\tau)}(t)$, and a softplus is used as the activation for $\beta_{i}(t)$ (shifted by $\beta_{\text{min}} > 0$, a hyperparameter that ensures a minimum importance is assigned to each ray).

Therefore, the loss for ray $\textbf{r}$ in image $i$ with true color $\textbf{C}_{i} (\textbf{r})$ is

$$ \begin{gather} L_{i} (\textbf{r}) = \frac{\lVert \textbf{C}\_{i} (\textbf{r}) - \hat{\textbf{C}}\_{i} (\textbf{r})\rVert\_{2}^{2}}{2 \beta_{i} (\textbf{r})^{2}} + \frac{\log \beta\_{i} (\textbf{r})^{2}}{2} + \frac{\lambda\_{u}}{K} \sum\_{k=1}^{K} \sigma\_{i}^{(\tau)} (t\_{k}). \end{gather} $$

The first two terms are the (shifted) negative log likelihood of $\textbf{C}\_{i} (\textbf{r})$ according to a normal distribution with mean $\hat{C}\_{i} (\textbf{r})$ and variance $\beta_{i} (\textbf{r})^{2}$. Larger value of $\beta_{i} (\textbf{r})$ means greater uncertainty in a pixel determined by ray $\textbf{r}$, thus reduces the importance assigned to that pixel. Additionally, a pixel with high uncertainty is more likely to belong to transient components. More precisely, the first term is balanced by the second, which corresponds to the log-partition function of the normal distribution and precludes (= excludes) the trivial minimum at $\beta_{i} (\textbf{r}) = \infty$ (one of possible degenerate cases is the network predicting all pixels to be "uncertain"). The third term is an $L_{1}$ regularizer with a multiplier $\lambda_{u}$ on (non-negative) transient density $\sigma_{i}^{(\tau)}(t)$, and this regularization discourages the model from using transient density to explain static phenomena (also this is one possible degenerate case).

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig5.png">
<span class="caption text-muted">Figure 5. <b>NeRF-W separately renders the static (a) and transient (b) elements of the scene, and then composites them (c)</b>. Training minimizes the difference between the composite and the true image (d) weighted by uncertainty (e), which is simultaneously optimized to identify and discount anomalous image regions. Photo by Flickr user vasnic64 / CC BY.</span>

After training, we can get clear, unoccluded rendering of the object of interest by omitting the transient and uncertainty fields, and render only $\sigma(t)$ and $\textbf{c}(t)$. **In a nutshell, by letting the network recognize different scene components and their distributions, we can filter-out unwanted artifacts introduced due to image-dependent features, and obtain a neural volumetric representation including only the central geometry and lighting conditions.**

# Optimization

Two copies of $F_{\theta}$, coarse and fine model, are optimized just as NeRF did. A fine model uses the entire architecture shown in the figure 3 as well as the loss described previously. On the other hand, a coarse model uses only the latent appearance modeling component, and knows nothing about uncertainty modeling. Thus, the loss related to this network is much simpler. Not only the parameters $\theta$ of MLPs but also per-image appearance embeddings $ \\{ \ell_{i}^{(a)} \\}\_{i=1}^{N}$ and transient embeddings $\\{ \ell_{i}^{(\tau)} \\}_{i=1}^{N}$ are optimized during training. Therefore, the full loss of NeRF-W is:

$$ \begin{gather} \sum_{ij} L_{i} (\textbf{r}\_{ij}) + \frac{1}{2} \lVert \textbf{C} (\textbf{r}\_{ij}) - \hat{\textbf{C}^{c}}\_{i} (\textbf{r}\_{ij}) \rVert_{2}^{2}, \end{gather} $$

where $\lambda_{u}$, $\beta_{\text{min}}$, and embedding dimensionalities $n^{(a)}$ and $n^{(\tau)}$ form the set of additional hyperparameters for NeRF-W.

As optimization only produces appearance embeddings $\{ \ell_{i}^{(a)} \}$ for images in the training set, the embeddings of test set images are unknown. Thus, when visualizing test set data, one may choose $\ell^{(a)}$ to best fit a target image or let it be arbitrary.

# Experiments

ðŸ¤” **NOTE: This post presents only qualitative results from the paper. For details such as quantitative comparisons, please refer to the original paper.** ðŸ¤”

The ability of NeRF-W shines when tested on unconstrained internet photo collections. The authors chose 6 landmarks from the Phototourism dataset. 

The authors built additional components of NeRF-W based on NeRF. In order to estimate camera parameters from images in the dataset, COLMAP was used.

It took **2 days to train the model for 300,000 steps with a batch size of 2048 on 8 GPUs** using Adam optimizer.

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig6.png">
<span class="caption text-muted">Figure 6. <b>Depth maps from NeRF and NeRF-W, rendered by computing the expected termination depth of each ray</b>. Note that the geometry of NeRF is corrupted by appearance variation and occluders, while that of NeRF-W is not. Photos by Flickr users burkeandhare, photogreuh- phies / CC BY.</span>

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig7.png">
<span class="caption text-muted">Figure 7. <b>Qualitative results from experiments on the Phototourism dataset</b>. Photos by Flickr users firewave, clintonjeff, leoglenn_g / CC BY.</span>

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig8.png">
<span class="caption text-muted">Figure 8. <b>Further qualitative results from experiments on Phototourism dataset</b>. Photos by Flickr users yatani, jingjing, lricecsp / CC BY.</span>

# Limitations

<img class="img-fluid" src="/assets/post-images/NeRFW/NeRFW_fig9.png">
<span class="caption text-muted">Figure 9. <b>Limitations of NeRF-W</b>. Rarely-seen parts of the scene and incorrect camera poses can result in blurry areas in images.</span>

Although NeRF-W can produce photorealistic, temporarily consistent images from sets of unconstrained photos, **the quality of rendering degrades in areas of the scene that are rarely observed** in the training images. Also, NeRF-W is sensitive to camera calibration errors similar to NeRF, which leads to blurry reconstructions on the parts of the scene captured by incorrectly-calibrated cameras.

# Conclusion

- NeRF-W, a novel approach for 3D scene reconstruction of complex environments from unstructured internet photo collections that builds upon NeRF.
- NeRF-W learns per-image latent embedding which captures photometric appearance variations often present in in-the-wild data.
- NeRF-W decomposes the scene into image-dependent and shared components thereby disentangling transient elements from the static scene.
- Experimental evaluation on real-world (also synthetic) data exhibits significant improvement both in qualitative and quantitative comparisons with previous state-of-the-art approaches.

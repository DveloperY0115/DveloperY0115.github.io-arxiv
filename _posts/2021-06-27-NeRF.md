---
layout: post
title: "Summary on 'NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis'"
use_math: true
background: '/assets/post-images/NeRF/NeRF_overview.png'
---

<img class="img-fluid" src="/assets/post-images/NeRF/NeRF_overview.png">
<span class="caption text-muted">Figure 1. <b>NeRF Overview</b>.</span>

# Motivation

- Novel view image synthesis is one of the long-standing problems in both computer vision and computer graphics fields, and its significance is becoming larger as demands on interactive media applications increases.
- There were huge success in representing highly detailed 3D shape with neural network, but these methods were not suitable for reproducing realistic images compared to discrete-representation-based methods using mesh or voxels. **→ In other words, no rendering technique for continuous geometry & radiance distributions.**

# Key Contributions

- State-of-the-art method for synthesizing novel views of complex scenes by optimizing **continuous volumetric scene function** (= radiance field) representing complex geometry and materials.
- **Differentiable rendering** based on **classical volume rendering** techniques, which generates images that can be compared with ground truth for optimization. And **hierarchical sampling** that reduces the cost by adequately sampling high frequency scene representation.
- **Positional encoding** for mapping 5D coordinates to higher dimensional space.

# Key Concepts

<img class="img-fluid" src="/assets/post-images/NeRF/NeRF_novel_view_synthesis.png">
<span class="caption text-muted">Figure 2. <b>Novel View Synthesis with NeRF</b>.</span>

The overall process can be summarized into three steps:

1. March camera rays through the scene to generate  a sampled set of 3D points.
2. Use these points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities.
3. Use classical volume rendering technique to produce 2D image, which is the final rendering of the scene.

**The entire process is differentiable, so this can be easily parametrized with neural network, and optimized through gradient descent.**

## Neural Radiance Field Scene Representation

The **continuous scene** of interest can be **represented as a 5D vector-valued function** $F_{\Theta}$whose input & output are:

- **Input**: Concatenation of a 3D location $\textbf{x} = (x, y, z)$ and 2D viewing direction $(\theta, \phi)$
- **Output**: A color $\textbf{c} = (r, g, b)$ and volume density $\sigma$ at that point

Thus, the relation between input and output in terms of $F_{\theta}$ is:

$$F_{\Theta}: (\textbf{x}, \textbf{d}) \to (\textbf{c}, \sigma)$$

Note that 5D input vector is obtained by casting ray though scene, **sampling points along each ray**. Also, **the viewing direction $(\theta, \phi)$ are substituted to a unit vector $\textbf{d}$ indicating the identical direction** in practice.

After sampling enough sample points and both $\textbf{c}$ and $\sigma$ associated with each point, the classical volume renderer comes into play, generating the final image.

Then the predicted image and ground truth image are compared & used to calculate loss function to be optimized. This way, the parameters $\Theta$ are optimized through gradient descent.

<img class="img-fluid" src="/assets/post-images/NeRF/NeRF_view_dependent_emitted_radiance.png">
<span class="caption text-muted">Figure 3. <b>View-dependent emitted radiance</b>. It's natural for a single point to be seen differently with respect to observers position and even view direction.</span>

Internally, **the volume density $\sigma$ is affected by $\textbf{x}$ only**, to ensure multiview consistency. On the other hand, **the color $\textbf{c}$ is a function of both $\textbf{x}$ and $\textbf{d}$**. By doing so, the model can successfully model non-Lambertian (i.e. specular) materials given different view settings as shown in the figure above. **Otherwise, NeRF won't be able to predict the color value properly for materials interacting with light & observer differently according to their relative position and orientation.**

<img class="img-fluid" src="/assets/post-images/NeRF/NeRF_effect_of_view_dependence.png">
<span class="caption text-muted">Figure 4. <b>Effect of view-dependence</b>. The model struggles to reproduce the behavior of specular materials with no information about viewpoint.</span>

<img class="img-fluid" src="/assets/post-images/NeRF/NeRF_network_structure.png">
<span class="caption text-muted">Figure 5. <b>Network Structure of NeRF</b>.  Notice that the viewpoint dependent information is only involves the very last part of the pipeline.</span>

## Volume Rendering with Radiance Fields

We now have a representation for a scene, describing it as the volume density and directional emitted radiance at any point in space (that's why it's called radiance *field*). Then our next question would be: *"How can we render an image with this?"*

One non-trivial work involving rendering is to **determine which part of a scene is visible**. The interaction between light and object happens naturally in nature but it's hard to mimic in computer graphics. And this is the part where the classical volume rendering comes into play.

The **volume density** $\sigma(\textbf{x})$, which is only dependent on the position $\textbf{x}$, **can be thought as the differential probability of a ray terminating at an infinitesimal particle located at $\textbf{x}$**. For example, if a particle located at $\textbf{x}$ is completely opaque (i.e. the ray cannot penetrate this point) that the volume density at this point becomes 1, preventing every points behind it along a ray from contributing to a pixel.

That being said, the expected color $C(\textbf{r})$ of camera ray $\textbf{r}(t) = \textbf{o} + t\textbf{d}$ with near and far bounds $t_n$ and $t_f$ can be defined as:

 

$$C(\textbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\textbf{r}(t))\textbf{c}(\textbf{r}(t), \textbf{d})dt$$

where $T(t) = exp(-\int_{t_n}^{t} \sigma(\textbf{r}(s))ds))$. The function $T(t)$ denotes the *accumulated transmittance* along the ray from $t_n$ to $t$. 

Observe that $T(t)$ decays rapidly as a casted ray meets many opaque points (i.e. having high $\sigma$ values) before reaching the point  $\textbf{x}^{\prime} = \textbf{r}(t)$. This means that the volume density and color of point $\textbf{x}^{\prime}$ have low or almost zero contribution to the pixel back-tracing the ray.

Then the only practical problem left is **how the integral is calculated (or approximated)** numerically. In short, integral can be estimated using quadrature. However, conventional deterministic quadrature typically used in rendering discretized voxel grids won't fit into our purpose since it only uses pre-fixed points when calculating integral.

Instead, we **introduce some randomness** by doing **stratified sampling** which does stochastic sampling of points where the value of integrand should be evaluated.  

Given near and far bounds $t_n$ and $t_f$, the interval $[t_n, t_f]$ can be partitioned into $N$ evenly-spaced subintervals. Then, samples are drawn uniformly from each subintervals. More rigorously,

$$t_i \sim \mathcal{U}[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)]$$

Then these samples are used to estimate the integral, but this time with slight modification,

$$\hat{C}(\textbf{r}) = \sum_{i=1}^{N} T_i(1 - exp(-\sigma_i\delta_i))\textbf{c}_i$$

where $T_i = exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j)$, and $\delta_i = t_{i+1} - t_i$, which is the distance between two adjacent samples along a ray. Note that, the function described above is fully differentiable and resembles the traditional alpha compositing with $\alpha_i = 1 - exp(-\sigma_i\delta_i)$.

## Optimizing a Neural Radiance Field

While the basic concept of scene representation as neural radiance field remains the same, there're some factors added to the pipeline to push the boundary even further.

1. **Positional encoding** of input coordinates assisting the MLP in representing high-frequency functions
2. **Hierarchical sampling** which is more efficient sampling method capturing high-frequency detail as well.

### Positional Encoding

It's well-known that neural networks are universal function approximators, but **letting the network $F_{\Theta}$ operate directly on $(x, y, z, \theta, \phi)$ results in poor performance, especially in capturing high-frequency detail** (both in color and geometry) contained in a scene. This is because of the tendency of neural network learning to regress low frequency functions (Rahaman, et al., On the spectral bias of neural networks, ICML 2018).

Thus, it's desirable to tweak the original setting of $F_{\Theta}$ a little bit, so that it could capture high-frequency information well. To this end, $F_{\Theta}$ is reformulated into a composition of two functions $F_{\Theta} = F_{\Theta}^{\prime} \circ \gamma$ . One is with learnable parameters while the other is not. Here, **the function $\gamma$ is a mapping from $\mathbb{R}$ to a higher dimensional space** $\mathbb{R}^{2L}$, where **$F^{\prime}_{\Theta}$ remains a MLP**. Formally, the encoding function $\gamma$ used here is (note that it's deterministic!):

$$\gamma(p) = (\sin(2^0\pi p), \cos(2^0 \pi p), \cdot\cdot\cdot, \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p))$$

**→ Any relation with Fourier analysis or mathematical justification?** 

In the pipeline, the function $\gamma(\cdot)$ is applied separately on each element of $\textbf{x} = (x, y, z)$, and $\textbf{d} = (d_x, d_y, d_z)$. Here, each coordinates of $\textbf{x}$ are assumed to be normalized into $[-1, 1]$, thus $\textbf{x}$ is lying in canonical cubic space, just like NDC in OpenGL pipeline. Also, $\textbf{d}$ is assumed to be an unit vector.

According to the authors, for positional encoding,

- $L=10$,  for $\gamma(\textbf{x})$
- $L=4$,  for $\gamma(\textbf{d})$

are used, respectively.

### Hierarchical Sampling

The sampling strategy introduced earlier which samples $N$ points uniformly from subintervals each of which equally partitions the tragectory of a ray is inefficient. To improve it, we might think of a way to predict *samples that would bring huge impacts on determining final render*.

Thus, instead of using a single network, two networks are trained separately - one using "coarse" sampling, the other using "fine" sampling strategy. **→ "Fine" network requires results from "coarse" network. Can be thought as some sort of post processing!**

Then, the color from coarse network $\hat{C}_c(\textbf{r})$ is just identical to the previously introduced discrete summation, but with slight modification in notation:

$$\hat{C}_c(\textbf{r}) = \sum_{i=1}^{N_c} w_ic_i, \, \text{where} \,\, w_i = T_i(1 - exp(\sigma_i \delta_i))$$

As one can see, the accumulated density before $i$-th sample and the density of $i$-th sample are considered together as the **weight of the color value** that $i$-th sample possesses.

By normalizing these weights as in the following: 

$$\hat{w}_i = w_i / \sum_{j=1}^{N_c} w_j$$ 

we can obtain a piecewise-constant PDF along the ray. Then, the new set $N_f$ number of points are sampled according to this PDF using *inverse transform sampling*. And the color for final render is calculated using the same method (numerical integration) but using all $N_c + N_f$ samples. This let the model focus more on important candidates for evaluating color for pixels.

# Implementation Details

**REMARK: A neural continuous volume representation network can represent only a single scene!**

The model takes **a set of RGB images** of the scene we want to represent as NeRF, and corresponding **camera intrinsic and extrinsic**, and **scene bounds**. Note that third-party software is used to extract these information when treating real world data.

At each optimization step, 

1. A batch of camera rays originated from all pixels are randomly sampled
2. Sample $N_c$ samples for evaluating & optimizing *coarse network*
3. Again sample $N_f$ samples and then pass these and previously sampled $N_c$ samples together to *fine network* and obtain the color values at each point along rays
4. Render the scene using volume rendering technique and get the final output image
5. Evaluate the loss function and back propagate

The loss for optimizing NeRF is simply the mean square error between predicted images (from coarse and fine network, respectively) and ground truth:

$$\mathcal{L} = \sum_{\textbf{r} \in \mathcal{R}} [\vert\vert \hat{C}_c(\textbf{r}) - C(\textbf{r})\vert\vert_{2}^{2} + \vert\vert \hat{C}_f(\textbf{r}) - C(\textbf{r})\vert\vert_{2}^{2} ]$$

where $\mathcal{R}$ is the set of rays in each batch, $C(\textbf{r})$, $\hat{C}_c(\textbf{r})$, and $\hat{C}_f(\textbf{r})$ are the ground truth, predicted color from coarse network, and predicted color from fine network for ray $\textbf{r}$ respectively. **Note that, although we obtain final output from fine network, coarse network should be optimized simultaneously for accurate hierarchical sampling.** 

Below are some numbers related to the training:

- Batch size: 4096 (rays)
- Sampled points: $N_c = 64$, $N_f = 128$
- Adam optimizer parameters: $\alpha = 5 \times 10^{-4}$, but decaying to $5 \times 10^{-5}$
- **1 ~ 2 days before convergence with a single NVIDIA V100 GPU**
I"WF<h1 id="motivations">Motivations</h1>

<ul>
  <li>This work is inspired by two reciprocal mechanisms underlying human perception that are often discussed in the field of cognitive science: the <strong>bottom-up</strong> (proceeding from the retina up to the visual cortex - local elements and salient stimuli hierarchically group together to form the whole) processing, and the <strong>top-down</strong> (surrounding global context, selective attention and prior knowledge inform the interpretation of the particular) processing.</li>
  <li>However, the convolutional neural network, which gained huge success in computer vision over the last decade, does not reflect this bidirectional nature of our visual system. Rather, its feed-forward propagation only mimics the bottom-up processing by building up higher, abstract representations from raw sensory signals.</li>
  <li>The lack of global context understanding leads to multiple problems - (1) <em>model struggling to capture long-range dependencies</em>, (2) <em>develop holistic understanding of global shapes and structures</em>, (3) <em>optimization and stability issues</em> (the difficulty of training GANs is infamous üòµ) due to the inherent difficulty in coordinating between fine details across the generated image.</li>
  <li>Thus, all of these bring us to the unanswered question: <strong>‚ÄúIs convolution alone a complete solution, or do we need more?‚Äù</strong></li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Introduces the <strong>GANformer</strong>, a novel and efficient type of transformer, a highly-adaptive architecture centered around relational attention and dynamic interaction, and explore it for the task of visual generative modeling.</li>
  <li>This new <em>bipartite</em> structure enables long range interactions across the image, reducing the computational cost from $O(n^{2})$ of conventional transformers to $O(n)$. This hints the new way of applying transformers to high-resolution synthesis which was considered impossible due to the expensiveness of transformers.</li>
  <li>Compared to the classic transformers, this structure utilizes multiplicative integration which allows flexible region-based modulation. ‚Üí GANformer can be regarded as a generalization of StyleGAN!</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This paper introduces GANformer, a combination of two renowned architectures - GAN &amp; Transformer - to overcome the limitations that previous GANs had in the task of synthesizing scenes composed of multiple objects. This novel, bipartite architecture successfully brings the notion of attention to the vision tasks with reasonable, linear time complexity.</p>

<h1 id="methods">Methods</h1>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>Sample images generated by the GANformer with a visualization of the model attention maps</b>.</span></p>

<h2 id="the-generative-adversarial-transformer">The Generative Adversarial Transformer</h2>

<p>The Generative Adversarial Transformer (GANformer) is a type of Generative Adversarial Network (GAN) consists of a <em>generator</em> network (G) that maps a sample from the <em>latent</em> space to the output space, and a <em>discriminator</em> network (D) whose goal is to distinguish real and fake samples. During training, the two networks compete with each other through a minimax game until reaching an equilibrium.</p>

<p>While typical generators and discriminators are built by composing multiple convolutional layers, in the case of GANformer, both of them are constructed by using a novel architecture called <strong><em>Bipartite Transformer</em></strong>.</p>

<h3 id="the-bipartite-transformer">The Bipartite Transformer</h3>

<p>Before explaining what the bipartite transformer is, let me briefly review the conventional transformers.</p>

<p>The standard transformer network is composed of alternating multi-head self-attention and feed-forward layers. The authors of GANformer refer to <em>each pair of self-attention and feed-forward layers</em> as <em>a transformer layer</em>, so a transformer can be thought as a stack of several transformer layers. The self-attention layer, the core idea &amp; structure of transformer, numericalizes all pairwise relations among the input elements (e.g. relations between each word in a sentence), thereby updating each single elements by attending to all the others.</p>

<p>Then, what makes the bipartite transformer so stand out from the standard one? In short, the bipartite transformer can be considered as a generalization of self-attention. Instead of taking relations of every element pair into account, <strong>the bipartite transformer formulates a bipartite graph between two groups of variables</strong>. In the GAN case, the set of latents and image features can be thought as such groups.</p>

<p>Moreover, the authors of GANformer suggest two forms of attention that could be computed over the bipartite graph, categorized by the direction in which information propagates - (1) <strong>Simplex attention (one way only)</strong>, and (2) <strong>Duplex attention (both ways)</strong>. Just like other transformers, the GANformer adopts multi-head structures in practice while the following descriptions are all done with one-head version for clarity.</p>

<p><img src="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-03_20.44.17.png" alt="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-03_20.44.17.png" /></p>

<p>Figure 2. <strong>Bipartite Attention</strong>.</p>

<h3 id="simplex-attention">Simplex Attention</h3>

<p>The <em>simplex attention</em> distributes information in a single direction over the bipartite transformer graph.</p>

<p>Formally, let $X^{n \times d}$ denote an input set of $n$ vectors of dimension $d$ (in particular, $n = W \times H$ for the image case), and $Y^{m \times d}$ denote a set of $m$ aggregator variables (in the generative case, they become latent vectors). Then we can compute attention over the derived bipartite graph between these two groups:</p>

\[Attention (Q, K, V) = \text{softmax} \Big( \frac{QK^{T}}{\sqrt{d}}\Big) V 
\\
a(X, Y) = Attention(q(X), k(Y), v(Y))\]

<p>Here, $q(\cdot), k(\cdot), v(\cdot)$ are functions that maps elements into queries, keys, and values, respectively, all maintaining dimensionality $d$. Also, the inputs to these mappings are all <em>positional encoded</em> reflecting the distinct position of each element (e.g. in the image). When $Y=X$, this bipartite attention becomes equivalent to the self-attention.</p>

<p>Then, we integrate the attended information with the input elements $X$. In the case of the standard transformer, it applies an additive update rule of the form:</p>

<p>$$u^{a}(X, Y) = LayerNorm(X + a(X, Y))$$</p>

<p>In contrast, in the case of GANformer, it exploits the retrieved information to control both the <em>scale</em> as well as the <em>bias</em> of the elements in $X$, following the practice proven to be effective by StyleGAN model. Such multiplicative integration improves the model performance by huge margin. Formally:</p>

<p>$$u^{s}(X, Y) = \gamma(a(X, Y)) \,  \odot \, \omega(X) + \beta (a(X, Y))$$</p>

<p>where $\gamma(\cdot), \beta(\cdot)$ are mappings that compute multiplicative and additive styles, maintaining a dimension of $d$. and $\omega(X) = \frac{X - \mu(X)}{\sigma(X)}$ normalizes each element with respect to the other features.</p>

<p>One important implication of this multiiplicative integration is that, by normalizing $X$ (image features in generative model), and then letting $Y$ (latents) control the statistical tendencies of $X$, the information can flow from $Y$ to $X$. <strong>That is, the higher &amp; abstract knowledge on images (latents) can control the visual generation of spatial (local) attended regions (features) within the image.</strong></p>

<p><img src="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_21.45.28.png" alt="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_21.45.28.png" /></p>

<p>Figure 3. <strong>Attention Maps</strong>.</p>

<h3 id="duplex-attention">Duplex Attention</h3>

<p>The idea of simplex attention can be extended to the case where the variables $Y$ have a key-value structure of their own, namely $Y = (K^{n \times d}, V^{n \times d})$. In such case, the values $V$ store the <em>content</em> of the $Y$variables as before, while the keys track the <em><strong>centroids</strong> $K$</em> of the attention-based assignments between $Y$ and $X$, which can be computed as $K = a(Y, X)$ - the weighted averages of the $X$ elements using the bipartite attention. As a result, we can define a new update rule:</p>

<p>$$u^{d} (X, Y) = \gamma (A(X, K, V)) \, \odot \, \omega(X) + \beta(A(X, K, V))$$</p>

<p>This new update rule includes two attention operations on top of each other:</p>

<ol>
  <li>Compute soft attention assignments between $X$ and $Y$, by $K = a(Y, X)$,</li>
  <li>Refine the assignments by considering their centroids, by $A(X, K, V)$.</li>
</ol>

<p>Note that this is analogous to the $k$-means algorithm and turned out to be more effective than the simpler update rule $u^{a}$ in the experiments.</p>

<p>To support bidirectional interaction between $X$ and $Y$, the authors chained two reciprocal simplex attentions from $X$ to $Y$ and from $Y$to $X$, obtaining the <strong><em>duplex attention</em></strong>, which is actually the alternating computation of $Y := u^{a}(Y, X)$ and $X := u^{d}(X, Y)$. By doing so, we can simultaneously update each representation through bidirectional interaction - bottom-up and top-down.</p>

<h3 id="overall-architecture-structure">Overall Architecture Structure</h3>

<p><img src="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_18.55.42.png" alt="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_18.55.42.png" /></p>

<p>Figure 4. <strong>Model Overview</strong>. <strong>Left</strong>: The GANformer layer consists of a bipartite attention operation to propagate information from the latents to the image grid, followed by convolution and upsampling. These layers are stacked multiple times and generates images by starting from a 4 X 4 grid refining them over different layers until reaching a final high-resolution image. <strong>Right</strong>: The latents and image features attend to each other to capture the scene structure. In contrast to StyleGAN where a single latent uniformly impacts the entire image, different latents in GANformer‚Äôs compositional latent space contribute to different (semantic) regions of the final output image.</p>

<p><strong>Vision-Specific Adaptations.</strong> In the standard transformer used for NLP, each self-attention layer is followed by a FC layer that processes each element independently (which can be seen as a $1 \times 1$ convolution). Since this work is about images not sentences, the authors instead used <strong>a convolution of which kernel size is</strong> $k=3$ ($3 \times 3$) after each application of the attention. For nonlinearity activation, a <strong>Leaky ReLU</strong> is applied after each convolution - they upsample the image in the generator, while the ones of discriminator downsamples the input image. To make the transformer be aware of the feature location in the image, the authors used a <strong>sinusoidal positional encoding</strong> along the horizontal and vertical dimensions for the visual features $X$, and trained positional embeddings for the set of latent variables $Y$.</p>

<p>This design decision - allowing latents to communicate through image features indirectly - enables adaptive long-range interaction between far away pixels passing through a compact and global <em>latent bottleneck</em> which selectively gathers information from the entire input and distributes it back to the relevant regions.</p>

<p>Fortunately, the bipartite structure make it possible to compute both the simplex and duplex attention operations with bilinear time complexity $O(mn)$. Furthermore, $m$, the number of latents is kept small (in the range of 8 to 32) in practice making the comparison with the standard self-attention, whose cost is $O(n^{2})$, almost meaningless.</p>

<h3 id="the-generator-and-discriminator-networks">The Generator and Discriminator Networks</h3>

<p><img src="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_21.47.16.png" alt="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_21.47.16.png" /></p>

<p>Figure 5. <strong>Upper-Layer Attention Maps</strong>.</p>

<p>The design of GANformer starts from that of StyleGAN, a structure that gained huge popularity in research community due to significant quality of generated images.</p>

<p>What makes StyleGAN stand out from the common GANs whose generator networks usually composed of multiple convolutional layers which transforms a randomly sampled latent vector $z$ into an image, is a feed-forward <em>mapping network</em> that outputs an intermediate latent vector $w$. Moreover, this transformed latent vector $w$ then goes through the <em>synthesis network,</em> which globally controls the feature maps‚Äô statistics at every layer.</p>

<p>One surprising outcome of this approach is that StyleGAN can decompose visual properties of images into different layers of it. This allows StyleGAN to control global aspects of the picture such as pose, lighting conditions, camera position, or color schemes in visually plausible ways. <strong>However, StyleGAN also has limit.</strong> While StyleGAN successfully decomposes global properties, it often fails to perform <em>spatial decomposition</em> since it provides no direct means to control the style of a localized regions.</p>

<p>Then the bipartite transformer comes to rescue. Compared to StyleGAN which tried to control the style of all features globally, the two types of attention brought to vision task with reasonable computational cost performs adaptive region-wise modulation. More precisely, the latent vector $z$ is splitted into $k$ components, $z = [z_{1}, \dots, z_{k}]$. These splitted variables are transformed into a set of intermediate latent variables $Y = [y_{1}, \dots, y_{k}]$ by a shared mapping network. Then, during image synthesis, the feature map $X$ and latents $Y$ play the roles of two elements groups, exchanging the information through the <em>bipartite latent bottleneck</em> introduced before, either by the simplex or duplex attention operation.</p>

<p>This setting allows for a <strong>flexible and dynamic style modulation at the region level</strong>. It is because of the tendency of soft attention which groups elements based on their proximity and content similarity.</p>

<p>For the discriminator, the authors applied attention after every convolution, and used trained embeddings to initialize the aggregator variables $Y$, hoping it to represent background knowledge the model learns about the task. At the last layer, they concatenated these variables $Y$ to the final feature map $X$ to make a prediction about the image source (real, fake, or ideally, indistinguishable).</p>

<p>For training, the authors adopted the settings and techniques of StyleGAN2, including the loss function, optimization and training configuration.</p>

<p><img src="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_22.07.26.png" alt="Generative%20Adversarial%20Transformers%20(ICML%202021)%20d9df3aed51554abdaf7a91c981879a37/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-04_22.07.26.png" /></p>

<p>Fig 6. <strong>Sample Images and Attention Maps of Different Layers</strong>. Samples of images generated by the GANformer on the CLEVR, LSUN-Bedroom and Cityscapes datasets, and visualizations of the produced attention maps.</p>

<h1 id="conclusion">Conclusion</h1>

<p>GANformer successfully unifies the GAN and Transformer architectures for the task of scene generation. The key contributions of this work is:</p>

<ul>
  <li><strong>Compositional Latent Space</strong> with multiple variables that coordinate through attention to produce the image cooperatively, while matching the inherent compositionality of natural scenes.</li>
  <li><strong>Bipartite Structure</strong> that balances between expressiveness and efficiency, modeling long-range dependencies while maintaining linear computational costs.</li>
  <li><strong>Bidirectional Interaction</strong> between the latents and the visual (image) features, which allows the transaction of information both in bottom-up, top-bottom manner.</li>
  <li><strong>Multiplicative Integration</strong> rule to impact the features‚Äô visual style more flexibly.</li>
</ul>
:ET
I"Ç^<div>
![](https://www.youtube.com/watch?v=gz5E9wszZSI)
&lt;&gt;
<span class="caption text-muted">Video 1. <b>GANverse3D: Knight Rider KITT Re-created with AI by NVIDIA</b>.</span>

# Motivations

- Although differentiable rendering has paved the way to training neural networks to perform inverse graphics, most of current approaches rely on multi-view imagery that are not readily available in practice.
- Recent GANs that synthesize images seem to acquire 3D knowledge implicitly during training. For example, object viewpoints in generated images can be manipulated by modifying the latent codes. However, these latent codes lack further physical interpretation and thus GANs cannot be directly used for 3D reasoning.
- This work tries to **mix the best of the two worlds** - differentiable rendering capable of performing inverse graphics and utilizing multiple realistic images generated by GAN as  training data.

# Key Contributions

- Proposes a pipeline which extract and disentangle 3D knowledge learned by generative models by utilizing differentiable graphics renderers.
- Suggests an idea of exploiting generative models (StyleGAN in this case) as a generator of multi-view imagery to train an inverse graphics neural network using a differentiable renderer.
- Jointly connected StyleGAN and the inverse graphics network, trained by minimizing cycle-consistency losses, significantly outperforms existing inverse graphics networks on existing datasets. We can further control 3D generation and manipulation of imagery by using the disentangled generative model.

---

# TL;DR

This work merges two cutting-edge fields in computer vision - differentiable rendering and GANs - to perform a task of reconstructing 3D model with proper appearance given just a single image. To overcome the limit of previous differentiable rendering approaches that required multi-view imagery of the target object that are not readily available in most cases, this work utilizes 3D knowledge implicitly acquired by GANs to populate multi-view dataset from an image. Those multi-view imageries are then passed to inverse graphics network resulting in high quality reconstruction of an object (as well as its appearance) depicted in the original image.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig1.png" />
</center>
<span class="caption text-muted">Figure 1. <b>Image GAN (StyleGAN) meets differentiable renderer</b>.</span>

# Methods

In short, this approach marries two types of renderers: GAN based neural renderer and a differentiable graphics renderer. Specifically, this work leverages the fact that the recent state-of-the-art GAN architecture **StyleGAN learns to produce highly realistic images and further allows a reliable control over the camera.**

The authors manually select a few camera views with a rough **viewpoint annotation** and **used StyleGAN to generate a large number of examples per view**. This data can be quickly generated and later be utilized to train an **inverse graphics network** utilizing the state-of-the-art differentiable renderer **DIB-R** with a small modification to deal with noisy cameras during training.

This trained inverse graphics network is then used to disentangle StyleGAN's latent code and turn StyleGAN into a 3D neural renderer that provides control over human-perceptible 3D properties.

## StyleGAN as Synthetic Data Generator

The goal of this step is to utilize StyleGAN to generate multi-view imagery.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig2.png" />
</center>
<span class="caption text-muted">Figure 2. <b>StyleGAN Overview</b>. The latent vector 'z' is first transformed to another latent vector 'w' and then again undergoes transform 'A', defined individually at each level of synthesis network, at the time of injection.</span>

StyleGAN is a 16 layers neural network that **maps a latent code** $z \in \mathcal{Z}$ **drawn from a normal distribution into a realistic image.** The code $z$ is first mapped to an intermediate latent code $w \in W$ which is transformed to $w^{\*} = (w\_{1}^{\*}, w\_{2}^{\*}, \dots, w\_{16}^{\*}) \in W^{\*}$ through 16 learned affine transformations. The latent space $W^{\*}$, obtained by applying transformations to $W$ is called *the transformed latent space* from now on. These transformed latent codes $w^{\*}$ are then injected as the style information to the StyleGAN synthesis network.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig3.png" />
</center>
<span class="caption text-muted">Figure 3. <b>Examples of generating multi-view dataset by manipulating different parts of latent code provided to StyleGAN</b>. (1) Along row: images generated using the same viewpoint code but with different content code. (2) Along column: images generated using the same content code but with different viewpoint code.</span>

One notable characteristic of StyleGAN is that different layers control different image attributes (disentanglement of image features). Those disentangled features include camera viewpoints (early layers), shape, texture and even background (intermediate and higher layers). **The authors empirically discovered that the latent code $w\_{v}^{\*} := (w\_{1}^{\*}, w\_{2}^{\*}, w\_{3}^{\*}, w\_{4}^{\*})$ in the first 4 layers controls camera viewpoints.** In other words, if we sample a new code $w\_{v}^{\*}$ but keep the remaining dimensions of $w^{\*}$ fixed, we can generate images of the same object but seen from different viewpoints. Conversely, if we fix $w\_{v}^{\*}$ but sample the remaining dimensions of $w^{\*}$, StyleGAN produces imagery of *different objects* in *the same camera viewpoint*. Thus we may conclude that the latent code $w\_{v}^{\*}$ is a handle for controlling viewpoint in images.

This observation is very important since the fact that we can control the viewpoint while keeping other image components fixed makes StyleGAN a *multi-view* data generator for an object. That is, even if we have only a single shot of an object, StyleGAN can generate the other aspects of it simultaneously achieving acceptable photorealism.

## StyleGAN multi-view dataset

The authors manually selected several views that cover all the common viewpoints of an object ranging from:

- 0 ~ 360 in azimuth
- 0 ~ 30 in elevation

These viewpoints are carefully chosen so that the captured object look consistent across the viewpoints. The authors also annotated the chosen viewpoint codes with a rough absolute camera pose. More precisely, they classified each viewpoint code into one of 12 azimuth angles, uniformly sampled along 360 degree. And they assigned each code a fixed elevation ($0^{\circ}$) and camera distance. Note that these poses are just coarse annotation of the actual poses. This annotation can be thought as the initialization of the camera which will be optimized throughout training. By doing so, we can **annotate all views in the dataset in only 1 minute** making effort for annotation negligible. For each viewpoint, the authors sampled a large number of content codes to synthesize different objects in corresponding views.

Since DIB-R requires segmentation masks during training, Mask R-CNN is applied on the generated multi-view dataset to get instance segmentation masks. As StyleGAN sometimes generates bad images (unrealistic, having multiple objects, etc), authors have filtered out such data before training.

## Training an Inverse Graphics Neural Network

The main objective of the 3D prediction network $f$, parametrized by $\theta$, is to infer 3D shapes (represented as meshes) and textures for them from 2D images.

Let $I_{V}$ denote an image in viewpoint $V$ from StyleGAN dataset (described earlier), and $M$ its corresponding object mask (generated by applying Mask R-CNN). The prediction made by the inverse graphics network is as follows: $\{ S, T \} = f_{\theta}(I_{V})$.

Here, $S$ denotes the predicted shape and T a texture map. The predicted shape $S$ is generated by deformation starting from a sphere (Note that this deformation based geometry reconstruction often hinders the application of pipeline to the shapes having different topologies).

To train the network, this work adopts DIB-R as the differentiable graphics renderer that takes $\{ S, T\}$ and $V$ as input and produces a rendered image $I_{V}^{\prime} = r (S, T, V)$ along with a rendered mask $M^{\prime}$. Then the loss function is defined as the following:

$$
\begin{gather*}
L(I, S, T, V; \theta) = \lambda_{\text{col}} L_{\text{col}}(I, I^{\prime}) + \lambda_{\text{percpt}} L_{\text{percpt}} (I, I^{\prime}) + L_{\text{IOU}} (M, M^{\prime}) + \lambda_{\text{sm}} L_{\text{sm}} (S) + \lambda_{\text{lap}} L_{\text{lap}} (S) + \lambda_{\text{mov}} L_{\text{mov}} (S)
\end{gather*}
$$

The meaning of each term is as follows:

- $L_{\text{col}}$: The standard $L_{1}$ image reconstruction loss defined in the RGB color space.
- $L_{\text{percpt}}$: The perceptual loss helping the predicted texture more realistic.
- $L_{\text{IOU}}$: The intersection-over-union between the ground-truth and the rendered mask (constraint on the silhouette).
- $L_{\text{sm}}$, $L_{\text{lap}}$: Flatten loss and Laplacian loss, respectively. Regularization losses that are commonly used to ensure that the predicted shape is well behaved.
- $L_{\text{mov}}$: Regularization loss for making shape deformation uniform and small.

Note that rendered images do not have background. Therefore, we need masks to compute $L_{\text{col}}$ and $L_{\text{percept}}$.

Moreover, since we already have multi-view images for each object (generated by StyleGAN), we can enforce consistency across different viewpoints by defining the final loss per object $k$ of form:

$$\begin{gather*}
\mathcal{L}_{k} (\theta) = \sum_{i,j, i \neq j} \Big( L (I_{V_{i}^{k}}, S_{k}, T_{k}, V_{i}^{k}; \theta) + L (I_{V_{j}^{k}}, S_{k}, T_{k}, V_{j}^{k}; \theta) \Big), \\
\text{where} \,\, \{ S_{k}, T_{k}, L_{k} \} = f_{\theta}(I_{V_{i}^{k}})
\end{gather*}$$

While there are multiple possibilities for choosing the number of views used to compute this loss, the authors empirically found that **two views are sufficient** for the task.

The above loss function is used to jointly train the neural network $f$ and optimize viewpoint cameras $V$. The underlying assumption is that different images generated from the same $w_{v}^{*}$ will have the same viewpoint $V$.

## Disentangling StyleGAN with the Inverse Graphics Model

Until now we've discussed the inverse graphics model which can infer a 3D mesh and texture from a given image. Now, it is the time to **utilize these 3D properties to disentangle StyleGAN's latent space, and turn StyleGAN into a fully controllable 3D neural renderer**. To differentiate it from the original StyleGAN, the authors call it **StyleGAN-R**. 

Before we move further, note that StyleGAN is an image generator which synthesizes not only object(s) in a scene but also background at the same time. **Ideally, what we want is a neural renderer that we can play with large degree of freedom.** This includes changing or modifying the object that appears in a generated image as well as the control over the background so that one can place 3D objects in various scenes. To address such challenge,  backgrounds are obtained from generated images by masking out the object.

The authors propose a mapping network that learns to map the viewpoint, shape (mesh), texture and background into the StyleGAN's latent code. Since **it's not guaranteed that StyleGAN is completely disentangled, it's further fine-tuned** while keeping the weights of the inverse graphics network fixed.

### Mapping Network

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig4.png" />
</center>
<span class="caption text-muted">Figure 4. <b>Mapping Network Overview</b>.</span>

The mapping network maps the viewpoints to fist 4 layers and maps the shape, texture and background to the last 12 layers of $W^{\*}$. The first 4 layers are denoted as $W\_{V}^{\*}$ and the last 12 layers are denoted as $W\_{STB}^{\*}$, where $W\_{V}^{\*} \in \mathbb{R}^{2048}$ and $W\_{STB}^{\*} \in \mathbb{R}^{3008}$. More precisely, each of scene components - camera, mesh, texture, background - passes through different mapping networks and transformed to latent vectors $\textbf{z}$:

- Viewpoint $V$ â†’ $g_{v}$ (MLP):  $\textbf{z}^{\text{view}} = g_{v} (V; \theta_{v})$
- Shape $S$ â†’ $g_s$ (MLP):  $\textbf{z}^{\text{shape}} = g_{s} (S; \theta_{s})$
- Texture $T$ â†’ $g_t$ (CNN):  $\textbf{z}^{\text{txt}} = g_{t} (T; \theta_{t})$
- Background $B$ â†’ $g_b$ (CNN):  $\textbf{z}^{bck} = g_{b} (B; \theta_{b})$

 where $\textbf{z}^{\text{view}} \in \mathbb{R}^{2048}$, $\textbf{z}^{\text{shape}}$, $\textbf{z}^{\text{txt}}$, $\textbf{z}^{\text{bck}} \in \mathbb{R}^{3008}$ and $\theta_{v}$, $\theta_{s}$, $\theta_{t}$, $\theta_{b}$ are network parameters trained independently to others. These codes are then combined to form the final latent code for StyleGAN and is defined as follows:

\$\$\widetilde{w}^{mtb} = \textbf{s}^{m} \odot \textbf{z}^{\text{shape}} + \textbf{s}^{t} \odot \textbf{z}^{\text{txt}} + \textbf{s}^{b} \odot \textbf{z}^{\text{bck}},\$\$

where $\odot$ denotes element-wise product, and $\textbf{s}^{m}$, $\textbf{s}^{t}$, $\textbf{s}^{b} \in \mathbb{R}^{3008}$ are shared across all the samples.

To achieve disentanglement, one desired property of the final latent code is that each dimension of it should be associated with only one property (e.g. shape, texture, or background). Thus each dimension of $\textbf{s}$ is normalized using softmax.

In practice, the authors found that mapping $V$ to a high dimensional code is challenging since StyleGAN dataset contains images with limited number of different viewpoints. Thus, it's better to reduce the dimensionality of latent vector corresponding to viewpoint $V$, otherwise it will suffer from the curse of dimensionality. In this work, $\textbf{z}^{\text{view}} \in \mathbb{R}^{144}$ is used.

### Training Scheme

The mapping network and StyleGAN are trained &amp; fine-tuned in two stages.

First, the mapping network is trained while the weights of StyleGAN are fixed. This makes the mapping network to output reasonable latent codes for StyleGAN. After that, both StyleGAN and the mapping network are fine-tuned to achieve clearer disentanglement of different attributes.

When warming up the mapping network, viewpoint codes $w\_{v}^{\*}$ are sampled among the chosen viewpoints and the remaining dimensions of $w^{\*} \in W^{\*}$ are sampled as well. Then the mapping network is trained so that **the mapped code $\widetilde{w}$ matches StyleGAN's code $w^{\*}$ by minimizing $L\_{2}$ difference** between these two. Precisely, the loss function for the mapping network is:

\$\$L\_{\text{mapnet}} (\theta\_{v}, \theta\_{s}, \theta\_{t}, \theta\_{b}) = \Vert \widetilde{w} - w^{\*} \Vert\_{2} - \sum\_{i} \sum\_{k \in \{ m, \, t, \, b\}} \textbf{s}\_{i}^{k} \log (\textbf{s}\_{i}^{k}).\$\$

Unfortunately, training only the mapping network does not disentangle the background from others. Therefore, StyleGAN has to be fine-tuned to get a better disentanglement. A cycle consistency loss involves in this fine-tuning process. Specifically, keep in mind that StyleGAN synthesizes images from sampled latent codes. The consistency between the original sampled properties and the shape, texture, and background of an image synthesized by StyleGAN from the code is encouraged via the inverse graphics network. The authors further fed the same background $B$ with two different $\{ S, T \}$ pairs (i.e. $\{ S_{1}, T_{1} \}$, $\{ S_{2}, T_{2} \}$) to generate two images $I_{1}$ and $I_{2}$. Then they enforced the re-synthesized backgrounds $\bar{B}\_{1}$ and $\bar{B}\_{2}$ to be similar. This loss lead the network to disentangle the background from the foreground object. Instead of directly measuring $L_{2}$ distance between the original images and the re-synthesized images in image space, which results in blurry images, they are first transformed to latent space and the distance between them is then computed. Therefore, the fine-tuning loss takes the following form:

$$\begin{gather*}
L_{\text{styleGAN}} (\theta_{\text{GAN}}) = \Vert S - \bar{S} \Vert_{2} + \Vert T - \bar{T} \Vert_{2} + \\ \Vert g_{b}(B) - g_{b}(\bar{B}) \Vert_{2} + \Vert g_{b}(\bar{B}_{1}) - g_{b}(\bar{B}_{2})\Vert_{2}
\end{gather*}$$

# Experiments

**ðŸ¤”  NOTE: This post presents only qualitative results from the paper. For details such as quantitative comparisons, please refer to the original paper. ðŸ¤”**

This approach is tested on various tasks including inverse graphics (3D image reconstruction), 3D neural rendering and 3D image manipulation.

## Dataset

Two kinds of datasets are used for the experiments.

1. **Image Datasets for training StyleGAN**
2. **StyleGAN Dataset proposed in this paper**

For details of datasets used, please refer to the paper.

## 3D Reconstruction Results

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig5.png" />
</center>
<span class="caption text-muted">Figure 5. <b>3D Reconstruction Results</b>.</span>

The quality of the predicted shapes and textures, and the diversity of the 3D car shapes is notable. Also, this pipeline can also work well on more challenging object classes such as horse and bird.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig6.png" />
</center>
<span class="caption text-muted">Figure 6. <b>Comparison on Pascal3D Test Set</b>.</span>

The effect of using StyleGAN-synthesized multi-view images turns out to be significant. The same inverse graphics networks but each separately trained on StyleGAN dataset and Pascal3D dataset exhibit huge difference between the reconstruction results both qualitatively and quantitatively. Note that the Pascal3D model's prediction is quite plausible in the input image view, but its quality degrades as the viewpoint changes. Meanwhile, **the same inverse graphics architecture trained on StyleGAN dataset is able to perform consistent reconstruction through various viewpoints.** 

Furthermore, the time for preparing &amp; annotating dataset is literally *incomparable*. While StyleGAN dataset takes about 1 minute for annotation, Pascal3D dataset costs about 200 ~ 350 hours for the same task proving the efficiency of this approach.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig7.png" />
</center>
<span class="caption text-muted">Figure 7. <b>Ablation Study</b>.</span>

Besides, the authors showed the importance of enforcing consistency across multiple views by introducing multi-view consistency loss during training. The figure above shows significant difference in quality between the model trained with or without such constraint. This justifies the process of generating StyleGAN dataset consists of multi-view images of objects by exploiting 3D knowledge acquired implicitly by StyleGAN itself.

## Dual Renderers

Note that two different renderers appear in the pipeline:

- **DIB-R** in the inverse graphics network
- **StyleGAN-R** as the result of disentanglement

The comparison is done by following the following steps:

1. Predict mesh and texture using the pretrained inverse graphics model.
2. Feed these 3D properties into StyleGAN-R to render a new image.
3. Or pass these properties to DIB-R instead for rendering.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig8.png" />
</center>
<span class="caption text-muted">Figure 8. <b>Dual Renderer (DIB-R vs StyleGAN-R)</b>.</span>

One significant difference between these two is that StyleGAN-R can put (textured) objects in various backgrounds of users' choice, while DIB-R can only render the object leaving the background empty. Also, the authors found that StyleGAN-R produces relatively consistent images compared to the input image - shapes and textured are well preserved while the background has only a slight content shift.

## 3D Image Manipulation with StyleGAN-R

One interesting aspect of this work is the discovery that StyleGAN can be modified and play a role of neural renderer which takes geometry, texture, and background, encodes them, and synthesizes images by compositing these elements in a semantically meaningful way.

To examine the capability of StyleGAN-R as a neural renderer, the authors tested it on their test set and a set of real images. In particular, given an input image, the authors first predicted 3D properties using the inverse graphics network and extracted background by masking out the object with Mask R-CNN. Then these properties are modified and passed to StyleGAN-R to synthesize new images different from the original image.

### Controlling Viewpoints

Previously, we found that some of latent codes of StyleGAN are associated with viewpoints of images. Therefore, to test whether only the viewpoint of images can be modified using StyleGAN-R, **only the viewpoint code is manipulated while keeping the content code fixed**. It turns out, StyleGAN-R can successfully generate images from multiple viewpoints, at the same time, keeping the content of images unchanged across them. The result is shown below.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig9.png" />
</center>
<span class="caption text-muted">Figure 9. <b>Camera Controller</b>.</span>

The qualitative comparison between this method - *using the mapping network which maps scene elements to proper latent codes for StyleGAN*, and an alternative approach - *direct optimization of latent code constrained on $L_{2}$ loss between images* is shown in the following figure:

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig10.png" />
</center>
<span class="caption text-muted">Figure 10. <b>Mapping Network vs Direct Latent Code Optimization</b>.</span>

As one can easily notice, the direct optimization of latent codes fails completely in viewpoints different from the original input viewpoint. This emphasizes the importance of the mapping network and the architecture fine-tuning exploiting the 3D inverse graphics network. This result implies that we need to associate GAN's latent code to image components in physically meaningful way, and this issue is well-resolved by introducing the mapping network in this case.

### Controlling Shape, Texture and Background

Similarily, by manipulating content code while keeping the viewpoint code frozen, we can change the appearance of the object depicted in the image. The result is shown in the following figure:

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig11.png" />
</center>
<span class="caption text-muted">Figure 11. <b>3D Manipulation via Content Code Modification</b>.</span>

### Real Image Editing

Also, thanks to the latent space formed by training StyleGAN on large number of image samples, StyleGAN-R outputs fine results in the application to real images as well.

<center>
    <img class="img-fluid" src="/assets/post-images/GANverse3D/fig12.png" />
</center>
<span class="caption text-muted">Figure 12. <b>Real Image Manipulation</b>.</span>

# Conclusion

This work presents a new powerful architecture that links two renderers: a state-of-the-art image synthesis network and a differentiable graphics renderer. By marrying the best of two worlds, the pipeline proposed in this work shows impressive results in various challenging tasks such as inverse graphics, neural rendering and manipulating existing real-world images.

However, there are plenty of rooms that we can expect improvements in follow-ups as well:

1. **The model fails to predict lighting correctly.** Effects related to lighting such as reflection, transparency, and shadows are predicted inaccurately using the spherical harmonic lighting model.
2. **Disentangling background from other image components is incomplete**, as one can notice that the modification of textures introduces slight changes in backgrounds as well.
3. **The model is also vulnerable to out-of-distribution objects** and suffers when predicting shapes of them.
</div>
:ET
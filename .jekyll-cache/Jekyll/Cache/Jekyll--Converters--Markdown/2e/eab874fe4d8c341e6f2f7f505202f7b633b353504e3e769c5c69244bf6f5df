I"ùl<h1 id="introduction">Introduction</h1>

<p>This post is written to serve as the minimal starting point, including basic concepts and ready-to-run code snippets, for you when integrating Weights &amp; Biases (I will call it W&amp;B after all) into your machine learning workflow.</p>

<p>As you can notice from the word ‚Äúminimal‚Äù, this post is just a very condensed summary covering only a small portion of the entire W&amp;B documentation. Therefore, I highly encourage you to check out <a href="https://docs.wandb.ai/">the full documentation</a> to learn more after finishing this.</p>

<p>In a nutshell, W&amp;B is <strong>a tool for experiment tracking, dataset versioning, and model management</strong> which we, ML researchers / engineers, do twenty-four-seven. If you are already familiar with TensorBoard, I bet you can easily get most of the contents and start writing codes immediately. Even if you aren‚Äôt, W&amp;B is quite straightforward to use yet incredibly powerful, so I strongly recommend you to take some time and go over this material (it won‚Äôt take that long!).</p>

<p>For explanation, and to give you a sense of adopting W&amp;B in real-world ML projects, I will use my implementation of <em>PointNet (PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, Charles R. Qi et al., CVPR 2017)</em> as an exempler. Note that all code snippets in this post are from my <a href="https://github.com/DveloperY0115/torch-pointnet"><em>torch-pointnet</em> repository</a>.</p>

<h1 id="initializing-wb">Initializing W&amp;B</h1>

<h2 id="sign-up--sign-in">Sign Up &amp; Sign In</h2>

<p>Don‚Äôt have an account for Weights &amp; Biases? You can easily make one, or directly link your Github account to it.</p>

<p>After signing up, you need to install <em>wandb</em> module to your Python environment. Everything we wil do after all assumes that you‚Äôve installed this module successfully. And in fact, this module is all you need. Installation is surprisingly simple - just run the following command in your shell.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>pip <span class="nb">install </span>wandb
wandb login
</code></pre></div></div>
<p>\</p>

<h2 id="starting-a-new-run">Starting a New Run</h2>

<p>If you have experience with TensorBoard, you would remember that we need to initialize a <em>SummaryWriter</em> object which is then used for logging throughout the rest of your program. W&amp;B has no difference, but is more elegant in some sense compared to TensorBoard. Running a single line of code will do everything for us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># imports
# import torch, numpy, etc
</span><span class="kn">import</span> <span class="nn">wandb</span>
<span class="c1"># ...
</span>
<span class="c1"># def main():
# bla bla bla
</span>
<span class="c1"># and many other functions
# bla bla bla
</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>

    <span class="c1"># initialize W&amp;B
</span>    <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"torch-pointnet"</span><span class="p">)</span>

    <span class="c1"># run main function
</span>    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>The above code snippet shows the overall structure of my <em>train.py</em> file, which defines various control flows involving the training of PointNet. As you can see, <em>wandb</em> is initialized before running the main function which contains a number of <em>wandb.log</em> (will be explained soon) calls for tracking important quantities (loss, accuracy, etc) during the experiment.</p>

<h1 id="logging--model-tracking">Logging &amp; Model Tracking</h1>

<h2 id="log-almost-everything-you-want">Log (Almost) Everything You Want</h2>

<p><em>wandb.log</em> is a function that we use for logging. You can log almost everything you can imagine in various ML workflows - numbers, images, audio, video, HTML, Histogram, 3D data, and much more. You can check the supported types <a href="https://docs.wandb.ai/guides/track/log">here</a>.</p>

<p>In my case, I use this for tracking:</p>

<ul>
  <li>Training loss</li>
  <li>Test loss</li>
  <li>Test accuracy</li>
  <li>Visualization of test result (for qualitative analysis)</li>
</ul>

<p>For instance, in the function <em>train_one_epoch</em> that carries out optimization for a single epoch, I simply write:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>

    <span class="c1"># forward propagation
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># back propagation
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># update
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># log data
</span>    <span class="n">wandb</span><span class="p">.</span><span class="n">log</span><span class="p">({</span><span class="s">"Train/Loss"</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">},</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
</code></pre></div></div>

<p>The quantity of interest is packed into a Python dictionary as a <em>(key, value)</em> pair where <strong>the key becomes the title of a plot</strong>, and <strong>the value represents the actual metric</strong> to be recorded. Notice how simply the logging works.</p>

<p>Of course, I can put as many kinds of data into a dictionary and log them at once.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">run_test</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>

    <span class="c1"># forward propagation
</span>
    <span class="c1"># compute metrics used for testing
</span>
    <span class="c1"># log data
</span>    <span class="n">wandb</span><span class="p">.</span><span class="n">log</span><span class="p">(</span>
        <span class="p">{</span><span class="s">"Test/Loss"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">"Test/Accuracy"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span> <span class="s">"Test/Result"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Image</span><span class="p">(</span><span class="n">fig</span><span class="p">)},</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>(<em>Disclaimer: Logging image data isn‚Äôt working as intended at the time I write this post. I‚Äôll try to resolve this issue ASAP.)</em></p>

<p>As a result, you can see this beautiful plot of training loss decreasing nicely over time. What makes this even cooler is that <strong>you can check the results in real time at the control panel through your web browser</strong> (both PC and mobile) without logging into your remote machine.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig1.png" />
</center>

<h2 id="anatomy-of-your-model-with-wb">Anatomy of Your Model with W&amp;B</h2>

<p>Having problem with exploding (or vanishing) gradients and don‚Äôt know which part<del>(s)</del> of your model is <del>(are)</del> in trouble? Using W&amp;B, you can easily diagnose the problem. To make W&amp;B pay attention to the internal of your model, simply wrap the model after initializing it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code>		
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># check GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

		<span class="c1"># model &amp; optimizer, schedulers
</span>    <span class="n">network</span> <span class="o">=</span> <span class="n">PointNetCls</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># make W&amp;B track model's gradient and topology
</span>    <span class="n">wandb</span><span class="p">.</span><span class="n">watch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">log_freq</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">log_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

		<span class="c1"># and the rest of main..
</span></code></pre></div></div>

<p>W&amp;B gives you the freedom of choosing which aspects of your network are going to be tracked. In the snippet above, I decided to record the gradients at all layers of my model every 100 steps and track the computational graph formed during training.</p>

<p>Just like logged metrics, this information will show up in your project dashboard.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig2.png" />
</center>

<h1 id="check-system-utilization">Check System Utilization</h1>

<p>You might want to check whether you‚Äôre pushing your machine to its limit. Thankfully, W&amp;B automatically tracks various statistics of the system throughout a session - GPU memory usage, power consumption, number of CPU threads in use, etc.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig3.png" />
</center>

<h1 id="hyperparameter-tuning-with-sweep">Hyperparameter Tuning with Sweep</h1>

<p>Personally, I think this is <strong>the coolest feature of W&amp;B</strong>. W&amp;B provides a powerful yet easy-to-use tool for hyperparameters tuning with fancy visualizations that helps users intuitively compare different combinations of them.</p>

<p>In this example, although my model has only few hyperparameters associated with its structure and training, I‚Äôll try to find the sweet spot for learning rate and batch size.</p>

<p>In order to find the optimal combination of hyperparameters, we need to tell W&amp;B,</p>

<ul>
  <li><strong>which hyperparameters to adjust</strong></li>
  <li><strong>what are the possible values (i.e. explorable space) for such hyperparameters</strong></li>
  <li><strong>which metric we want to opimize further through hyperparameter tuning</strong></li>
</ul>

<h2 id="tell-wb-a-list-of-adjustable-variables">Tell W&amp;B a list of adjustable variables</h2>

<p>In my case, I personally prefer using <em>argparse</em> for tweak variables for training. As you can see at the beginning of my <em>train.py,</em> I store the set of arguments in variable <em>args</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Parsing argument"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--beta1"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Beta 1 of Adam optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--beta2"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Beta 2 of Adam optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--lr"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Learning rate for optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--step_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Step size of StepLR"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--gamma"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Gamma of StepLR"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--num_epoch"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of epochs"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--num_iter"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of iteration in one epoch"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--batch_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Size of a batch"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--num_worker"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of workers for data loader"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"--out_dir"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"out"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Name of the output directory"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span>
    <span class="s">"--save_period"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of epochs between checkpoints"</span>
<span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div></div>

<p>Then I need to inform W&amp;B what the controllable variables are. All I need to do this is simply passing the set of parsed arguments at the time of initialization. Note that this is one way of doing this, and you can see other methods <a href="https://docs.wandb.ai/guides/sweeps/quickstart">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>

    <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"torch-pointnet"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>And in function <em>main</em>, I replace all occurences of <em>args.** to *config[</em>]<em>. Here, the object *config</em> holds the variables that we informed to W&amp;B at initialization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    
    <span class="n">config</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">config</span>

    <span class="c1"># ...
</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">"beta1"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s">"beta2"</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"lr"</span><span class="p">])</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"step_size"</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"gamma"</span><span class="p">])</span>

		<span class="c1"># ...
</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"batch_size"</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"num_worker"</span><span class="p">])</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"batch_size"</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

		<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">"num_epoch"</span><span class="p">]),</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

				<span class="c1"># rest of main..
</span></code></pre></div></div>

<h2 id="configure-sweep">Configure Sweep</h2>

<p>Next, I wrote a YAML file to specify the hyperparameters that I want to sweep over, the way how to explore the hyperparameter space, and the set of possible values for each hyperparameter.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">program</span><span class="pi">:</span> <span class="s">train.py</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">grid</span>
<span class="na">metric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">Test/Loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">lr</span><span class="pi">:</span> 
    <span class="na">values</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="m">0.1</span>
    <span class="pi">-</span> <span class="m">0.01</span>
    <span class="pi">-</span> <span class="m">0.001</span>
    <span class="pi">-</span> <span class="m">0.0001</span>
  <span class="na">batch_size</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="m">32</span>
    <span class="pi">-</span> <span class="m">64</span>
    <span class="pi">-</span> <span class="m">128</span>
    <span class="pi">-</span> <span class="m">256</span>
</code></pre></div></div>

<p>In particular, I‚Äôm telling W&amp;B:</p>

<ul>
  <li>that the training routine is defined at the file <em>train.py</em></li>
  <li>to use ‚Äúgrid‚Äù method (examine all possible combinations of hyperparameter values) for exploration</li>
  <li>to ‚Äúminimize‚Äù the metric ‚ÄúTest/Loss‚Äù. <strong>One important note is that you should log a quantity with name ‚ÄúTest/Loss‚Äù somewhere in the training routine.</strong> And I did it previously in the function <em>run_test</em>.</li>
  <li>variables <em>‚Äúlr‚Äù</em> and <em>‚Äúbatch_size‚Äù</em> are the ones that can be modified for each different run. And each of them has a set of possible values (e.g. learning rate (<em>lr</em>) can be one of 0.1, 0.01, 0.001, 0.0001)</li>
</ul>

<h2 id="initialize--run-sweep">Initialize &amp; Run Sweep</h2>

<p>After setting up a sweep configuration in <em>*.yaml</em> file, run the following command to initialize the sweep:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>wandb sweep <span class="k">*</span>.yaml
</code></pre></div></div>

<p>W&amp;B will automatically set up things for you and give you a <em>sweep ID</em> which specifies that exact sweep to be run. <strong>Copy that and use it in the next step.</strong></p>

<h2 id="launch-agents">Launch Agent(s)</h2>

<p>What makes sweep feature more powerful is that multiple machines or processes can contribute to the same sweep. This means, you can concurrently test different combinations of hyperparameters across your devices or processes in a single machine. As long as you share the same sweep ID, W&amp;B will distribute tasks to agents participating in the sweep and all you have to do is just waiting for the result.</p>

<p>The below image shows the result of each trial in the middle of sweeping (i.e. it was still on going). As soon as you see the plot at the bottom, you will immediately notice that the cases using large learning rate tend to give higher test losses while batch size seems to have no effect on test loss.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig4.png" />
</center>

<h1 id="summary">Summary</h1>

<p>In this post I briefly introduced only a few of, yet fundamental functionalities of W&amp;B. These include:</p>

<ul>
  <li>How to set up W&amp;B for your project</li>
  <li>Logging metrics and various kinds of data using <em>wandb.log</em></li>
  <li>Analyzing model with <em>wandb.watch</em></li>
  <li>Checking statistics of system usages collected by W&amp;B</li>
  <li>Sweep - a powerful way of tuning hyperparameters &amp; visualizing results to gain insights</li>
</ul>

<p>I hope this post was helpful &amp; comprehensible to you, would be glad if you can benefit from it, work in more productive way. Furthermore, as I mentioned in the introduction, be sure to check out the official documentation for more details and things that were not covered here.</p>
:ET
I"Jj<h1 id="motivations">Motivations</h1>

<ul>
  <li>Lack of models with photorealistic appearance in online 3D shape repositories <strong>‚Üí Increasing demand of repository containing photorealistic appearance model along with actual 3D shapes</strong></li>
  <li><strong>Given a 3D shape of a chair consists of several parts, how can we design an automated pipeline that assigns proper materials to each part and producing plausible output?</strong></li>
</ul>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_PhotoShapes.png" />
<span class="caption text-muted">Figure 1. <b>Fully automatic texturing of 3D shapes with SV-BRDF models</b>.</span></p>

<h1 id="Ô∏è-problem-settings-Ô∏è">‚ö†Ô∏è Problem Settings ‚ö†Ô∏è</h1>

<p><strong>Heads up, analyze the problem setting (input, processing step, output) of this study carefully!!</strong></p>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Propose an approach to <strong>automatically assign high-quality, realistic appearance models to large scale 3D shape collections</strong> ‚Üí Jointly leverage three types of online data - shape collections, material collections, and photo collections</li>
  <li><strong>Convolutional network</strong> used to <strong>classifying materials in real photos</strong>, then following <strong>3D-2D alignment technique transfer these materials to different parts of each shape</strong> model. ‚Üí Photorealistic, relightable, 3D shapes (PhotoShapes)</li>
  <li>The method of using photos of <em>real</em> objects as reference when assigning materials to each part of 3D shapes ‚Üí <strong>NOTE: This is not a texture transfer problem which directly transfers the visible appearance onto a given shape while inferring invisible parts.</strong></li>
  <li>As a result, a proposed system is able to produce about 11,000 PhotoShapes automatically</li>
</ul>

<h1 id="key-concepts">Key Concepts</h1>

<p>There are three types of datasets used in the paper - <strong>Shape, Photo, and Texture</strong> collections.</p>

<h2 id="3d-shape-collections">3D Shape Collections</h2>

<p>The 3D models to be textured are retrieved from <strong>ShapeNet</strong> and <strong>Herman Miller</strong> dataset. Specifically, <em><strong>5,740 3D models</strong> are from ShapeNet</em> while only <em><strong>90 models</strong> are from Herman Miller</em>. Most of them are furnitures such as chair, sofa, etc. Note that ShapeNet provides wide variety &amp; large number of shapes, while models from Herman Miller have higher quality as a trade-off for their rarity.</p>

<p>3D models in the dataset are in <strong>OBJ format</strong> and are <strong>segmented into parts</strong> as well. Note that those parts do not always correspond to semantic groups. ‚Üí ü§î Maybe we CANNOT exploit the segmentation information‚Ä¶? ü§î</p>

<p>In order to ensure quality of models in the datset, they undergo two preprocessing steps before added to the repository.</p>

<ol>
  <li><strong>Filtering</strong>: Manually remove 3D models that do not belong to the categories of our interest. Remove unrealistic shapes and shapes with poor quality as well. Next, <strong>vertex doubles are removed and smooth shading is enabled</strong>. Finally, these <strong>models are scaled to fit in a unit bounding cube</strong>.</li>
  <li><strong>UV Map Generation</strong>: Blender‚Äôs ‚ÄúSmart UV projection‚Äù algorithm is used to estimate UV map for each material segment.</li>
</ol>

<h2 id="photographic-references-for-photoshapes">Photographic References for PhotoShapes</h2>

<p>3D shapes are paired with photographic references, and these guide the appearance of PhotoShapes later. <strong>Total 40,927 product photos</strong> were collected through various media. Since most of product photos were taken with white background for better visibility, the objects and background are segmented nicely.</p>

<p>Also, to ensure that these images are unique, <strong>duplicates were removed by computing dense HOG features and compare the distance between features</strong> of images. If an L2 distance between two images is less than 0.1, one of them is regarded as duplicate and removed.</p>

<p>Also, <strong>foreground mask is computed</strong> for each image with a pixel value threshold. <strong>‚Üí Subtracting background of the image while leaving the object of interest</strong></p>

<p>Then, these <strong>images were then cropped</strong> with a square bounding box and resized to 1000 x 1000.</p>

<h2 id="material-collection">Material Collection</h2>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_material_examples.png" />
<span class="caption text-muted">Figure 2. <b>Examples of materials from the database</b>.</span></p>

<p><strong>Texture = SVBRDFs with spatially varying diffuse, specular and roughness maps</strong></p>

<p>Furthermore, textures often come with geometric information such as <strong>normal map</strong> and <strong>height map</strong> that can be used to model more realistic light-material interaction when an object is rendered by physically based renderer.</p>

<p>The authors manually captured SVBRDFs by scanning real surfaces, and collected synthetic textures from online repositories. More precisely,</p>

<ul>
  <li>33 manually scanned materials</li>
  <li>15 manually crafted metals and plastics</li>
  <li>34 materials provided in [Aittala et al., 2015]</li>
  <li>68 materials from [Poliigon 2018]</li>
  <li>57 materials from VRay Materials [Vray-materials.de 2018]</li>
  <li>238 materials from [Adobe Stock 2018]</li>
</ul>

<p>There are <strong>445 materials</strong> in total, forming $\mathcal{M}$, a set of labels of materials in the database.</p>

<p>Especially, the materials from [Aittala et al., 2015] was converted to an anisotropic Beckmann model so that they can be imported to Blender and rendered.</p>

<p><strong>Additional modifications</strong> were done on the materials collected such as:</p>

<ul>
  <li><strong>Normalizing Scale</strong>: Since scanned or crafted materials have arbitrary and unknown scale, <strong>a scale value $s_i$ is assigned for each material $m_i \in \mathcal{M}$.</strong> These scale factors are <strong>multiplied to the UV mappings as a factor of $\text{log}s_i$</strong> during rendering.</li>
  <li><strong>Environment Maps</strong>: The material collection contains a small set of 30 HDR environment maps from various sources. These are used to simuate studio-like lighting conditions.</li>
</ul>

<h2 id="shape-image-alignment">Shape-Image Alignment</h2>

<p>Here‚Äôs the problem setting: We want to <strong>synthesize realistic textured version of 3D shapes</strong> <strong>given a collection of uncorrelated 3D shapes and images of the same cateogry</strong>.</p>

<p>To this end, we want to develop a pipeline that first <strong>extract appearance information from reference images</strong> and then <strong>transfer that information onto the shape</strong> collection, end up assigning plausible materials in the database to each part of shapes.</p>

<p>Thus, such pipeline consists of two parts:</p>

<ol>
  <li><strong>Coarse step</strong>: Assigns to each shape a list of exemplars (images) and associated camera poses</li>
  <li><strong>Fine step</strong>: Creates a pixel-wise alignment between shapes and exemplars</li>
</ol>

<p>The following terminology will be used in the rest of our discussion.</p>

<ul>
  <li><strong>Shape</strong>: A 3D model obtained from an online shape collection, divided into <strong>object parts</strong> and <strong>material parts</strong></li>
  <li><strong>Object parts</strong>: Defines <strong>structural divisions</strong> (segmentation) of a shape</li>
  <li><strong>Material parts</strong> ($\mathcal{P}$): Defines <strong>which objects should share the same material</strong></li>
</ul>

<p>To texture a 3D shape, a plausible texture for that shape must be inferred by leveraging a set of associated image exemplars. Then, the quantitative measure of association between a shape and exemplar images should be calculated inside the pipeline. This task is called <strong>alignment</strong> and is broken down into two steps: 1) A <em>corse</em> step, 2) A <em>fine</em> step.</p>

<h3 id="coarse-step-shape-to-exemplar-matching">Coarse Step: Shape to exemplar matching</h3>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_index_from_image_to_shape.png" />
<span class="caption text-muted">Figure 3. <b>Index which maps a given image to a set of shapes</b>.</span></p>

<p><strong>Given a 3D shape</strong>, the pipeline first <strong>looks for proper exemplars for the geometry, as well as the camera pose</strong> for every such shape-exemplar pair. This can be stated as an image retrieval problem. For efficiency, this problem is solved by <strong>creating a reverse-index from exemplars to the top $k$ shape renderings</strong>. By inverting this index, we can find proper exemplars given a shape.</p>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_index_from_shape_to_image.png" />
<span class="caption text-muted">Figure 4. <b>Index which maps a given shape to a set of images</b>. Note that this index can be obtained by reversing the direction in the index shown in the previous figure.</span></p>

<p>To do this, <strong>each shape is rendered from various viewpoints</strong> sampled from a sphere around the object. The camera position is parametrized in spherical coordinates. Here, the elevation values are uniformly sampled in $\phi \in [\frac{\pi}{4}, \frac{9}{16}\pi]$, and azimuth values are sampled uniformly over $\theta \in [0, 2\pi)$. This gives us <strong>456 distinct viewpoints</strong>.</p>

<p>In order to match the reference images (i.e. exemplars from the database) and the rendered 3D images, a distance metric is used to measure how close two different images are. In this case the HOG descriptor from [Felzenszwalb et al., 2010] is used as $F$. Then this alignment problem can be addressed like the following:</p>

\[M(I) = \underset{\phi \in \Phi, \theta \in \Theta, s \in \mathcal{S}}{argmin} \vert\vert F(R(s; \phi, \theta) - F(I) \vert\vert\]

<p>where $M$ is a function that returns the top match for image query $I$, $R$ is the renderer used to render shape $s$ in the set of all shapes $\mathcal{S}$, with the elevation $\phi$ from the set of all possible elevations $\Phi$, as well as the azimuth $\theta$ which belongs to the set of all possible azimuths $\Theta$. Note that $\vert \Theta \times \Phi \vert = 456$ , from the previous discussion.</p>

<p>Calculating distance metric in feature space reduces computational cost significantly. While the raw image has resolution of $1000 \times 1000$, lying in $\mathbb{R}^{1 \times 10^6}$, the feature vector used here is only 1352 dimensional.</p>

<p>The input image is blurred with a Gaussian filter with $\sigma=0.1$ to compensate texture effects.</p>

<p>Furthermore, the rendered image $R(s; \phi, \theta)$ and the image $I$ are both cropped with a square bounding box around their foreground masks, as in the preprocessing step discussed earlier.</p>

<h3 id="fine-step-segmentation-refinement">Fine Step: Segmentation Refinement</h3>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_overall_refinement_pipeline.png" />
<span class="caption text-muted">Figure 5. <b>Overall refinement pipeline</b>.</span></p>

<p>Normally, 3D shapes are semented into both object and material parts by their authors. However, there‚Äôs no guarantee that these are authored properly though.</p>

<p>Here, we <strong>assume that any parts of the shape having the same material label share the same appearance</strong>. (same part shape ‚Üí same material assigned) Of course, one may exploit the segmentation as supervision, but this approach often over segments the object (both shape and material), and lacks the appearance symmetry found in various objects. (e.g. each leg of chair may be assigned different materials end up with awkward look)</p>

<p>From the result of coarse alignment (i.e. found several photos associated with given shape), we can compute a 2D material part labeling \(\textbf{p}_{\text{coarse}} \in \mathcal{P}^{N}\) for an image of size $N$ by projecting the shape parts \(\mathcal{P} = \{ p_1, p_2, \dots, p_{\vert \mathcal{P} \vert}\}\) with the estimated camera pose. (Note that each pixel is assigned a probability distribution which governs the probability that a pixel is of a specific material class)<strong>‚Üí We now seek correspondance between parts of a given shape and associated parts of the objects in reference images.</strong></p>

<p>Using <strong>simple, naive projecting of the coarsely aligned part mask is insufficient</strong> for our task. Since,</p>

<ul>
  <li>these masks doesn‚Äôt align perfectly, and</li>
  <li>thin structures may end up with zero overlap.</li>
</ul>

<p>Thus, we shall start from the coarse alignment and refine it to get clearer pixel-wise alignment of the projected part mask. Two steps involve in this particular task: ‚Üí In short, think of this as image segmentation task whose output will be used to <em>(1) identify corresponding materials per part</em>, <em>(2) and assign them properly to the target shape</em>.</p>

<ol>
  <li><strong>SIFT Flow</strong>: Compute a flow which warps the projected shape segment map onto the exemplar. This flow is computed by using the SIFT Flow algorithm on the silhouettes of each map.</li>
  <li><strong>Dense CRF</strong>: The SIFT Flow refinement results in an overlapping but noisy segmentation. Thus, this segmentation mask should further be cleaned up by using a dense pixel-wise CRF.</li>
</ol>

<p>After applying Dense CRF, the obtained part mask $\textbf{p}_{\text{crf}} \in \mathcal{P}^{N}$ is used for the rest of pipeline. This aligned part mask let us share information between shapes and corresponding image exemplars.</p>

<h3 id="substance-segmentation">Substance Segmentation</h3>

<p>The <strong>aligned image exemplar is used to infer <em>types</em> of materials, also known as <em>substances</em> for each part of the aligned object</strong> (e.g. matte material for arms, glossy leather material for seat, etc). ‚Üí Note that <em>substance</em> is more general concept than <em>material</em> is. For example, different materials such as ‚Äòoak‚Äô, ‚Äòcherry‚Äô, and ‚Äòmaple‚Äô is a subclass of substance ‚Äòwood‚Äô.</p>

<p>After that, these substances will be converted into fine-grained SV-BRDFs. The image will be segmented and each pixel will be labeled with a substance category such as ‚Äòleather‚Äô, ‚Äòfabric‚Äô, ‚Äòmetal‚Äô, wood‚Äô, and ‚Äòplastic‚Äô. Similar categories are mapped to a canonical category (e.g. ‚Äòcarpet‚Äô ‚Üí ‚Äòfabric‚Äô).</p>

<p>By this assignment, all <em>other</em> category probabilities are set to zero, and the remainder <strong>are</strong> re-scaled to make a proper probability distribution (= normalization). This process results in a substance mask $\textbf{q} \in \mathcal{Q}^N$ where \(\mathcal{Q} = \{ q_1, q_2, \dots, q_{\vert\mathcal{Q}\vert}\}\) is the set of substance labels.</p>

<p>A substance labeling of <strong>the shape (not a pixel) $\textbf{q}_{\text{shape}} \in \mathcal{Q}^{\vert \mathcal{P} \vert}$</strong> is calculated by the substance label that has the most overlap. <strong>‚Üí Assigning proper material to each classe of segment that shares the same material</strong></p>

<h2 id="image-segment-to-sv-brdf">Image Segment to SV-BRDF</h2>

<p>Previous methods mostly exploit extracted planar patches and optimize textures. However, extracting local planar patches from images yields low resolution, distorted textures and often loses global context which is useful for inferring glossiness.</p>

<p>Instead, since we have segmented each part of the object and associated material classes together, this problem can be addressed as a classification. <strong>‚Üí Which class of material is proper for this segment?</strong></p>

<p>The desired input &amp; output of the system is the following:</p>

<ul>
  <li><strong>Input</strong>: An image and a corresponding binary mask representing a single material</li>
  <li><strong>Output</strong>: A material label $m \in \mathcal{M}$ chosen from the collection of SV-BRDFs</li>
</ul>

<p>It would be great to have real world data with ground truth SV-BRDF labelings, but it‚Äôs not how reality works. Thus, we instead create synthetic data where the ground truth SV-BRDF labels are known.</p>

<h3 id="synthesizing-training-data">Synthesizing Training Data</h3>

<p>Using the 3D shape and material databases, a large amount of training data can be created by applying different materials to shapes and rendering it with various camera intrinsic &amp; extrinsic under different illuminations.</p>

<ul>
  <li><em>Camera Pose Prior</em>: Real product images (especially photographs created for commercial purpose) have strong bias in camera poses. Therefore, <strong>parameters for camera poses are sampled uniformly from the distribution of camera poses obtained in the coarse alignment step</strong>.</li>
  <li><em>Substance Prior</em>: Substances do not occur randomly in objects. To enforce a substance prior, the shape substance labelings \(q_{p_i} \in \textbf{q}_{\text{shape}}\) from substance segmentation (section 4.3) is used. <strong>Instead of sampling totally random material for certain part, we condition it on the substance category and sample</strong> $m_{p_i} \sim \mathcal{U}({ m \vert m \in \mathcal{M}, q_m = q_{p_i} })$.</li>
  <li><em>Texture Scale Normaliztion</em>: Since different tessellation and UV mappings can arbitrarily change the rendered scale of textures, we first normalize the UV scale for each mesh segment \(\mathcal{S}_i\) by computing a density $D_i=A_i^{\text{UV}} / A_{i}^{\text{World}}$  where $A_i^{\text{UV}}$ is the local UV-space surface area of the mesh and $A_{i}^{\text{World}}$ is the local world-space surface area.  The UV coordinates for the segment is then scaled by $\frac{1}{D_i}$. <strong>‚Üí In my understanding, the modification of object mesh in renderer might lead to wrong texturing, thus this scale factor compensates that effect</strong></li>
  <li><em>Randomized Rendering</em>: To generate a single random rendering, we first uniformly sampe a shape-exemplar pair computed during coarse step (section 4.1). Given such pair, we
    <ol>
      <li><strong>sample a camera pose</strong> from the distribution computed above (see <em>Camera Pose Prior</em>)</li>
      <li><strong>assign a random material</strong> (SV-BRDF) to each shape part conditioned on the substance label (see <em>Substance Prior</em>) computed in substance segmentation (section 4.3)</li>
      <li><strong>Select a random environment map</strong> from the database to approximate various global illuminations</li>
    </ol>
  </li>
</ul>

<p>To make a classifier more robust, we can augment the synthetic data by:</p>

<ol>
  <li>randomly jittering azimuth and elevations,</li>
  <li>randomly select a FoV of camera,</li>
  <li>randomly select a camera distance,</li>
  <li>randomly scale the radiance of the environment map (adjust lighting),</li>
  <li>randomly scale, rotate, and translate UV mappings,</li>
</ol>

<p>respectively.</p>

<p>The rendering is done using Blender, and <strong>total 156,262 synthetic images</strong> were generated.</p>

<p><img class="img-fluid" src="assets/post-images/PhotoShape/PhotoShape_pipeline_overview.png" />
<span class="caption text-muted">Figure 6. <b>Pipeline overview</b>.</span></p>

<h3 id="material-classification">Material Classification</h3>

<p>Note that the synthetic dataset is generated by conditioning on substances as well as other factors. One desirable feature when generating PhotoShapes is that each part is assigned accurate, fine-grained <em>materials.</em> Even though the selection of material is conditioned on the substance category that a given object part is in, <strong>the renderings of the synthetic dataset contain ground truth labels for which specific <em>material</em> is rendered at each pixel</strong>. (e.g. This pixel is from ‚Äòoak‚Äô, that pixel is from ‚Äòmetal‚Äô, etc)</p>

<p>These renderings with ground truth material labels are used to <strong>train a classifier which predicts which materials are present in a given image</strong>. While classical brute-force approaches such as color histogram are applicable, it turned out such method is not suitable for large database like PhotoShape dataset. Therefore, the classifier is designed as a feed-forward neural network.</p>

<p><img class="img-fluid" src="/assets/post-images/PhotoShape/PhotoShape_material_classification.png" />
<span class="caption text-muted">Figure 7. <b>The top-3 material predictions of the material classification network</b>. (a) shows the predictions when trained without substance supervision, whereas (b) shows the predictions when trained with substance supervision.</span></p>

<p>The input to the classifier is <strong>an image exemplar + a binary segmentation mask (as one can see in the above figure, where input image and a binary mask is paired)</strong>. Here, the input mask represents the portion of the image that is to be classified.</p>

<p>Then the output of the classifier is an $\vert \mathcal{M} \vert$ dimensional vector $\text{x}^{m}$ which becomes a discrete probability mass function after going through softmax layer. This distribution is then compared with the ground truth resulting in the form of cross entropy loss:</p>

\[\mathcal{L}_{\text{mat}} (\textbf{x}^{m}, \mathcal{y}_i^{m}) = - \text{log}(\frac{\exp x_{i}^{m}}{\sum_{j} \exp x_{j}^{m}})\]

<p>where $\mathcal{y}_{i}^{m}$ is the ground truth label. This loss function is then optimized throughout the training.</p>

<p>However, this naive approach - hoping for the network to classify the material in the given image to proper class - leads to poor results especially in the experiments using real images (i.e. the model lacks generalizability) Therefore, additional constraints must be added on top of $\mathcal{L}_{\text{mat}}$ for better precision.</p>

<p>One intuitive regularization one can think of is <strong>a distance metric, which responds sensitively to totally wrong material assignments</strong>. For example, it‚Äôs less wrong to classify ‚Äòbeech wood‚Äô as ‚Äòcherry wood‚Äô since they share the same substance category ‚Äòwood‚Äô anyway. However, classifying any wooden material to some metalic classes should be penalized severely.</p>

<p>Therefore, <strong>an additional fully connected layer is added to the network, and its task is to predict the substance category</strong> $q \in \mathcal{Q}$ of an object part in the input image designated by the binary mask. The loss for this network can be defined in a similar manner:</p>

\[\mathcal{L}_{\text{sub}} (\text{x}^{s}, \mathcal{y}_{i}^{s}) = - \log(\frac{\exp x_{i}^{s}}{\sum_j \exp x_{j}^{s}})\]

<p>where $\mathcal{y}_{i}^{s}$ is the ground truth label.</p>

<p>There are many possible ways to form the final loss by combining two losses introduced so far.</p>

<p>The most straightforward way to achieve this is to compute a weighted sum of loss functions:</p>

\[\mathcal{L} (\text{x}, \text{y}; \lambda) = \mathcal{L}_{\text{mat}} (\text{x}^{m}, \text{y}^{m}) + \lambda \mathcal{L}_{\text{sub}} (\text{x}^{s}, \text{y}^{s})\]

<p>for some weighting coefficient $\lambda$. However, the authors of the paper found it more efficient to use the uncertainty weighted multitask loss from [Kendall et al., 2018] which defines a weighting based on learned homoscedastic (task-dependent, but not data-dependent) uncertainties $\hat{\sigma}<em>{m}^{2}$, $\hat{\sigma}</em>{s}^{2}$. To summarize, the finalized form of the loss function is:</p>

\[\mathcal{L} (\text{x}, \text{y}; \hat{\sigma}) = \mathcal{L}_{\text{mat}} (\text{x}^{m}, \text{y}^{m}) \hat{\sigma}_{m}^{-2} + \log \hat{\sigma}_{m}^{2} + \mathcal{L}_{\text{sub}} (\text{x}^{s}, \text{y}^{s}) \hat{\sigma}_{s}^{-2} + \log \hat{\sigma}_{s}^{2}\]

<p>Since the coefficients with negative exponent may cause divide-by-zero or any kinds of numerical unstability, the following form is used instead, by optimizing log variance $\hat{s} \coloneqq \log \hat{\sigma}^{2}$:</p>

\[\mathcal{L} (\text{x}, \text{y}; \hat{\text{s}}) = \mathcal{L}_{\text{mat}} (\text{x}^{m}, \text{y}^{m}) \exp (- \hat{s}_{m}) + \hat{s}_{m} + \mathcal{L}_{\text{sub}} (\text{x}^{s}, \text{y}^{s}) \exp (- \hat{s}_{s}) + \hat{s}_{s}^{2}\]

<p>The initial values for uncertainties are $\hat{s}<em>{m} = 0.0$, $\hat{s}</em>{s} = -1.0$, respectively.</p>

<h2 id="implementation--experimental-results">Implementation &amp; Experimental Results</h2>

<h3 id="model-architecture-and-training">Model Architecture and Training</h3>

<p>The <strong>ResNet-34 pretrained on ImageNet dataset</strong> is used. Additionally, along with the RGB channels, <strong>the fourth alpha channel is added to the input layer of the network, which is used to represent the segment of the image we wish to classify</strong>. Then, the filters associated with this new channel is initialized with a random Gaussian.</p>

<p>The output of the network is a score for each of the $C+1$ categories, and the additional 1 slot is for background (not in materials).</p>

<p>The network is trained with stochastic gradient descent (SGD) with a fixed learning rate of 0.0001 until convergence.</p>

<p>In addition, as other image classfiers normally do, data is augmented before passed to the network:</p>

<ul>
  <li>random rotations</li>
  <li>random crops</li>
  <li>random aspect ratio changes</li>
  <li>random horizontal flips</li>
  <li>random chromatic/brightness/contrast shifts</li>
</ul>

<h3 id="pretraining-on-real-images">Pretraining on Real Images</h3>

<p>Along with synthetic images described previously, <strong>it‚Äôs good for the network to have natural image supervision for better generalizability</strong>. The network is pre-trained using the dataset from [Bell et al., 2015] using their ground truth region polygons to generate input masks. Also, since most of images in synthetic dataset have white background, so the real world data should be interleaved with cropped inputs with white backgrounds.</p>

<p>OpenSurfaces dataset (dataset of surface, material, and texture) is randomly splitted into training and validation sets as a ratio of 9:1. And the network, already pretrained on ImageNet, is then fine-tuned and trained using only the substance task with a learning rate of 0.0001 until convergence.</p>

<h3 id="inference">Inference</h3>

<p>Given an image, we take the segmentation mask and infer a material for each segment using the network introduced previously.</p>

<h3 id="generating-photoshapes">Generating PhotoShapes</h3>

<p>The ultimate goal of this work is to create a collection of photorealistic, relightable 3D shapes. Then, imagine that we‚Äôre given the collection of all aligned shape-exemplar pairs. We say that these shapes become PhotoShapes when each of their material parts are assigned a relightable texture.</p>
:ET
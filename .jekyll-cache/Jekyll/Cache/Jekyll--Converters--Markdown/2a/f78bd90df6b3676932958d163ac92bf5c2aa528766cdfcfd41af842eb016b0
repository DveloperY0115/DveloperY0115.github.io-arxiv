I"CP<h1 id="motivations">Motivations</h1>

<ul>
  <li>While the resolution and quality of images generated by GANs greatly improved in recent years, the internal mechanism of generators is still undiscovered despite numerous recent efforts to better understand the way how images are synthesized.</li>
  <li>Besides, the properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations provide no quantitative way to compare different generators against each other.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>A novel architecture, inspired by style transfer literature, that automatically learns unsupervised separation of high-level attributes and is able to introduce stochastic variation in the generated images, and enables intuitive, scale-specific control of the synthesis.</li>
  <li>The new generator improves the state-of-the-art in terms of traditional metrics, and also outperforms others when it comes to qualitative comparisons.</li>
  <li>Moreover, the generator better disentangles the latent factors of variation.</li>
  <li>Besides, the authors suggest two new, automated methods - perceptual path length and linear separability - to quantify interpolation quality and disentanglement.</li>
  <li>Last but not least, the authors introduce a new, highly varied and high-quality dataset of human faces.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This work proposes StyleGAN, a novel generative adversarial network architecture inspired by studies on style transfer which generates images by gradually adjusting ‚Äòstyle‚Äô of them at each convolution layer thereby automatically learns to separate high-level image attributes without any supervision.</p>

<h1 id="methods">Methods</h1>

<p><strong>This work doesn‚Äôt introduce any modifications to the discriminator or the loss function.</strong> Thus, this work has nothing to do with the ongoing discussion about GAN loss functions, regularization, and hyperparameters.</p>

<h2 id="style-based-generator">Style-based generator</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>Overview of Style-based generator</b>.</span></p>

<p>What generators commonly do is to provide the latent code to the input layer in the beginning. The newly proposed generator has different design which differentiate it from typical generator networks. Instead of input layer that takes the latent, <strong>the generator starts from a learned constant instead.</strong></p>

<p>Given a latent code $\textbf{z}$ in the input latent space $\mathcal{Z}$, a non-linear mapping network $f: \mathcal{Z} \to \mathcal{W}$ first maps it to $\textbf{w} \in \mathcal{W}$. This mapping network $f$ is implemented using an 8-layer MLP and the dimensionality of input and output feature vector ($\textbf{z}$ and $\textbf{w}$) are equally set to 512.</p>

<p>Then the learned affine transformations specialize $\textbf{w}$ to <em>styles $\textbf{y} = (\textbf{y}_{s}, \textbf{y}_{b})$</em> that control <em>adaptive instance normalization (AdaIN)</em> operations after each convolution layer of the synthesis network $g$. The AdaIN operation is defined as follows:</p>

<p>$$\text{AdaIN} (\textbf{x}_{i}, \textbf{y}) = \textbf{y}_{s, i} \frac{\textbf{x}_{i} - \mu(\textbf{x}_{i})}{\sigma(\textbf{x}_{i})} + \textbf{y}_{b, i},$$</p>

<p>where each feature map $\textbf{x}_{i}$ is normalized separately, and then scaled and biased using the corresponding scalar components from style $\textbf{y}$. This implies that each $\textbf{y}_{s}$, $\textbf{y}_{b}$ has to have the same dimensionality as $\textbf{x}_{i}$, meaning that the overall dimensionality of $\textbf{y}$ is twice the number of feature maps on that layer.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Uncurated set of images generated by the style-based generator</b>.</span></p>

<p>Apart from fusing style information into an intermediate image (i.e. the image which will eventually become the output of the generator), the generator receives <em>noise inputs</em> before &amp; after each convolution layer to introduce stochastic details. The noises are in fact single-channel images consisting of uncorrelated Gaussian noise, and injected to each layer of the synthesis network. As one can see in the overall architecture these single channel images are first broadcasted to all feature maps using learned per-feature scaling factors and then added to the output of the convolution.</p>

<h3 id="truncation-trick-in-mathcalw-optional">Truncation trick in $\mathcal{W}$ (Optional)</h3>

<p>Poorly represented distribution in regions with low density (i.e. rare data) can be a problem when training the generator. This is still an open problem in all generative models, so the generator in this work is not an exception. However, it‚Äôs known that sampling latent vectors from a truncated (or shrunk) sampling space tends to improve the quality of generated image, although the loss of some amount of variation is inevitable trade-off.</p>

<p>That being said, we can use a similar approach. Instead of using $\textbf{w} = f(\textbf{z})$ directly, we can compute the center of mass of $\mathcal{W}$ as $\bar{\textbf{w}} = \mathbb{E}_{\textbf{z} \sim P(\textbf{z})} \big[ f(\textbf{z}) \big]$. Then, we can scale the deviation of a given $\textbf{w}$ from the center as $\textbf{w}^{\prime} = \bar{\textbf{w}} + \psi (\textbf{w} - \bar{\textbf{w}})$, where $\psi &lt; 1$.</p>

<p>While some studies argue that only a subset of networks (even with orthogonal regularization due to unstability!) can benefit from the truncation , truncation in $\mathcal{W}$ seems to work reliably even without changes to the loss function.</p>

<h2 id="properties-of-the-style-based-generator">Properties of the Style-based Generator</h2>

<p>The generator architecture enables us to control the image synthesis via scale-specific modifications to the styles. We can think of the mapping network and each learned affine transform as a way to draw samples for each style from certain distribution. And the role of synthesis network can be thought as appropriately fusing these styles together to generate novel images. We suspect that the effects of each style are localized in the network. That is, <strong>modification on a specific subset of the styles affects only certain aspects of the image.</strong></p>

<p>One possible hint for this localization is in the way how AdaIN operation works. It first normalizes each channel to zero mean and unit variance, and then applies scales and biases based on the style. This changes the per-channel statistics, and eventually modifies the relative importance of features for the subsequent convolution operation. Note that such features do not depend on the original statistics since the normalization has wiped out the original statistics. In other words, <strong>each style controls only one convolution before the next AdaIN operation overrides statistics of feature maps.</strong></p>

<h3 id="style-mixing">Style Mixing</h3>

<p>‚Äú<strong>Each style code contributes to different high-level image attribute.‚Äù</strong></p>

<p>In order to encourage the localization of each style, authors used <em>mixing regularization</em> which is a method to generate some portion of output images from not only one latent code, but two during training. An operation for such regularization is called <em>style mixing</em>, which simply switches one latent code to another at a randomly selected point in the synthesis network (i.e. the injected code changes at some point in the synthesis process).</p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Uncurated set of images generated by the style-based generator</b>.</span></p>

<p>Figure 3. <strong>Examples of style mixing at various scales</strong>. Observe how each subset of styles controls meaningful high-level attributes of the image.</p>

<p>Specifically, with <em>style mixing</em>, <strong>two latent codes $\textbf{z}<em>{1}$, $\textbf{z}</em>{2}$ are transformed by the mapping network and turned into $\textbf{w}<em>{1}$, $\textbf{w}</em>{2}$, respectively. Until some randomly picked crossover point (say, the fourth block of synthesis network), $\textbf{w}<em>{1}$ control styles and $\textbf{w}</em>{2}$ governs the style manipulation after that. This technique prevents the network from assuming that adjacent styles are correlated. **‚Üí ü§î  Does ‚Äúadjacent style‚Äù mean the order of injected style codes?</strong></p>

<h3 id="stochastic-variation">Stochastic variation</h3>

<p>‚Äú<strong>Random noises introduced during synthesis affect appearance attributes that are confined to local region and has inherent stochasticity.‚Äù</strong></p>

<p><img src="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_16.22.28.png" alt="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_16.22.28.png" /></p>

<p>Figure 4. <strong>Examples of stochastic variation</strong>.</p>

<p>One important observation from looking at various human portraits is that randomness involves in determining lots of aspects of images from the placement of hairs, stubble, freckles, or skin pores. Choice for such appearance aspects can be randomized without affecting our perception of the image as long as they follow the correct distribution (e.g. putting freckles in the eyes would leads to very weird images). Then what is the correct way to introduce stochasticity to images correctly?</p>

<p>The reason why traditional generators often fail to model plausible randomness is because of the input layer of their architectures. Compared to the style-based generator proposed in this work, the only input to such generators is single latent vector which will eventually be decoded to the (hopefully, photorealistic) image.</p>

<p><img src="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_16.27.58.png" alt="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_16.27.58.png" /></p>

<p>Figure 5. <strong>Effect of noise inputs at different layers of the generator</strong>.</p>

<p>However, since that is the only input to the network, it needs to somehow find a way to generate spatially-varying pseudorandom numbers from earlier activations whenever they are needed. Such randomness introduced implicitly by the network itself is not powerful enough to give rise to natural patterns instead leads to repetitive patterns in generated images. <strong>To address this issue, this work suggests to add per-pixel noise after each convolution so the randomness is now explicitly handled at each style level.</strong></p>

<p>Each portrait in figure 4 (middle column) is generated from the same underlying image (same constant initialization, same latents, same affine transforms to obtain styles) but with different noises applied during synthesis. Surprisingly, the introduced noises affect only the stochastic aspects (e.g. different placements of hairs) of images not the global context (e.g. overall composition, identity intact, etc). Furthermore, the figure 5 shows the effect of applying stochastic variation to different subsets of layers.</p>

<h3 id="separation-of-global-effects-from-stochasticity">Separation of Global Effects from Stochasticity</h3>

<p>What we‚Äôve been discussing can be summarized into two simple facts:</p>

<ol>
  <li>Changing style effects the synthesized image globally.</li>
  <li>Noises injected to each convolution layer affect only inconsequential random variation.</li>
</ol>

<p>This discovery is well-aligned with style transfer literature which have been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc) encode the style of an image (global) while spatially varying features encode a specific instance (local). These observations in style transfer literature can explain why changing style code, noise behaves differently.</p>

<p>In the style-based generator, the style can affect the entire image because of AdaIN operation, which shifts the distribution of all feature maps by uniformly scaling and adding biases to them. Such operation is spatially invariant, thus leads to the modification of global attributes like pose, lighting, or background style. In contrast, the noise (per-pixel Gaussian) is added independently to each pixel thus it varies over the spatial domain. Thus, in theory, it‚Äôs suitable for controlling stochastic variation.</p>

<h2 id="disentanglement-studies">Disentanglement Studies</h2>

<p><img src="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_17.24.47.png" alt="A%20Style-Based%20Generator%20Architecture%20for%20Generativ%20c7bb35b50379439786e904f58832b4b4/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-08-12_17.24.47.png" /></p>

<p>Figure 6. <strong>Illustration of latent space of image features</strong>. Mapping from initial latent space to image features is nonlinear, thus one cannot guarantee that changing the single axis of latent code will affect only one image attribute. Meanwhile, the mapping between intermediate latent space and image features are (hopefully) linear, so that one can expect modification in single image attribute when exploring the latent space along specific axis.</p>

<p>The goal of disentanglement is to <strong>find a latent space that consists of linear subspaces, each of which controls only one factor of variation</strong> so that we can modify only one aspect of images while exploring the space along specific direction.</p>

<p>However, we cannot easily do this in $\mathcal{Z}$ since it needs to match the corresponding density in the training data. This means that each axis of $\mathcal{Z}$ is not well-aligned to perceptually meaningful image features hindering the complete disentanglement that we desire.</p>

<p>Fortunately, the style-based generator circumvents this issue by introducing a mapping network $f$ which maps the latent space $\mathcal{Z}$ to the intermediate latent space $\mathcal{W}$ that doesn‚Äôt have to support sampling according to any <em>fixed</em> distribution (e.g. distribution of training data). Rather, it‚Äôs the ‚Äúunwarped‚Äù space consisting of vectors $\textbf{w}$ transformed from $\textbf{z}$ by <em>learned</em> piecewise continuous mapping $f$. <strong>This ‚Äúunwarping‚Äù $\mathcal{W}$ makes the factors of variation become more linear making it easier to identify each important feature along each dimension</strong> of the space. And the authors expected $f$ to be trained in a way that it unwarps the space well achieving nice disentanglement of features.</p>

<p>While there has been numerous metrics suggested to quantify the degree of disentanglement, they cannot be directly applied to this work since they require a pipeline with an encoder network which maps input images to latents. As introduced earlier, the overall structure of GAN used as a baseline employed encoder-less architecture making it impossible to compare this method to others. Also, it‚Äôs definite waste of time and resources to add an extra encoder network in the architecture that is not directly related to the problem.</p>

<p>Thus, <strong>the authors present two new ways of quantifying disentanglement neither of which requires an encoder or known factors of variation</strong> (i.e. which part of latent contributes to which image attribute).</p>

<h3 id="perceptual-path-length">Perceptual Path Length</h3>

<p>Interpolation of latent vectors lying in latent space sometimes leads to surprising non-linear changes in the image. To be more specific, features that did not appear in either endpoint may appear in the middle of a linear interpolation path. <strong>This is a strong evidence that the latent space is entangled and the factors of variation are not separated.</strong></p>

<p>To quantify this phenomena, one can come up with an idea to measure how the image changes as interpolation is performed in the latent space. One good intuition to start from is that the more curvy the latent space is, the more drastic change will be introduced to the image during the latent space exploration.</p>

<p>That being said, the authors propose a metric which uses a perceptually-based pairwise image distance as a basis. Such distance can be computed as a weighted difference between two VGG16 embeddings, where the weights are designed to match the similarity difference perceived by human.</p>

<p>Then, the length of path drawn by interpolated latent vector can be defined mathematically as the limit of the sum of the length of line segments where the length of each segment goes to infinitesimal. Thus, the average perceptual path length in latent space $\mathcal{Z}$, over all possible endpoints can be defined as follows:</p>

<p>$$l_{\mathcal{Z}} = \mathbb{E} \Big[ \frac{1}{\epsilon^{2}} d \big(  G(\text{slerp}(\textbf{z}<em>{1}, \textbf{z}</em>{2}; t)), G(\text{slerp}(\textbf{z}<em>{1}, \textbf{z}</em>{2}; t + \epsilon))\big)\Big],$$</p>

<p>where $\epsilon = 10^{-4}$,  $\textbf{z}<em>{1}$, $\textbf{z}</em>{2} \sim P(\textbf{z})$, $t \sim U(0, 1)$, $G = g \circ f$ is the generator, and $d(\cdot, \cdot)$ evaluates the perceptual distance between the resulting images. Here, $\text{slerp}$ denotes spherical interpolation used to interpolate between normalized input latent vectors.</p>

<p>Similarily, one can define the average perceptual path length in $\mathcal{W}$ as well.</p>

\[l_{\mathcal{W}} = \mathbb{E} \Big[ \frac{1}{\epsilon^{2}} d \big( g (\text{lerp} (f(\textbf{z}_{1}), f(\textbf{z}_{2}); t)), \\
g (\text{lerp}(f(\textbf{z}_{1}), f (\textbf{z}_{2}); t+\epsilon))\big) \Big],\]

<p>where the only difference is that interpolation is carried out in $\mathcal{W}$. Since vectors in $\mathcal{W}$ are not normalized, linear interpolation ($\text{lerp}$) is used instead of $\text{slerp}$.</p>

<h3 id="linear-separability">Linear Separability</h3>

<p>Furthermore, if a latent space is well-disentangled, it should be possible to find direction vectors that consistently correspond to individual factors of variations (e.g. modifying the first and second dimension of $\textbf{w}$ changes camera pose).</p>

<p>Thus, another metric proposed by the authors quantifies this effect. Evaluation of such metric is done by measuring how well the vectors in the latent space can be separated into two distinct sets, say $S$ and $S^{C}$ via a linear hyperplane - For example, suppose that we have found a hyperplane which perfectly divides the latent space into two subspaces, so that all vectors from one set are decoded to portraits of men, while the vectors in its complement are decoded to photographs of women.</p>

<p>To label the generated images, the authors trained auxiliary classification networks for a number of binary attributes (e.g. binary classification on male and female faces). These networks are trained to classify images generated from $\textbf{z} \sim P(\textbf{z})$ with respect to certain predefined attributes. Then the authors utilize SVM to find proper hyperplanes lying in the latent space $\mathcal{Z}$, that best separate the generated images $G(\textbf{z})$ in RGB space with respect to the selected attributes (actually, the images are already labeled, so it‚Äôs nothing but a simple classification using ML).</p>

<h1 id="conclusion">Conclusion</h1>

<p>The newly proposed style-based generator outperforms the prior works in qualitative comparisons, as well as in quantitative evaluations including the novel metrics suggested by the authors.</p>
:ET
I"|N<p><img class="img-fluid" src="/assets/post-images/NeuTex/NeuTex_overview.png" />
<span class="caption text-muted">Figure 1. <b>NeuTex overview</b>. NeuTex separates the volumetric 3D geometry and the surface appearance represented as 2D UV map.</span></p>

<h1 id="motivation">Motivation</h1>

<ul>
  <li>Volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering on scene having high-complexity. However, <strong>these methods entangle geometry and appearance, thus scene is not editable.</strong></li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>An approach that explicitly <strong>disentangles geometry</strong> (represented as a continuous 3D volume) <strong>from appearance</strong> (represented as a continuous 2D texture map).</li>
  <li>Introducing a 3D-to-2D texture mapping (or surface parametrization) network into the pipeline.</li>
  <li>2D-to-3D inverse mapping and a novel cycle consistency loss to ensure consistent matching (similar to one-to-one correspondance) between a surface point in 3D and a point on UV map, which is a subset of 2D space.</li>
  <li>As a result of separating geometry and appearance, one can edit appearance by editing 2D texture maps extracted during the training.</li>
</ul>

<h1 id="key-concepts">Key Concepts</h1>

<h2 id="tldr">TL;DR</h2>

<p><strong>NeuTex</strong> is a neural scene representation that represents <strong>geometry as a 3D volume</strong> but <strong>appearance as 2D neural texture in an automatically discovered texture UV space</strong>.</p>

<p>Basic idea still remains the same. In order to render the scene, sampling point in 3D space by ray marching and accumulating per-point radiance values must be done.</p>

<p>However, while NeRF uses a single MLP to regress both density and radiance in 3D volume, NeuTex has 4 networks which disentangles radiance from geometry:</p>

<ol>
  <li>\(F_{\text{uv}}\): For mapping 3D coordinate to a 2D coordinate in UV space</li>
  <li>\(F^{-1}_{\text{uv}}\): As the inverse of \(F_{uv}\) to exploit the cycle loss to enforce consistency</li>
  <li>\(F_{\text{tex}}\): A network which maps UV coordinate to actually texture RGB color in color space</li>
  <li>\(F_{\sigma}\): A network inferring density (i.e. geometry) given a point in 3D space</li>
</ol>

<p>While NeuTex is more constrained than a fully-volumetric method and outputs slightly worse results, this approach is still able to perform photorealistic novel view synthesis. Most importantly, <strong>this is the first work introducing functionality for editing appearance of the object.</strong></p>

<p><img class="img-fluid" src="/assets/post-images/NeuTex/NeuTex_method_overview.png" />
<span class="caption text-muted">Figure 2. <b>Method overview</b>. This work presents a disentangled neural representation consisting of multiple MLPs for neural volumetric rendering.
</span></p>

<h2 id="disentangled-neural-scene-representation">Disentangled neural scene representation</h2>

<h3 id="volume-rendering">Volume Rendering</h3>

<p>Volume rendering requires volume density $\sigma$ and radiance $\textbf{c}$ at all 3D locations in a scene. Then a pixel’s radiance value (RGB) $\textbf{I}$ is computed by ray marching algorithm, which samples points in 3D space along a specific ray. Specifically,</p>

\[\textbf{I} = \sum_{i} T_i (1 - \text{exp}(- \sigma_i \delta_i)) \textbf{c}_i, \\

T_i = \text{exp}(- \sum_{j=1}^{i-1} \sigma_j \delta_j)\]

<p>where $i = 1, \dots, N$ denotes the index of a shading point on the ray, $\delta_i$ represents the distance between two consecutive points, $T_i$ is known as the transmittance, and $\textbf{c}_i$ and $\sigma_i$ are the radiance and volume density at $i$-th shading point, respectively. Notice that this is a discretized sum derived from the original continuous integral.</p>

<h3 id="radiance-field">Radiance Field</h3>

<p>A general volume scene representation can be seen as a 5D function (i.e. a radiance field):</p>

\[F_{\sigma, \textbf{c}}: (\textbf{x}, \textbf{d}) \to (\sigma, \textbf{c})\]

<p>which outputs volume density and radiance $(\sigma, \textbf{c})$ given a 3D location $\textbf{x} = (x, y, z)$ and viewing direction $\textbf{d} = (\theta, \phi)$. NeRF approximated this per-scene radiance field as a single MLP network which encapsulates the entire scene geometry and appearance as a whole. However, this approach bakes the scene into the network, not allowing any modification after training.</p>

<h3 id="disentangling">Disentangling</h3>

<p>However, NeuTex explicitly decomposes the radiance field $F_{\sigma, \textbf{c}}$ into two components, $F_{\sigma}$ and $F_{\textbf{c}}$, modeling scene geometry and appearance, respectively.</p>

\[F_{\sigma} : \textbf{x} \to \sigma, \quad F_{\textbf{c}}: (\textbf{x}, \textbf{d}) \to \textbf{c}\]

<h3 id="texture-mapping">Texture mapping</h3>

<p>And the network $F_{\textbf{c}}$ can further be decomposed into two sub-networks $F_{\text{uv}}$ and $F_{\textbf{tex}}$. This decomposition allow us to model scene appearance in a 2D texture space that explains the object’s 2D surface appearance as conventional texture mapping with mesh representations.</p>

<p>In this setting, a 3D point $\textbf{x} = (x, y, z)$ in a volume is first mapped onto a 2D UV coordinate $\textbf{u} = (u, v)$ in a texture. Then we define another mapping that maps this UV coordinate and viewing direction vector $\textbf{d}$ (to model view-dependent appearance changing) to a color $\textbf{c} = (r, g, b)$ in color space. These mappings are approximated with neural network such that:</p>

\[F_{\text{uv}}: \textbf{x} \to \textbf{u}, \quad F_{\text{tex}}: (\textbf{u}, \textbf{d}) \to \textbf{c}\]

<p>Then the appearance function $F_{\textbf{c}}$ introduced previously can be expressed as:</p>

\[F_{\textbf{c}}(\textbf{x}, \textbf{d}) = F_{\text{tex}}(F_{\text{uv}}(\textbf{x}), \textbf{d})\]

<h3 id="neural-representation">Neural representation</h3>

<p>To sum up, NeuTex way of scene representation can be thought as a composition of three functions: a geometry function $F_{\sigma}$, a texture mapping function $F_{\text{uv}}$, and a texture function $F_{\text{tex}}$, given by:</p>

\[(\sigma, \textbf{c}) = F_{\sigma, \textbf{c}}(\textbf{x}, \textbf{d}) = (F_{\sigma}(\textbf{x}), F_{\text{tex}}(F_{\text{uv}}(\textbf{x}), \textbf{d}))\]

<p>In total, three separate MLP networks for $F_{\sigma}$, $F_{\text{uv}}$ and $F_{\text{tex}}$ are used.</p>

<h2 id="texture-space-and-inverse-texture-mapping">Texture space and inverse texture mapping</h2>

<p>Texture space used in this study is parametrized by a 2D UV coordinate $\textbf{u} = (u, v)$. While <strong>any continuous 2D topology can be used for the UV space, 2D unit sphere is mostly used</strong>. Then, $\textbf{u}$ is interpreted as a point on the unit sphere.</p>

<p>However, as mentioned earlier, <strong>directly training three networks $F_{\sigma}$, $F_{\text{uv}}$, and $F_{\text{tex}}$ with pure rendering supervision leads to a highly distorted texture space and degenerate cases</strong> (e.g. multiple points map to the same UV coordinate). Meanwhile, the desired way is to uniformly map the 2D surface onto the texture space preventing different points from being mapped to the single point in UV space.</p>

<p>A possible walkaround for this problem is to introduce the inverse network, $F_{\text{uv}}^{-1}$, opposed to $F_{\text{uv}}$, which maps a 2D UV coordinate $\textbf{u}$ on the texture to a 3D point $\textbf{x}$ in the volume:</p>

\[F_{\text{uv}}^{-1}: \textbf{u} \to \textbf{x}\]

<p>This function projects the 2D texture space onto a 2D manifold. Including this inverse transform allow us to reason about the 2D surface of the scene, and regularizes the texture mapping process, preventing the network from producing degenerate mappings due to too much degree of freedom during training.</p>

<p>Together, $F_{\text{uv}}$ and $F_{\text{uv}}^{-1}$  forms a cycle mapping between the 2D object surface and the texture space, leading to high-quality texture mapping.</p>

<h2 id="training-neural-texture-mapping">Training neural texture mapping</h2>

<p>Then our next question would be: <em>“How can we enforce $F_{\text{uv}}$ and $F_{\text{uv}}^{-1}$ to be a mapping similar to one-to-one correspondance?”</em></p>

<p>With this question in mind, we’ll cover the losses used in the paper guiding the network to learn proper geometric &amp; appearance representation of the scene.</p>

<h3 id="rendering-loss">Rendering loss</h3>

<p>As in NeRF, one straightforward to measure the quality of synthesized image is the direct comparison with the ground truth. Thus, the rendering loss per pixel is the $L_2$ distance between estimated pixel radiance and the one from ground truth:</p>

\[L_{\text{render}} = \vert\vert \textbf{I}_{\text{gt}} - \textbf{I} \vert\vert^{2}_{2}\]

<p>This is the main source of supervision.</p>

<h3 id="cycle-loss">Cycle loss</h3>

<p><img src="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.36.44.png" alt="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.36.44.png" /></p>

<p>Figure 3. <strong>A checkerboard texture applied to scenes</strong>.</p>

<p>Suppose that we have a shading point $\textbf{x}<em>i$ on a ray obtained by ray marching. Then previously suggested texture mapping network $F</em>{\text{uv}}$ finds its UV $\textbf{u}<em>i$ in texture space for radiance regression. Then, the inverse mapping network $F</em>{\text{uv}}^{-1}$ is used to map this point back to the 3D space:</p>

\[\textbf{x}_i^{\prime} = F_{\text{uv}}^{-1}(F_{\text{uv}}(\textbf{x}_i))\]

<p>By minimizing the distance between $\textbf{x}_i$ and $\textbf{x}_i^{\prime}$, the network can learn the proper mapping between 3D space where geometry lies in and 2D UV space.</p>

<p>However, note that it’s redundant and unreasonable to enforce a cycle mapping at <em>any</em> 3D point. Since the texture is mapped <em>onto</em> the surface of geometry, we only expect a correspondance between the texture space and points on the 2D <em>surface</em> of the scene. That being said, it’s <strong>meaningless enforcing the cycle mapping in the empty space far away from the surface</strong>. Thus, from the intuition that the pixel is mostly affected by a shading point on, or nearby the surface, we leverage the radiance contribution weights per shading point to weigh our cycle loss. Specifically the weight we consider here is:</p>

\[w_i = T_{i} (1 - \text{exp}(\sigma_i \delta_i))\]

<p>which determines the contribution to the final pixel color for each shading point $i$. This weight implies how close a point is to the surface, thus suitable for our purpose. Then the cycle loss for a single ray is given by:</p>

\[L_{\text{cycle}} = \sum_{i} w_i \vert\vert F_{\text{uv}}^{-1} (F_{\text{uv}} (\textbf{x}_i)) - \textbf{x}_i\vert\vert_{2}^{2}\]

<h3 id="mask-loss">Mask loss</h3>

<p>This additional loss is built upon the purpose of supervising a foreground-background mask.</p>

<p>The transmittance used consistently in NeRF and its variants has the physical meaning that whether the $i$-th point along a ray is occluded by the other densities before that or not.</p>

<p>Thus, it’s natural to interpret the transmittance $T_N$ at the last shading point on a pixel ray as an indicator, saying that whether the pixel color is from the part of the background or not (i.e. if the transmittance at $N$-th sample is close to 1, it’s almost not occluded by any other objects in the scene).</p>

<p>Given the ground truth mask $M_{\text{gt}}$ per pixel, then the mask loss can be computed as:</p>

\[L_{\text{mask}} = \vert\vert M_{\text{gt}} - (1 - T_N) \vert\vert_{2}^{2}\]

<p>Experimentally discovered fact is that, this loss is necessary when viewpoints do not cover the object entirely, so the large portion of the ground truth image is originated from the background of the scene. In such situation, the network can produce hallucinates, which is not the desired behavior. When the view coverage is dense enough, this loss is often optional.</p>

<h3 id="full-loss">Full loss</h3>

<p>Finally, the loss $L$ used during training is of form:</p>

\[L = L_{\text{render}} + a_1 L_{\text{cycle}} + a_2 L_{\text{mask}}\]

<p>$a_1 = 1$ is used for all scenes in the experiments, whereas $a_2 =1$ is used for most of the scenes, except for those having good view coverage. For such exceptional cases, the mask loss is dropped by setting $a_2 = 0$.</p>

<h1 id="implementation--experimental-results">Implementation &amp; Experimental Results</h1>

<h2 id="network-details">Network details</h2>

<p><img src="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_14.37.17.png" alt="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_14.37.17.png" /></p>

<p>Figure 4. <strong>Network structure for the 4 networks</strong>.</p>

<ul>
  <li>All four sub-networks, $F_{\sigma}$, $F_{\text{tex}}$, $F_{\text{uv}}$, and $F_{\text{uv}}^{-1}$, are designed as <strong>MLP</strong> networks.</li>
  <li>Both viewing direction $\textbf{d}$ and UV coordinate $\textbf{u}$ are represented as <strong>unit vectors</strong>.</li>
  <li><strong>Positional encoding</strong> is also used just like NeRF did, to infer high-frequency geometry and apearance details. Precisely, the positional encoding is applied for geometry network $F_{\sigma}$, and texture network $F_{\text{tex}}$ on all their input components including $\textbf{x}$, $\textbf{u}$ and $\textbf{d}$. For those networks $F_{\text{uv}}$ and $F_{\text{uv}}^{-1}$, positional encoding is not used since it’s expected to be smooth and uniform.</li>
</ul>

<h2 id="training-details">Training details</h2>

<p>Before training, <strong>the scene space is normalized to the unit box</strong> (remember that NeRF requires near and far boundaries for ray marching). Then the shading points on each pixel ray are sampled inside the box.</p>

<p>Unlike NeRF which used hierarchical sampling for enhancing the quality of the rendered image, <strong>NeuTex uses only stratified but coarse sampling</strong> (uniform sampling with local jittering) to sample 256 point on each ray for ray marching.</p>

<p>For each iteration, <strong>the number of pixels in a batch is sampled randomly</strong> in between 600 and 800, and this number of pixels are sampled from an input image. Idealy, 2/3 pixels are from foreground with the rest of them are from background.</p>

<p>The inverse mapping network $F_{\text{uv}}^{-1}$ <strong>is initialized with a point cloud from COLMAP using a Chamfer loss</strong>. However, since the MVS point cloud is often very noisy, Chamfer loss is only used for initialization. Although, NeuTex works fine without it in most cases, it seems to help a bit. What’s more interesting to see is that <strong>even though $F_{\text{uv}}^{-1}$ is initialized with very noisy geometry at the beginning, it succesfully learns smooth, continuous mapping</strong> from 3D space to 2D UV space, after training with rendering and cycle loss together.</p>

<p>To be more specific, the training is done following the procedure below:</p>

<ol>
  <li>Train the entire network using the Chamfer loss and the rendering loss for 50,000 iterations.</li>
  <li>Remove the Chamfer loss and train with the full loss suggested in the paper until convergence (takes around 500,000 iterations).</li>
  <li>Fine-tune the texture network $F_{\text{tex}}$ until convergence, while fixing the parameters of the other networks.</li>
</ol>

<p><strong>The entire process described above takes about 2-3 days on a single RTX 2080Ti GPU.</strong></p>

<h2 id="results---view-synthesis">Results - View Synthesis</h2>

<p><strong>For more details, please refer to the paper.</strong></p>

<p>The datasets used for the experiments are the following:</p>

<ul>
  <li>5 scenes from DTU dataset → 49 or 64 input images from multiple viewpoints per scene</li>
  <li>2 scenes from Neural Reflectance Fields → about 300 images per scene</li>
  <li>3 scenes crafted by the authors of NeuTex → about 100 images per scene</li>
</ul>

<p>Since this work is about capturing surface of the objects, <strong>the input images should have a clean, easily segmentable background</strong>. To obtain it, U2Net is used to automatically compute masks for the scenes captured by the authors. Other datasets also have exact masks or image features that does the similar thing (e.g. images in Neural Reflectance Fields dataset are already have very dark background, eliminating the need of masking)</p>

<p><img src="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.38.45.png" alt="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.38.45.png" /></p>

<p>Figure 5. <strong>Comparisons on DTU scenes</strong>. Note that NeuTex is on par with NeRF, while supporting texture editing.</p>

<h2 id="results---texture-mapping-and-appearance-editing">Results - Texture mapping and appearance editing</h2>

<p><strong>NeuTex is the first neural rendering method that extracts texture mapping from the scene, and most importantly, enables surface appearance editing in a 2D texture space</strong> (discovered by NeuTex). One can see in (Fig. 6.d) that it also successfully unwraps the object surface into a reasonable texture.</p>

<p><img src="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.45.38.png" alt="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_22.45.38.png" /></p>

<p>Figure 6. <strong>Texture editing on DTU (rows 1-2) and the scenes from author (rows 3-5)</strong>.</p>

<p>Furethermore, as one can see from (Fig 6. e ~ g), the texture can be modified either globally or locally. That is, one can either modify the entire material, or just add some patterns onto the existing texture. Those appearance editing can directly be done in the discovered texture space, and the scene still looks natural and view point consistent after such modification. <strong>→ NeuTex successfully disentangles the geometry and appearance of real objects and model the surface appearance in a meaningful texture space.</strong></p>

<h3 id="results---extension-to-reflectance-fields">Results - Extension to reflectance fields</h3>

<p><img src="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_23.08.53.png" alt="NeuTex%20Neural%20Texture%20Mapping%20for%20Volumetric%20Neura%2040d18b7657d64a89accb0acf21448017/_2021-06-28_23.08.53.png" /></p>

<p>Figure 7. <strong>NeuTex in reflectance fields setting</strong>. The captured diffuse albedo (d) is editted to be (g). The images produced using this modified albedo are shown in (e, f).</p>

<p>NeuTex can regress normal $\textbf{n}$ and reflectance parameters $\textbf{r}$ at each shading point, instead of directly outputting radiance $\textbf{c}$. Then, reflectance-aware volume rendering method can be applied on such reflectance field, to compute radiance from these shading properties defined at each point in 3D volume. With this high-level idea in mind, we can modify some parts of NeuTex:</p>

<ul>
  <li>Modify geometry network $F_{\sigma}$ to jointly regress volume density and normal (since normal is also geometric property)</li>
  <li>Modify texture regression network $F_{\text{tex}}$ to regress the reflectance parameters $\textbf{r}$ in the texture space.</li>
</ul>

<p>The central UV mapping and its inverse network retain their original structure. This modified version of NeuTex can still generate plausible images via reflectance-aware volume rendering process. See (Fig. 7) for details.</p>
:ET
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dvelopery0115.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dvelopery0115.github.io/" rel="alternate" type="text/html" /><updated>2023-04-04T16:48:50+00:00</updated><id>https://dvelopery0115.github.io/feed.xml</id><title type="html">DveloperY0115’s Blog</title><subtitle>A blog of computer science enthusiast</subtitle><author><name>Seungwoo Yoo</name></author><entry><title type="html">Summary of ‘An Improved Illumination Model for Shaded Display’</title><link href="https://dvelopery0115.github.io/2021/09/02/Whitted.html" rel="alternate" type="text/html" title="Summary of ‘An Improved Illumination Model for Shaded Display’" /><published>2021-09-02T00:00:00+00:00</published><updated>2021-09-02T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/09/02/Whitted</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/09/02/Whitted.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>The goal of computer graphics has been achieving photorealism from the beginning. For instance, even the earliest algorithms included shaders that simulated effects such as specular reflection, shadows, and transparency.</li>
  <li>While several works demonstrate the importance of illumination models in practice, existing methods are usually limited in scope since they only focus on light sources and surface orientations (normally represented by normals), ignoring the effect of global illumination.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Proposes a novel rendering method based on physical simulation of light-surface interaction and recursive evaluation of intensities by tracing rays backward from the viewer.</li>
</ul>

<hr />

<h1 id="methods">Methods</h1>

<h2 id="conventional-models">Conventional Models</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>An example generated by using the method proposed in the paper</b>.</span></p>

<p>The simpliest visible surface algorithms use shaders based on Lambert’s cosine law. According to the law, the intensity of the reflected light is proportional to the dot product of the surface normal and the light source direction. However, note that we can only simluate a perfect diffuser (dull, matte surface) with this simple principle. For more sophiscated effect such as specular reflection which can easily be observed in materials like plastic, or metal, we need to add more terms in our model. One simple, yet powerful solution for simulating specular reflection is proposed by Bui-Tuong Phong in 1975. According to Phong’s model, the intensity is computed as:</p>

<p>$$
\begin{gather} 
I = I_{a} + k_{d} \sum_{j=1}^{j=ls} (\bar{N} \cdot \bar{L}<em>{j}) + k</em>{s} \sum_{j=1}^{j=ls} (\bar{N} \cdot \bar{L}_{j}^{\prime})^{n},
\end{gather}$$</p>

<p>where $I$ is the reflected intensity, $I_{a}$ is the reflection due to ambient light, $k_{d}$ is the diffuse reflection constant, $\bar{N}$ is the unit surface normal, $\bar{L}_{j}$ is the vector in the direction of the $j$th light source, $k_{s}$ is the specular reflection coefficient, $\bar{L}_{j}^{\prime}$ is the direction of halfway vector, and $n$ is an exponent which determines the glossiness of the surface. Note that only the last term is dependent to the viewing direction, and larger $n$ tends to make specular reflection more sharper.</p>

<p>However, Phong’s model assumes that each light source is located at a point infinitely far from the objects in the scene, thus do not consider:</p>

<ol>
  <li>objects within a scene acting as light sources</li>
  <li>light reflected from the surface of other objects in the scene</li>
</ol>

<p>While the model works sufficiently well with diffuse materials, it suffers from serious degrade of quality when it comes to specular reflections. For remedy, Blinn and Newell came up with an idea to tackle the problem by modeling an object’s environment and mapping it onto a sphere of infinite radius (the method so called <em>environment mapping</em>). However, this approach is not appropriate to apply in the general case.</p>

<p>Not only the specular reflection, but also the simulation of shadows is one of the features that we want an illumination model to support. The principle is still simple - a point on a surface lies in shadow if it is visible to the viewer but not visible to the light source. And various approaches has been proposed to draw more realistic shadows.</p>

<p>Last but not least, transmission of light <em>through</em> transparent objects has been simulated by painting surfaces in reverse depth order (i.e., similar to alpha compositing, partially overwrite background). Even though such technique showed impressive results, it could not simulate refraction.</p>

<h2 id="improved-model">Improved Model</h2>

<p>Then how can we build more realistic model for reflection? What is more physically accurate way to simulate light bouncing off from a point on a surface? Actually, classical ray optics already provides the answer in the case of perfect mirror reflection as illustrated in the figure below.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Reflection and refraction of a ray at a point on a perfect mirror</b>.</span></p>

<p>The light intensity $I$ at a point on the surface observed by the viewer is determined by two primary components:</p>

<ul>
  <li>The specular reflection $S$</li>
  <li>The transmission $T$</li>
</ul>

<p>these intensities represent light propagated along the $\bar{V}$, $\bar{R}$, and $\bar{P}$ directions, respectively. Additionally, since the surfaces do not only exhibit specularity, we must add a term to model the diffuse component as well. Ideally, the diffuse reflection <em>should</em> contain components due to reflection of nearby objects as well as predefined light sources, then the computational cost will be increased dramatically making the rendering infeasible (<strong>Note: considered impossible in 1980, we are now able to simulate such light-surface interaction in real-time after 40 years</strong>). Therefore, the authors adopted the same diffuse model from previous works. Then the newly proposed model is:</p>

<p>$$
\begin{gather}
I = I_{a} + k_{d} \sum_{j=1}^{j=ls} (\bar{N} \cdot \bar{L}<em>{j}) + k</em>{s} S + k_{t} T,
\end{gather}
$$</p>

<p>where $S$ is the intensity of light incident from the $\bar{R}$ direction, $k_{t}$ is the transmission coefficient, and $T$ is the intensity of light from the $\bar{P}$ direction. Here, the coefficients $k_{s}$ and $k_{t}$ are held constant for the model, but the authors strongly recommend to regard them as functions each of which is an approximation of the Fresnel reflection law for better quality. In other words, these coefficient should also be dependent to the incidence angle. Even when they are fixed as constants, the actual values of them must be chosen with care in order to generate images that physically make sense.</p>

<p>The direction of reflected ray $\bar{R}$ is determined by the simple rule that the angle of reflection must equal the angle of incident. In similar manner, the direction of transmitted ray $\bar{P}$ can be determined by consulting Snell’s law. More specifically, $\bar{R}$ and $\bar{P}$ are functions of $\bar{N}$ and $\bar{V}$ defined as follows:</p>

<p>$$
\begin{gather}
\bar{V}^{\prime} = \frac{\bar{V}}{\vert \bar{V} \cdot \bar{N} \vert}, <br />
\bar{R} = \bar{V}^{\prime} + 2 \bar{N}, <br />
\bar{P} = k_{f} (\bar{N} + \bar{V}^{\prime}) - \bar{N},
\end{gather}
$$</p>

<p>where $k_{f} = (k_{n}^{2} \vert \bar{V}^{\prime} \vert ^{2} - \vert \bar{V}^{\prime} + \bar{N} \vert^{2})^{-1/2}$ and $k_{n}$ is the index of refraction. Note that these equations assume that $\bar{V} \cdot \bar{N}$ is less than zero, thus the sign of $\bar{N}$ must be adusted so that it points to the side of the surface from the direction which the intersecting ray is coming from. Similarily, the index of refraction must be adjusted to account for the sign change. Furthermore, one can simply assume $T = 0$, total internal reflection is occured when $k_{f}$ becomes imaginary.</p>

<p>One intuition we can obtain from the proposed rule is that the higher $k_{d}$ and the smaller $k_{s}$ will make the surface look less glossy (i.e., matte). This is obvious since we are weighting the diffuse intensity more than that of specular reflection. At the same time, it is worth noting that this method does not use the specular exponent $n$ as Phong’s model did. Rather, it is based on the physical observation that it is microscopic mirror-like facets that contribute to the intensity of reflected light in a small region of surface. Of course, one may come up with an idea to consider such irregularity of surface in microscopic scale by introducing random perturbation to surface normals during computation. Considerably small variation will result in glossy surface, while large variation will spread out rays randomly making the surface exhibit diffuse behavior.</p>

<p>Let’s turn our attention to more complicated pheonomena - interreflection. The model we discussed until now is nothing but an approximation of light-surface interaction at a surface point. Then one might ask: <em>“What happens to the reflected or transmitted rays?”.</em> Answering this is the key to understanding <em>global illumination</em>, one of the core concepts of modern physically-based rendering techniques.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 3. <b>Trajectory of light traveling various media</b>.</span></p>

<p>Imagine a scene having multiple objects and light sources. Let’s follow the journey of a light ray starting from one of light sources in the scene. Before reaching the viewer, this ray will bounce off from several surfaces, go through transparent medium, etc. This process can be modeled using well-known tree structure like the figure below.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 4. <b>Tree representing the trajectory of ray reaching the viewer</b>.</span></p>

<p>More pricisely, the tree above shows how the ray reaching the viewer from point $A$ in figure 2 is composed of. As one can see in the figure 2, two rays from different directions $T_{1}$ and $S_{1}$ are merged at point $A$, and then go straight to the viewer. However, the ray coming from direction $S_{1}$ is actually the compound of two rays from two directions $S_{2}$ and $T_{2}$. In reality, one can <em>backtrace</em> the path of each ray capture by the camera by simply reversing the direction of incident, reflected, and transmitted rays computed in the model discussed earlier. When implemented in the form of algorithm, such procedure can be performed recursively, until meeting the termination condition (e.g., setting the upper bound for the number of recursion, ray travels to free space of nothing, etc).</p>

<p>Additionally, at each step of recursion, especially when computing $\bar{L}_{j}$ to determine the diffuse reflection, one can determine whether the point is being directly illuminated by lights by casting rays to every light source in the scene and check if there is something between the point and a light source. If so, it implies that the effect of $\bar{L}_{j}$ must be attenutaed.</p>

<h2 id="visible-surface-processor">Visible Surface Processor</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig5.png" />
</center>
<p><span class="caption text-muted">Figure 5. <b>Rendering an image by casting rays from each pixel lying on the image plane</b>.</span></p>

<p>As mentioned earlier, it is much more efficient to trace the rays backward from the viewer to sources, rather than forward tracing the photons emitted from light sources in the scene. This is obviously the desirable choice since only few rays will reach the viewer and contribute to the rendered image. Thus, repeating the earlier discussion, we do the following:</p>

<ol>
  <li>Cast a ray from the viewer through a pixel of final image.</li>
  <li>If a ray intersect a surface of an object in the scene, produce rays in the $\bar{R}$ and $\bar{P}$ direction. Note that it depends on the property of the surface. For example, if the object is not transparent at all, a ray to direction $\bar{P}$ should not be generated.</li>
  <li>Repeat 1 and 2 for all pixels, until none of generated ray intersect a surface in the scene.</li>
</ol>

<p>Since one cannot know whether a ray will run into an object or not until following the entire path, the traditional optimization methods such as clipping and back-face culling cannot be used in this scheme. Instead, we can reduce the computation by introducing the concept of bounding volume with which we can test whether the ray intersect an object in the scene quickly. For simplicity, this work first used spheres for rendering since it is easy to perform intersection test and a sphere itself can serve as its own bounding volume. For nonspherical objects, sophiscated intersection processors must be designed for better performance.</p>

<ul>
  <li>Polygonal surfaces: Solve for the point of intersection of the ray and the plane of the polygon and then check whether the point is on the interior of that polygon.</li>
  <li>Surface consists of bicubic patches: Generate bounding spheres for each patch. If a ray intersect the bounding sphere, subdivide (using the method called <em>Catmull-Clark subdivision</em>) the patch and create bounding spheres for each subpatch and perform the same test over and over again.</li>
</ul>

<p>Moreover, this visible surface algorithm is also able to perform anti-aliasing. In this work, a pixel is defined as the rectangular region whose corners are four sample points as in the figure below.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 6. <b>A pixel defined as the region surrounded by four sample points</b>. Intensities computed at each sample are almost identical in the case illustrated above.</span></p>

<p>As shown in the figure above, if the intensities computed at the four corners have nearly equal values, and no small object lies in the region between them, there is no problem. The rendering algorithm may assume that the average of four values is a good approximation for the pixel intensity. However, how about this case?</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 7. <b>An example of inadequate sampling</b>. While three rays intersect the same surface thus yielding similar values, one sampling travels further resulting in dramatically different intensity value.</span></p>

<p>In such case, the algorithm will subdivide the sample square into more smaller regions and then start over again. This will repeat until the computer runs out of resolution or until the algorithm collects enough information for determining the pixel intensity. And the values from each subregions are weighted according to their area (similar to bilinear interpolation).</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Whitted/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 8. <b>An example generated by using the method proposed in the paper</b>.</span></p>

<h1 id="conclusion">Conclusion</h1>
<p>This paper proposes an illumination model based on techniques suggested by Phong and Blinn. However, instead of adapting previous rendering scheme with little modification, this paper presents a new paradigm of rendering - ray tracing. With ray tracing, global illumination can be simulated realistically by tracing rays generated at each point of intersection while recording the intensities at the same time.</p>

<p>However, this work also needs some improvements:</p>

<ul>
  <li>Diffuse reflection from distributed light sources</li>
  <li>More realistic specular reflections from less glossy surfaces</li>
  <li><strong>Runs slowly</strong></li>
</ul>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘Perceiver: General Perception with Iterative Attention’</title><link href="https://dvelopery0115.github.io/2021/08/29/Perceiver.html" rel="alternate" type="text/html" title="Summary of ‘Perceiver: General Perception with Iterative Attention’" /><published>2021-08-29T00:00:00+00:00</published><updated>2021-08-29T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/29/Perceiver</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/29/Perceiver.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>Biological systems perceive the world by simultaneously processing high-dimensional inputs of various forms such as vision, audition, touch, proprioception, etc. On the other hand, the perception models implemented in deep learning often rely on domain specific assumptions and lack multi-modality. → <strong>Architectures must be redesigned from scratch as their input vary.</strong></li>
  <li>While the inductive biases such as spatial locality in early vision models were valuable since they could increase the efficiency of learning perceptual models by focusing only on data from certain domain. However, considering that there are lots of large scale datasets available, is such decision - introducing constraints to our model - still valid?</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Proposes the <strong>Perceiver</strong>, a model which builds upon Transformers, designed to handle arbitrary configurations of different modalities using a single Transformer-based architecture.</li>
  <li>Proposes a novel method which combines the ideas from the Transformer and recurrent neural networks, reducing the computation time and memory usage while maintaining flexibility and performance.</li>
</ul>

<hr />

<h1 id="methods">Methods</h1>

<center>
    <img class="img-fluid" src="/assets/post-images/Perceiver/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>The Perceiver architecture overview</b>.</span></p>

<h2 id="the-perceiver-architecture">The Perceiver architecture</h2>

<h3 id="overview">Overview</h3>

<p>The architecture consists of two components:</p>

<ol>
  <li>A cross-attention module which maps a byte array (representing input data) and a latent array to a latent array.</li>
  <li>A Transformer tower that maps a latent array to a latent array (can be thought as self-attention).</li>
</ol>

<p>Here, the size of the byte array is determined by the input data and is generally large (e.g., images from ImageNet dataset at resolution 224 have 50,176 pixels), while <strong>the size of the latent array is a hyperparameter</strong> which is typically much smaller (e.g., the authors used 512 latents on ImageNet).</p>

<p>The model applies the cross-attention module and the Transformer in an alternative fashion. This is equivalent to repeatedly fusing high-dimensional information of input data into a lower-dimension attention bottleneck. Such low dimensional representation is then used to query the input (byte array) again. This also can be seen as performing a fully end-to-end clustering of the inputs with latent positions as cluster centers.</p>

<p>Since the weights are optionally shared between each instance of the Transformer tower (and between all instances of the cross-attention module but the first), <strong>the model can be interpreted as a recurrent neural network (RNN), but unrolled in depth taking the same input at every step</strong>, not a particular data point in a temporal sequence. Also, all attention modules in the Perceiver are non-causal (i.e., no masks).</p>

<h3 id="taming-quadratic-complexity-with-cross-attention">Taming quadratic complexity with cross-attention</h3>

<center>
    <img class="img-fluid" src="/assets/post-images/Perceiver/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>The authors trained the Perceiver architecture on images from ImageNet (left), video and audio from AudioSet (center), and 3D point clouds from ModelNet40 (right)</b>.</span></p>

<p>The Perceiver heavily relies on attention mechanisms since it is generally applicable and powerful. Both cross-attention and Transformer modules are based on the query-key-value (QKV) attention. Then, the main challenge the authors encountered while designing such structure was to scale attention architectures to very large and generic inputs which were regarded impossible to process with the traditional Transformers due to the quadratic complexity of QKV self-attention. This is the reason why the Transformer architecture could not be directly applied to other domains.</p>

<p>Prior works usually made compromises in order to avoid applying standard QKV attention directly, which is very expensive, to large scale data such as pixel arrays of images, audio samples, etc. <strong>In contrast, this paper applies attention directly to the inputs by introducing an assymetry into the attention operation.</strong></p>

<p>Specifically, let $Q \in \mathbb{R}^{M \times D}$, $K \in \mathbb{R}^{M \times C}$, and $V \in \mathbb{R}^{M \times C}$, where $C$ and $D$ are channel dimensions. For example, if self-attention is computed on all pixels of an image, then $M = W \times H$, and $D = 3$ (assuming the image is in RGB format) where $W$ and $H$ are width and height of the image, respectively. Then one can easily observe that the complexity of the QKV attention operation - $\text{softmax} (QK^{T})V$ - is $\mathcal{O}(M^{2})$, as it involves two matrix multiplications with matrices of large dimension $M$. If the image is of size $1024 \times 1024$, the quadratic complexity will immediately explode - $\mathcal{O}(1024^{2}) = \mathcal{O}(2^{20})$. This is the primary reason for introducing assymetry in this work. In this setting, <strong>$K$ and $V$ become projections of the input byte array, while $Q$ is instead a projection of a learned latent array with index dimension</strong> $N \ll M$. As mentioned earlier, the dimensionality $N$ of latents is a hyperparameter. Then the resulting cross-attention operation would have complexity $\mathcal{O}(MN)$.</p>

<h3 id="uncoupling-depth-with-a-latent-transformer">Uncoupling depth with a latent Transformer</h3>

<p>The output of the cross-attention module has the same shape as the input to the $Q$ network. This is then consumed by deep, expressive Transformers in the latent space which cost only $\mathcal{O}(N^{2})$. This design allows us to implement much deeper Transformers without relying on domain-specific assumptions. It is worth mentioning that a Transformer built on bytes has complexity $\mathcal{O}(LM^{2})$ while a latent Transformer has complexity $\mathcal{O}(LN^{2})$ where $N$ is significantly smaller than $M$, and $L$ is the number of layers.</p>

<p>Then the total complexity becomes $\mathcal{O}(MN + LN^{2})$. Since the Perceiver decouples the input size and the depth, it can embrace additional Transformer layers without any concern on cost related to the size of input data. It is the core idea which enables us to build very large networks on large-scale data (e.g., the best performing Perceiver model on ImageNet has 48 latent Transformer blocks, which was considered impossible with traditional Transformers).</p>

<p>The latent Transformer used in this work adopted the GPT-2 architecture, which is based on the decoder of the original Transformer architecture. In experiments, the authors used values of $N \leq 1024$ for the dimensionality of latents. Furthermore, the latent array is initialized using a learned position encoding.</p>

<h3 id="iterative-cross-attention--weight-sharing">Iterative cross-attention &amp; weight sharing</h3>

<p>In the previous section, we analyzed how the computational cost is affected by the size of input. Thanks to the smaller latent array and cross-attention between inputs and latents, we can:</p>

<ol>
  <li>Use large scale data (e.g., images, audio samples) directly without any optimization or assumption on them.</li>
  <li>Build deeper Transformers whose cost is solely dependent to the number of latents, not the size of inputs.</li>
</ol>

<p>On the other hand, <strong>one also must think about the trade-off between the network’s ability to capture details from the input data and the computational cost.</strong> Due to the existence of latent bottleneck, the network might struggle to capture necessary information from the input data. And one possible solution for that is to add more cross-attention layers into the model. This will allow the latent array to iteratively extract information from the input data as much as needed. However, while experimental results show that more cross-attends lead to better performance, such gain comes with higher computational cost since the cost of cross-attention has linear dependence on the input size.</p>

<p>Last but not least, the parameter efficiency of the Perceiver can further be improved by exploiting its iterative structure. That is, it is possible to share weights between:</p>

<ul>
  <li>corresponding blocks of each latent Transformer</li>
  <li>cross-attend modules</li>
</ul>

<p>Latent self-attention blocks can be shared if only a single cross-attend is used. In their experiments on ImageNet, the authors could reduce the number of parameters by 10 times by sharing weights. Also, this also reduced overfitting and boosted validation performance.</p>

<p>Therefore, the resulting architecture has the functional form of an RNN with:</p>

<ul>
  <li>A cross-attention input projection</li>
  <li>A bottlenecked latent dimensionality</li>
  <li>A latent Transformer recurrent core</li>
</ul>

<h2 id="position-encodings">Position encodings</h2>

<h3 id="permutation-invariance-and-position-information">Permutation invariance and position information</h3>

<p>Attention is a permutation-invariant operation, and this property is still valid in the Perceiver and related models. In other words, a pure attention model will return the same output regardless of the order of its inputs. This is the reason why attention-based architectures are well-suited for many types of data - they make no assumptions about spatial relationships or symmetries. Meanwhile, convolutional layers that are widely adopted in image processing often make several assumptions on 2D spatial structure within images. These assumptions naturally arise from the structure &amp; mechanisms of convolutional layer itself such as:</p>

<ul>
  <li>Use of filters that look only at local regions of image → makes it easier to capture the relationship between nearby pixels than distant pixels</li>
  <li>Sharing weights across both spatial dimensions (i.e., a single kernel wipes the image along both width and height directions) → helps to model data with translation-invariant statistics</li>
  <li>Applying small filters repeatedly → helps to model data with scale-invariant statistics</li>
</ul>

<p>However, apart from preventing the Perceiver from making presumptions on data, the model should be able to exploit spatial relationships in its input. Because spatial relationships are essential for sensory reasoning and such limitation is undesirable. To circumvent such issue, position information is usually injected by tagging <em>position encodings</em> onto the input features, and this paper use the approach as well. While position encoding was originally introduced in natural language processing literature to encode the position inside word sequences, it can also be used to encode spatial, temporal, and modality identity as well.</p>

<h3 id="scalable-fourier-features">Scalable Fourier features</h3>

<center>
    <img class="img-fluid" src="/assets/post-images/Perceiver/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 3. <b>Attention maps from the first (blue), second (green), and eighth (orange) cross-attention layers of a model on ImageNet with 8 cross-attention modules</b>.</span></p>

<p>Here, the authors used Fourier feature position encodings which has shown good performance both in language and in vision. They used a parametrization of Fourier features with which so they can:</p>

<ol>
  <li>Directly represent the position structure of the input data (preserving 1D temporal or 2D spatial structure for audio or images, respectively, or 3D spatiotemporal structure for videos).</li>
  <li>Control the number of frequency bands in the position encoding independently of the cutoff frequency.</li>
  <li>Uniformly sample all frequencies up to a target resolution.</li>
</ol>

<p>The authors parametrized the frequency encoding to take the values $[\sin (f_{k} \pi x_{d}), \cos (f_{k} \pi x_{d})]$, where the frequency $f_{k}$ is the $k^{\text{th}}$ band of a bank of frequencies spaced equally between 1 and $\frac{\mu}{2}$ and $x_{d} \in [-1, 1]$ is the value of the input position along the $d^{\text{th}}$ dimension. $\frac{\mu}{2}$ can be interpreted as the Nyquist frequency corresponding to a target sampling rate of $\mu$. Providing encodings will encourage the model to learn to compare the values of bytes at any positions in the input array. As other works normally did, these values are then concatenated with the raw position value $x_{d}$ to produce the final representation of position. This gives us a position encoding of size $d(2K + 1)$ (for each dimension, $K$ sine and cosine components and a coordinate along the dimension).</p>

<p>This parametrization scheme is related to the NeRF’s position encoding scheme which is built around frequency bands with increasing powers of two (i.e., the $k^{\text{th}}$ band has frequency $2^{k}$). However, this yields very high frequencies for even modest number of bands, thus turned out to be numerically unstable in some experiments conducted by the authors.</p>

<p>Also, it is worth to compare this scheme with that of Transformer. In the Transformer, inputs are produced by adding a position encoding to the input encoding (NB. NLP usually involves encoding of symbols using embedding layer, through the process so called <em>word embedding</em>). Note that a position encoding must be of same size as the input encoding when following that scheme. The authors found it beneficial to concatenate the position and input features rather than just adding them together.</p>

<h3 id="position-encodings-are-generally-applicable">Position encodings are generally applicable</h3>

<p>One important thing to note here is that the use of position encodings does NOT contradict the foundation of this work - building a domain agnostic architecture for general perception tasks. There are three reasons:</p>

<ol>
  <li>While the architectural imposition of position information hard codes a specific positional prior, <strong>the feature-based approach allows the network to learn how to use the position structure</strong>.</li>
  <li><strong>Position encoding can be easily adapted to a new domain since Fourier features are trivial to adapt as long as the input dimensionality is relatively small and known</strong> (NB. lots of signal processing algorithms are built on top of Fourier analysis). The Transformer is one example which shows that simple, learned position encoding is sufficient for good results, and the authors themselves discovered that similar strategy works well on ImageNet (without knowing anything about input 2D structure) and on other kinds of modalities.</li>
  <li><strong>Position encodings can be naturally extended to multimodal data</strong> - each domain can use a position encoding with the correct dimensionality for its data, with learned encodings used to distinguish domains.</li>
</ol>

<h1 id="conclusion">Conclusion</h1>

<p>This paper presented the Perceiver, a Transformer-based model which scales to more than a hundred thousand inputs. Built upon a novel iterative cross-attention between input byte array and latent array whose size is controlled by a hyperparameter, the Perceiver is able to handle arbitrary sensor configurations, enabling fusion of information at all levels. This makes the Perceiver an architecture for general perception since it makes few assumptions about its inputs.</p>

<p>However, the Perceiver also has drawbacks that are expected to be improved in future works:</p>

<ol>
  <li>Its great flexibility comes with great overfitting, thus many parts of its design were intended to mitigate this. → Requires lots of data</li>
  <li>Although the authors have reduced the amount of modality-specific prior knowledge in the model, they had to employ modality-specific augmentation and position encoding. Thus, searching for a method toward end-to-end modality agnostic learning is still an interesting research topic.</li>
</ol>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘Attention Is All You Need’</title><link href="https://dvelopery0115.github.io/2021/08/26/Transformer.html" rel="alternate" type="text/html" title="Summary of ‘Attention Is All You Need’" /><published>2021-08-26T00:00:00+00:00</published><updated>2021-08-26T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/26/Transformer</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/26/Transformer.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>While recurrent neural networks (RNN), long short-term memory (LSTM), and gated recurrent (GRU) neural networks have been established as state-of-the-art approaches in sequence modeling and transduction problems (e.g., language modeling, machine translation), their inherent sequential nature precludes parallelization within training examples, slowing down both training and inference. Furthermore, such models perform poorly on long sequences where the memory constraints become a bottleneck.</li>
  <li>On the other hand, there has been tremendous attempts adopting attention mechanisms to the existing recurrent models, allowing modeling of dependencies without regard to their distance in the input or output sequences. However, the limitation originated from the nature of recurrent models prevents us from seeing the full potential of attention mechanisms.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Proposes the <strong>Transformer</strong>, a model architecture without any recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.</li>
</ul>

<hr />

<h1 id="methods">Methods</h1>

<h2 id="model-architecture">Model Architecture</h2>

<p><img class="img-fluid" src="/assets/post-images/Transformer/fig1.png" />
<span class="caption text-muted">Figure 1. <b>The architecture of the Transformer</b>.</span></p>

<p>Most of neural sequence transduction models before the Transformer had an encoder-decoder structure.</p>

<p>In such models, the encoder first maps an input sequence of symbol representations $(x_{1}, \dots, x_{n})$ to a sequence of continuous representations $\textbf{z} = (z_{1}, \dots, z_{n})$. Only after that, the decoder given $\textbf{z}$ generates an output sequence $(y_{1}, \dots, y_{n})$ of symbols one element at a time. Such models are said to be auto-regressive since they consume the previously generated symbols as additional input for the next prediction.</p>

<p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.</p>

<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>

<p><strong>&lt; Encoder &gt;</strong></p>

<p>The encoder of the Transformer is composed of a stack of $N=6$ identical layers, and each layer is composed of two sub-layers. The former is a multi-head self-attention mechanism, and the latter is a simple, position-wise fully connected feed-forward network. The authors adopted a residual connection around each of the two sub-layers, followed by layer normalization. In other words, the output of each sub-layer can be formally expressed as:</p>

<p>$$\text{LayerNorm} \big(x + \text{Sublayer}(x) \big),$$</p>

<p>where $\text{Sublayer}(x)$ is the function implemented by the sub-layer. Note that all sub-layers in the model, as well as embedding layers, produce outputs of dimension $d_{\text{model}} = 512$.</p>

<p><strong>&lt; Decoder &gt;</strong></p>

<p>The decoder is also composed of a stack of $N = 6$ identical layers. <strong>However, the structure of decoder is different from that of the encoder in the sense that there is a third sub-layer in each layer. This layer performs multi-head attention over the output of the encoder stack.</strong> As in the encoder, the authors employed residual connections around each of the sub-layers, followed by layer normalization and addition. <strong>Another important difference to focus on is that the self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions</strong> (i.e., the decoder must not know about the rest of a sequence before predicting a symbol at a specific step). This modification, combined with the output embeddings shifted by one position, ensures that the predictions for position $i$ is only dependent to the known outputs at positions less than $i$.</p>

<h2 id="attention">Attention</h2>

<p>An attention function can be described as the following:</p>

<blockquote>
  <p>An attention function $\text{Attention}(\textbf{Q}, \textbf{K}, \textbf{V})$ is a mapping which maps a query $\textbf{Q}$ and a set of key-value $(\textbf{K}, \textbf{V})$  pairs to an output where the $\textbf{Q}$, $\textbf{K}$, $\textbf{V}$, and outputs are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function (i.e., how two quantities are related) of the query with the corresponding key.</p>
</blockquote>

<p><img class="img-fluid" src="/assets/post-images/Transformer/fig2.png" />
<span class="caption text-muted">Figure 2. <b>(left) Scaled Dot-Product Attention. (right) Multi-Head Attention</b>. Note that multiple attentions are computed in parallel in the multi-head attention structure.</span></p>

<p><strong>&lt; Scaled Dot-Product Attention &gt;</strong></p>

<p>This paper proposes a new attention mechanism called “Scaled Dot-Product Attention”, where the input consists of queries and keys of dimension $d_{k}$, and values of dimension $d_{v}$. Using this method, we first compute the dot products of the query with all keys, and then divide each by $\sqrt{d_{k}}$, and apply a softmax function to obtain the weights on the values.</p>

<p>In the actual implementation, the attention function can be computed on a set of queries simultaneously, by packing a queries into a matrix $\textbf{Q}$. Similarly, the keys and values can also be packed together into matrices $\textbf{K}$ and $\textbf{V}$, respectively. More precisely, the scaled dot-product attention is defined as:</p>

<p>$$\text{Attention}(\textbf{Q}, \textbf{K}, \textbf{V}) = \text{softmax} \big( \frac{\textbf{Q}\textbf{K}^{T}}{\sqrt{d_{k}}}\big) \textbf{V}$$</p>

<p>Unlike conventional dot-product attention, the authors decided to scale the output of the matrix multiplication since the dot product grows large in magnitude as $d_{k}$ gets larger, pushing the softmax function into regions where it has extremely small gradients. Thus, the dot products are scaled by $1 / \sqrt{d_{k}}$ to counteract this effect.</p>

<p><strong>&lt; Multi-Head Attention &gt;</strong></p>

<p>The authors found it beneficial to linear project the queries, keys, and values $h$ times with <strong>different</strong>, <strong>learned</strong> linear projections to $d_{k}$, $d_{k}$ and $d_{v}$ dimensions, respectively. The attention function is computed on each different version of queries, keys and values while taking advantage of massive parallelism thanks to the capability of modern GPUs. The result of each computation is $d_{v}$ dimensional output values. These are concatenated and once again projected, resulting in the final values. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. That is, attentions on different embeddings focus differently on compatibility between elements in the input &amp; output sequences. Mathematically, the multi-head attention is defined as:</p>

<p>$$ 
\begin{gather}
\text{MultiHead}(\textbf{Q}, \textbf{K}, \textbf{V}) = \text{Concat}(\text{head}<em>{1}, \dots, \text{head}</em>{h}) W^{O} <br />
\text{where } \text{head}<em>{i} = \text{Attention}(\textbf{Q} \textbf{W}</em>{i}^{\text{Q}}, \textbf{K} \textbf{W}<em>{i}^{\textbf{K}}, \textbf{V} \textbf{W}</em>{i}^{\textbf{V}}),
\end{gather}
$$</p>

<p>where $\textbf{W}_{i}^{\textbf{Q}} \in \mathbb{R}^{d_{\text{model}} \times d_{k}}$, $\textbf{W}_{i}^{K} \in \mathbb{R}^{d_{\text{model}} \times d_{k}}$, $\textbf{W}_{i}^{\textbf{V}} \in \mathbb{R}^{d_{\text{model}} \times d_{v}}$, $\textbf{W}^{O} \in \mathbb{R}^{hd_{v} \times d_{\text{model}}}$ are matrices representing trainable parameters. In this work, the authors used $h = 8$ parallel attention layers, or heads. For each of these they used $d_{k} = d_{v} = d_{\text{model}} / h = 64$. Note that the total computational cost is similar to that of single-head attention with full dimensionality due to the reduced dimension of each head.</p>

<p><strong>&lt; Applications of Attention in the Model &gt;</strong></p>

<p>The Transformer uses multi-head attention in three different ways:</p>

<ul>
  <li>In “encoder-decoder attention” layer only in the attention “blocks” of the decoder, the queries are from the previous decoder layer, and the memory keys and values come from the output of the encoder. <strong>This structure allows every position in the decoder to attend over all positions in the input sequence.</strong> This behavior is similar to the typical encoder-decoder attention mechanisms in Seq2Seq models.</li>
  <li>The encoder itself has self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. <strong>Each position in the encoder can attend to all positions in the previous layer of the encoder (and that’s why it is called <em>“self”</em>-attention).</strong></li>
  <li>Similar to the encoder, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder <strong>up to and including that position</strong>. It would not make any sense if a word at some timestep in the <em>predicted</em> sentence attends to other words that are not even predicted yet (i.e., unknown to the model). That being said, the Transformer has to prevent leftward information flow in the decoder to preserve the auto-regressive property. In practice, this can be done inside of scaled dot-product attention layer by masking out all values (i.e., set to $\infty$), that are not allowed to be observed by the decoder itself, in the input of the softmax.</li>
</ul>

<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>

<p>Each of the layers in the encoder and decoder also contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between:</p>

<p>$$\text{FFN}(x) = \max (0, x \textbf{W}_{1} + b_{1}) \textbf{W}_{2} + b_{2},$$</p>

<p>where the matrices $\textbf{W}_{1}$ and $\textbf{W}_{2}$ represent the trainable parameters. Note that while the different positions share the same linear transformation, the parameters of transformations differ from layer to layer. This can be implemented by two convolutions with kernel size 1. The dimensionality of input and output is $d_{\text{model}} =512$, and the inner-layer has dimensionality $d_{ff} = 2048$.</p>

<h3 id="embeddings-and-softmax">Embeddings and Softmax</h3>

<p>Just like other sequence transduction models, this work used learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\text{model}}$. The authors also used the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In the Transformer, the same weight matrix is shared among the two embedding layers and the pre-softmax linear transformation. Moreover, the weights of the embedding layers are scaled by multiplying $\sqrt{d_{\text{model}}}$.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Here is one important fact about the Transformer:</p>

<blockquote>
  <p>The Transformer contains no recurrence and no convolution.</p>
</blockquote>

<p>The reason why recurrent neural networks had been widely adopted in various problems involving sequences was clear - their sequential structure fits well, and naturally models sequential data. <strong>Then, without recurrence and convolution, how can we teach the Transformer to be aware of the order of tokens in the given sequences?</strong> To do so, one must provide the Transformer some information about the relative or absolute position of the tokens in the sequence. To this end, this paper proposes to add “positional encodings” to the input embeddings at the bottom of the encoder and decoder stacks. The positional encodings have the same dimensionality $d_{\text{model}}$ as the embeddings, so that the two can be summed. While there are many choices of positional encodings available, the authors used sine and cosine functions of different frequencies:</p>

<p>$$
\begin{gather}
\text{PE}<em>{(pos, 2i)} = \sin (pos / 10000^{2i / d</em>{\text{model}}}) <br />
\text{PE}<em>{(pos, 2i + 1)} = \cos (pos / 10000^{2i / d</em>{\text{model}}}),
\end{gather} $$</p>

<p>where $pos$ is the position and $i$ is the dimension. In other words, each dimension of the positional encoding corresponds to a sinusoid. Note that the wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.</p>

<h2 id="why-self-attention">Why Self-Attention</h2>

<p><img class="img-fluid" src="/assets/post-images/Transformer/table1.png" />
<span class="caption text-muted">Figure 2. <b>Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types</b>.</span></p>

<p>The authors also provide thorough comparisons between the proposed self-attention layers and the recurrent and convolutional layers that were commonly used for mapping one variable-length sequence of symbol representations $(x_{1}, \dots, x_{n})$ to another sequence of equal length $(z_{1}, \dots, z_{n})$, with $x_{i}, z_{i} \in \mathbb{R}^{d}$ (e.g., a hidden layer in a typical sequence transduction encoder or decoder). The authors focus on three aspects:</p>

<ol>
  <li>The total computational complexity per layer.</li>
  <li>The amount of computation that can be parallelized, measured by the minimum number of sequential operations required.</li>
  <li>The path length between long-range dependencies in the network. This is important since learning long-range dependencies is a key challenge in many sequence transduction tasks. And one key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter, the better.</li>
</ol>

<p>The table 1 presents side-by-side comparison on each case. In terms of the maximum path length, a self-attention layer is able to connect all positions with a constant number of sequentially executed operations, while a recurrent layer requires $O(n)$ sequential operations.</p>

<p><img class="img-fluid" src="/assets/post-images/Transformer/fig3.png" />
<span class="caption text-muted">Figure 3. <b>An example of the attention mechanism following long-distance dependencies in one self-attention layer of the encoder</b>.</span></p>

<p>When it comes to computational complexity, self-attention layers are much faster than recurrent layers when $n « d$, which is common for sequences and their representations handled in state-of-the-art models in machine translations (e.g., word-piece, byte-pair, etc). For very long sequences, one may come up with a restricted version of self-attention which considers only a neighborhood of size $r$ in the input sequence centered around the respective output position.</p>

<p>Finally, a convolutional layer with kernel size $k &lt; n$ cannot connect all pairs of input and output positions. Otherwise, it requires a stack of $O(n / k)$ convolutional layers in the case of contiguous kernels, or $O \big( \log_{k} (n) \big)$ in the case of dilated convolutions. At the same time, convolutional layers are more expensive than recurrent layers, by a factor of $k$.</p>

<p><img class="img-fluid" src="/assets/post-images/Transformer/fig4.png" />
<span class="caption text-muted">Figure 4. <b>Many of the attention heads exhibit behaviour that seems related to the structure of the sentence</b>.</span></p>

<p>Additionally, self-attention can be used to analyze the internal of models. The output of self-attention layers are in fact the data so called <em>attention distributions</em>. Experimental results tell us that different attention heads learn to perform different tasks, and at the same time, many of them seem to be related to the syntactic and semantic structure of the sentences.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This paper presents the Transformer, the first sequence transduction model based entirely on attention. Compared with most commonly used encoder-decoder architectures, the Transformer benefits from multi-headed self-attention layers in both encoder and decoder in terms of performance and efficiency.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘GANSpace: Discovering Interpretable GAN Controls’</title><link href="https://dvelopery0115.github.io/2021/08/22/GANSpace.html" rel="alternate" type="text/html" title="Summary of ‘GANSpace: Discovering Interpretable GAN Controls’" /><published>2021-08-22T00:00:00+00:00</published><updated>2021-08-22T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/22/GANSpace</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/22/GANSpace.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>While generative adversarial networks (GANs) such as BigGAN and StyleGAN are powerful image synthesis models that are able to generate a wide variety of high-quality images, they provide little direct control over image contents.</li>
  <li>Current approaches suggested for allowing more control over GANs mostly focus on supervised learning of latent directions or training GAN with labeled images both require tremendous, expensive manual supervision for each new control to be learned.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>This paper shows the way how to identify new interpretable control directions for existing GANs without requiring supervision or expensive optimization.</li>
  <li>This paper show that semantically meaningful directions in GAN latent spaces can be discovered by applying Principal Component Analysis (PCA) in latent space for StyleGAN, and feature space for BigGAN.</li>
  <li>Additionally, the authors show how BigGAN can be modified to allow StyleGAN-like layer-wise style mixing and control without retraining.</li>
  <li>Last but not least, the authors show that layer-wise decomposition of PCA edit directions leads to many interpretable controls. Note that they do not explicitly target an attribute and the direction in latent space associated with its modification. Rather, it’s up to the user to identify useful control directions and annotate them properly.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This paper suggests to use PCA for the task of analyzing &amp; exploring the latent spaces of widely used GAN models such as StyleGAN, StyleGAN2, and BigGAN. The authors show that the principal directions obtained from PCA are related to different attributes of generated images, and further can be used for image editing.</p>

<h1 id="methods">Methods</h1>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig1.png" />
<span class="caption text-muted">Figure 1. <b>Sequences of image edits performed using control discovered with the proposed method</b>.</span></p>

<h2 id="discovering-gan-controls">Discovering GAN Controls</h2>

<p>This paper suggests new techniques for augmenting existing GANs with new control variables. These techniques are very simple, thus possesses an advantage over previous methods: these methods enable a range of powerful tools for analysis and control of GANs with very little effort.</p>

<p>💡 <strong>IMPORTANT NOTE: In this paper, the authors worked exclusively with pretrained GANs.</strong></p>

<h3 id="background">Background</h3>

<p>Let us briefly review GAN representation first. The most basic GAN works on a probability distribution $p(\textbf{z})$, from where a latent vector $\textbf{z}$ is sampled. And importantly, there is a neural network $G(\textbf{z})$ which maps such latent vector to an output image $I: \textbf{z} \sim p(\textbf{z}), I = G(\textbf{z})$. Furthermore, the network can be decomposed into a series of $L$ intermediate layers $G_{1}, \dots, G_{L}$, where the first layer $G_{1}$ takes a latent vector as an input and the each of following layers $G_{i}$ take an intermediate feature from the previous layer $G_{i-1}$ and outputs another intermediate feature for its follower. Note that the output of the last layer $G_{L}$ is an RGB image.</p>

<p>In the case of BigGAN model, the intermediate layers also take the latent vector as input:</p>

<p>$$\textbf{y}_{i} = G_{i} (\textbf{y}_{i-1}, \textbf{z}),$$</p>

<p>which are called Skip-$z$ inputs.</p>

<p>In a StyleGAN model, however, the first layer takes a constant feature map input $\textbf{y}_{0}$. And then, the output is controlled by a non-linear function of $\textbf{z}$ as input to intermediate layers:</p>

<p>$$\textbf{y}_{i} = G(\textbf{y}_{i-1}, \textbf{w}) \quad \text{with } \textbf{w} = M(\textbf{z}),$$</p>

<p>where $M$ is an 8-layer multi-layer perceptron. StyleGAN applies different (learned) affine transformations to $\textbf{w}$ at each layer $G_{i}$, turning it into a style code $\textbf{w}_{i}$, which varies over different layers of the synthesis network $G$. It is empirically shown that each of style codes affects different semantic element(s) of generated images.</p>

<h3 id="principal-components-and-principal-feature-directions">Principal Components and Principal Feature Directions</h3>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig2.png" />
<span class="caption text-muted">Figure 2. <b>2D illustration of identifying a principal activation direction for BigGAN</b>.</span></p>

<p>… And we return to the fundamental question:</p>

<blockquote>
  <p>“How can we find useful directions in $\textbf{z}$ space then?”</p>
</blockquote>

<p>In fact, the isotropic prior distribution $p(\textbf{z})$ provides almost zero information about directions associated with meaningful modifications on images. What makes it even worse is that the distribution of outputs in the high-dimensional pixel space is extremely complex, thus difficult to reason about. The main observation is that <strong>the principal components of feature tensors on the early layers of GANs represent important factors of variation</strong>.</p>

<hr />

<p><strong>StyleGAN</strong></p>

<p>Analysis of the latent space of StyleGAN is the simpliest example we can start with (actually there are only two GANs discussed here). In the context of StyleGAN, the goal is now finding the principal axes of $p(\text{w})$. To this end, the authors sampled $N$ random vectors $\textbf{z}_{1:N}$, and computed the corresponding $\textbf{w}_{i} = M(\textbf{z}_{i})$ values. Then they carried out PCA with these $\textbf{w}_{1:N}$ values. The result of PCA is a basis $\textbf{V}$ for $\mathcal{W}$. More generally, given a new image decoded from $\textbf{w}$, one can edit it by varying PCA coordinates $\textbf{x}$ before feeding it to the synthesis network:</p>

<p>$$\textbf{w}^{\prime} = \textbf{w} + \textbf{V} \textbf{x},$$</p>

<p>where each entry $x_{k}$ of $\textbf{x}$ is a separate control parameter. The entries $x_{k}$ are initialized to zero until modified by a user.</p>

<hr />

<p><strong>BigGAN</strong></p>

<p>When it comes to BigGAN, the procedure becomes more complicated, since the $\textbf{z}$ distribution is not learned, and there is no $\textbf{w}$ latent that parameterizes the output image. The authors instead performed PCA at an intermediate network layer $i$, and then transferred these directions back to the $\textbf{z}$ latent space, as follows:</p>

<p>Specifically, the authors first sampled $N$ random latent vectors $\textbf{z}_{1:N}$. These are processed through the model to produce $N$ feature tensors $\textbf{y}_{1:N}$ at the $i$-th layer, where $\textbf{y}_{j} = \hat{G}_{i}(\textbf{z}_{j})$. Then the authors performed PCA on the $N$ feature tensors $\textbf{y}_{1:N}$ resulting in a low-rank basis matrix $\textbf{V}$, and the data mean $\boldsymbol{\mu}$. The PCA coordinates $\textbf{x}_{j}$ of each feature tensor are then computed by projection: $\textbf{x}_{j} = \textbf{V}^{T} (\textbf{y}_{j} - \boldsymbol{\mu})$.</p>

<p>This basis is then transferred to the latent space by linear regression, which is done as follows:</p>

<p>The authors started with an individual basis vector $\textbf{v}_{k}$ (i.e., a column of $\textbf{V}$), and the corresponding PCA coordinates $x_{1:N}^{k}$, where $x_{j}^{k}$ denotes the $k$-th element of $\textbf{x}_{j}$. Finally, the corresponding latent basis vector $\textbf{u}_{k}$ is computed as:</p>

<p>$$\textbf{u}_{k} = \text{argmin} \sum_{j} \Vert \textbf{u}_{k} x_{j}^{k} - \textbf{z}_{j} \Vert^{2}$$</p>

<p>and this basis vector is a latent direction corresponding to this principal component. That being said, the entire basis can be computed at once with:</p>

<p>$$\textbf{U} = \text{argmin} \sum_{j} \Vert \textbf{U} \textbf{x}_{j} - \textbf{z}_{j} \Vert^{2}$$</p>

<p>computing $\textbf{U}$ can be done using a standard least-squares solver, without any additional orthogonality constraints. Each column of $\textbf{U}$ aligns to the variation along the corresponding column of $\textbf{V}$. The columns $\textbf{u}_{k}$’s are called <em>principal directions</em>. Knowing principal directions, one can edit images using similar (actually, almost the same) method as in StyleGAN:</p>

<p>$$\textbf{z}^{\prime} = \textbf{z} + \textbf{U} \textbf{x},$$</p>

<p>where $x_{k}$, $k$-th element of $\textbf{x}$, specifies the offset along the column $\textbf{u}_{k}$ of the principal dimension matrix.</p>

<hr />

<h3 id="layer-wise-edits">Layer-wise Edits</h3>

<p>Given the directions found with PCA, this paper shows that these can be further decomposed into interpretable edits by applying them only to certain layers.</p>

<hr />

<p><strong>StyleGAN</strong></p>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig3.png" />
<span class="caption text-muted">Figure 3. <b>(Rows 1-3) illustrate the three largest principal components in the intermediate latent space of StyleGAN2. (Rows 4-5) demonstrate the effect of constraining the variation to a subset of the layers</b>.</span></p>

<p>As its name implies, StyleGAN allows us to control different <em>“styles”</em> of the image by manipulating the $\textbf{w}_{i}$’s, intermediate latent vectors fed to each layer of the synthesis network. Given an image with latent vector $\textbf{w}$, layerwise edits can be done by modifying only the $\textbf{w}$ inputs to a range of layers, leaving the other layers’ inputs fixed. In this paper, the notation $\text{E}(\textbf{v}_{i}, \text{j}-\text{k})$ was used to denote edit directions. For example, $\text{E}(\textbf{v}_{1}, 0-3)$ means moving along the direction of (PCA) component $\textbf{v}_{1}$ at the first four layers only. $\text{E}(\textbf{v}_{2}, \text{all})$ means moving along the direction of component $\textbf{v}_{2}$ at all layers. Meanwhile, edits in the $\mathcal{Z}$ latent space are denoted $\text{E} (\textbf{u}_{i}, \text{j-k})$. The effects of such modifications are illustrated in the figure 3.</p>

<hr />

<p><strong>BigGAN</strong></p>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig4.png" />
<span class="caption text-muted">Figure 4. <b>Style variation in BigGAN</b>. Experiments reveal the fact that modifications of the latent vectors in the middle of the network affect the style of the generated image in semantically meaningful ways.</span></p>

<p>In contrast to StyleGAN, BigGAN does not have any layer-wise control mechanism. However, the authors discovered that <strong>BigGAN can be modified to produce behavior similar to StyleGAN, by varying the intermediate Skip-$z$ inputs $\textbf{z}_{i}$ separately from the latent</strong> $\textbf{z}$: $G(\textbf{y}_{i-1}, \textbf{z}_{i})$. In this setting, the latent inputs $\textbf{z}_{i}$ can vary individually across different layers of BigGAN, similar to style mixing of StyleGAN. In the beginning, all inputs are determined by an initially sampled or estimated $\textbf{z}$, but those can be edited independently yielding different effects to generated images. Surprisingly, while BigGAN is not trained with style mixing regularization, it models images in a form of style &amp; content hierarchy.</p>

<p>Similar to StyleGAN, changing the latent vectors at lower layers is related to lower-level style edits. However, the latent space of BigGAN is more entangled than that of StyleGAN due to the difference between their structures and training schemes. Note that the Skip-$z$ connections in BigGAN were not originally intended for style resampling, thus such entanglement is inevitable. Still, there is no doubt that they are useful for layer-wise editing.</p>

<hr />

<h2 id="findings-and-results">Findings and Results</h2>

<p>This paper presents a number of discoveries from the PCA analysis, as well as comparisons to the baseline. The authors show edits discovered on state-of-the-art pretrained GANs, including:</p>

<ul>
  <li>BigGAN512-deep</li>
  <li>StyleGAN (Bedrooms, Landscapes, WikiArt training sets)</li>
  <li>StyleGAN2 (FFHQ, Cars, Cats, Church, Horse training sets)</li>
</ul>

<p>The analysis on these renowned generative model reveals interesting properties of StyleGAN and BigGAN models.</p>

<h3 id="gan-and-pca-properties">GAN and PCA Properties</h3>

<p>One important observation obtained from all trained models is that:</p>

<blockquote>
  <p><strong>“Large-scale changes to geometric configuration and viewpoint are limited to the first 20 principal components ($\textbf{v}_{0} - \textbf{v}_{20}$). Modifying the rest of them leaves layout unchanged, and instead controls object appearance &amp; background and other details.”</strong></p>
</blockquote>

<p>Another interesting property discovered from PCA is that <strong>StyleGAN2’s latent distribution $p(\textbf{w})$ has a relatively simple structure.</strong> Specifically, the principal coordinates are nearly independent variables with non-Gaussian unimodal distributions. The authors also found that <strong>the first 100 principal components are sufficient to describe overall image appearance,</strong> and other 412 dimensions control subtle, but noticable changes in appearance.</p>

<p>Turning our attention to BigGAN, the authors found that BigGAN components appear to be class-independent (e.g., PCA components for one class were identical to PCA components for another class in the cases tested by the authors). That is, an editing direction found in one class can easily be transferred to other classes.</p>

<h3 id="model-entanglements-and-disallowed-combinations">Model entanglements and disallowed combinations</h3>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig5.png" />
<span class="caption text-muted">Figure 5. <b>Illustration of the significance of the principal components as compared to random directions in the intermediate latent space of StyleGAN2</b>.</span></p>

<p><strong>It is suspected that some properties of GAN principal components were inherited from GANs’ training sets</strong>. These properties may be desirable in some cases, or prevent the complete disentanglement we all desire. Some of these can also be thought as undesirable biases of the trained GAN. For example, in the case of StyleGAN2 trained on the FFHQ dataset, geometric changes are limited to rotations in the first 3 components. One possible reason for this behavior is the carefully aligned training set, since none of the modifications affected the translation at all.</p>

<p>In the models on human faces, the authors observed “disallowed combinations”, the attributes that the model will not apply to certain faces. For example, the “wrinkles” edit will age and add wrinkles to adult faces, while having no significant effect on a child’s face. Similarily, “makeup and lipstick” edits add or remove makeup to female faces, but have little or no effect on male faces. This is another evidence that editability of GANs are inherently limited by the dataset they were trained on.</p>

<h1 id="conclusion">Conclusion</h1>

<p><img class="img-fluid" src="/assets/post-images/GANSpace/fig6.png" />
<span class="caption text-muted">Figure 6. <b>A selection of interpretable edits discovered by selective application of latent edits across the layers of several pretrained GAN models</b>.</span></p>

<p>This paper shows simple but powerful ways to modify images using pretrained GANs. Instead of training a new model which is computationally expensive and time consuming, the authors investigated existing general-purpose image representations and discovered techniques for controlling them. This work presents a new way of analyzing image representations, discovering sophiscated control techniques within GAN spaces by leveraging PCA in unsupervised manner.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort’</title><link href="https://dvelopery0115.github.io/2021/08/20/DatasetGAN.html" rel="alternate" type="text/html" title="Summary of ‘DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort’" /><published>2021-08-20T00:00:00+00:00</published><updated>2021-08-20T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/20/DatasetGAN</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/20/DatasetGAN.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig1.png" />
<span class="caption text-muted">Figure 1. <b>DatasetGAN overview</b>.</span></p>

<ul>
  <li>Modern deep neural networks are extremely data-hungry, benefiting from training on large-scale datasets, which are time consuming to annotate.</li>
  <li>Recently, semi-supervised learning has been a popular approach in the tasks of reducing the need for labeled data, by instead using a large unlabeled dataset.</li>
  <li>Several studies on the latent space of GANs (particularly StyleGAN) witnessed that GANs acquire semantic knowledge in their high dimensional latent space, and one can introduce various semantic changes by exploring such latent space.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Introduces DatasetGAN, an automatic procedure to generate massive datsets of high-quality semantically segmented images requiring minimal human effort.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This work proposes DatasetGAN, a novel way to utilize StyleGAN as a generator for massive scale datasets. With only little human supervision, the trained Style Interpreter architecture can synthesize infinite number of semantic (or keypoint) labels while running in parallel to StyleGAN backbone which generates images corresponding to labels from the Style Interpreter.</p>

<h1 id="methods">Methods</h1>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig2.png" />
<span class="caption text-muted">Figure 2. <b>Overall architecture of DatasetGAN</b>.</span></p>

<p>This paper introduces DatasetGAN that synthesizes image-annotation pairs. The authors focus on pixel-wise annotation tasks for semantic segmentation and keypoint prediction because these are typical problems that require manually annotated dataset where such annotation is extremely labor-intensive.</p>

<p><strong>The key insight of DatasetGAN is that generative models (e.g. GANs) trained to synthesize highly realistic images must acquire semantic knowledge in their high dimensional latent space.</strong> DatasetGAN is designed to utilize such powerful properties of image GANs. In particular, the authors trained a very simple MLP which maps the feature vector of each pixel to the semantic label on a small, human-labeled segmentation dataset. And they expected this information, acquired by label supervision, to be effectively propagated across the GAN’s latent space.</p>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig3.png" />
<span class="caption text-muted">Figure 3. <b>Small, detailed human-annotated face and car datasets</b>.</span></p>

<p>Inspite of its simple structure, the proposed architecture is turned out to be powerful. Specifically, the authors <strong>first synthesized a small number of images by utilizing a GAN architecture</strong>, StyleGAN in this case, and <strong>recorded their corresponding latent feature maps</strong>. On top of that, a human annotator labeled these images with a desired set of labels. Using this latent-annotated image pairs, the authors trained <strong>a simple ensemble of MLP classifiers</strong> that take the StyleGAN’s pixel-wise feature vectors, which is referred to as the <em>Style Interpreter</em>. One huge advantage this method takes compared to traditional, annotate-by-hand approaches is that this technique requires only a few annotated examples to achieve good accuracy.</p>

<p>After training the Style Interpreter, the authors used it as a subnetwork which runs in parallel with StyleGAN and outputs semantically segmented images that are ready to be used for training any related computer vision architectures.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>DatasetGAN uses StyleGAN as the generative backbone because it is capable of synthesizing high quality images. The StyleGAN’s synthesis network maps a latent code $\textbf{z} \in \mathcal{Z}$ drawn from a normal distribution to a realsitic image. Latent code $z$ is first mapped to an intermediate latent code $\text{w} \in \mathcal{W}$ by a mapping function. $\textbf{w}$ is then transformed to $k$ vectors, $\textbf{w}^{1}, \dots, \textbf{w}^{k}$, through $k$ learned affine transformations.</p>

<p>These $k$ transformed latent codes are injected as style information into $k / 2$ synthesis blocks in a progressive fashion (i.e. an image is gradually constructed by fusing style codes each accounting for different aspect into it). Specifically, each synthesis block consists of an upsampling layer and two convolutional layers. Each convolutional layer is followed by an adaptive instance normalization (AdaIN) layer controlled by its corresponding $\textbf{w}^{i}$. In this paper, the output feature maps from the $k$ AdaIN layers are denoted as $\{ S^{0}, S^{1}, \dots, S^{k}\}$.</p>

<h2 id="style-interpreter">Style Interpreter</h2>

<p>In this work, the authors interpreted StyleGAN as a “rendering” engine which takes latent codes representing “graphics attributes” that define what to render.</p>

<p>One important hypothesis which deserves our attention is that:</p>

<blockquote>
  <p>A flattened array of features that output a particular RGB pixel contains semantically meaningful information for rendering the pixel realistically.</p>
</blockquote>

<p>Using this as the foundation, the authors upsampled all feature maps $\{ S^{0}, S^{1}, \dots, S^{k} \}$ from AdaIN layers to the highest output resolution (i.e. the resolution of $S^{k}$) and concatenated them to get a 3D feature tensor $S^{*} = (S^{0, *}, S^{1, *}, \dots, S^{k, *})$ where $S^{k, *}$ stands for the upsampled $k$-th feature map and $(\cdot, \cdot)$ denotes concatenation. In this context, each pixel $i$ in the output image has its associated feature  vector $S_{i}^{*} = (S_{i}^{0, *}, S_{i}^{1, *}, \dots, S_{i}^{k, *})$, as shown in figure 2. The authors adopted three-layer MLP classifier on top of each feature vector to predict labels. The weights are shared across all pixels for simplicity.</p>

<h3 id="training-of-style-interpreter">Training (of Style Interpreter)</h3>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig4.png" />
<span class="caption text-muted">Figure 4. <b>Examples of synthesized images and labels from DatasetGAN for faces and cars</b>.</span></p>

<p>Suppose we have already synthesized few images and annotated them manually. Then the strategy used for training Style Interpreter is as follows:</p>

<p>Since per-pixel feature vectors $S_{i}^{*}$ are of high dimensionality (5056) and the feature map has  high spatial resolution (1024 at most), one cannot consume all image feature vectors in a batch due to the shortage of memory space. The authors came up with the idea of performing random sampling of feature vectors from each image, while ensuring that at least one sample is drawn from each labeled region.</p>

<p>Furthermore, different losses are used for different tasks. For semantic segmentation, the classifier is trained with cross-entropy loss. For keypoint prediction, the authors first built a Gaussian heatmap for each keypoint in the training set and let the MLP to fit the heat value for each pixel. <strong>One important point is that the weights of StyleGAN is fixed during the training procedure (i.e. gradients are not back propagated to the StyleGAN).</strong></p>

<p>The reason for adopting an ensemble of classifier was to amortize the effect of random sampling. In this paper, total $N = 10$ classifiers were trained. The authors used majority voting in each pixel at test time for semantic segmentation. And they averaged the $N$ heat values predicted by each of the $N$ classifiers in the case of keypoint prediction.</p>

<h2 id="datasetgan-as-a-labeled-data-factory">DatasetGAN as a Labeled Data Factory</h2>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig5.png" />
<span class="caption text-muted">Figure 5. <b>Examples of synthesized images and labels from DatasetGAN for birds, cats, and bedrooms</b>.</span></p>

<p>Once trained, the Style Interpreter can be used as a label-synthesis branch running in parallel to the StyleGAN backbone, forming the entire DatasetGAN architecture. Using the same latent code $\textbf{z} \in \mathcal{Z}$ sampled from StyleGAN’s latent space, we can synthesize both photorealistic image and corresponding semantic label simultaneously. In practice, synthesizing an image-annotation pair requires a forward pass through StyleGAN, which takes 9 seconds on average.</p>

<p>However, StyleGAN occasionally fails introducing noise in the synthesized dataset. The authors noticed that the StyleGAN’s discriminator score is not a robust measure of failure and also found that utilizing their ensemble of classifiers to measure the uncertainty of a synthesized example is a more robust approach. For measurement of uncertainty, this work adopted the Jensen-Shannon (JS) divergence for a pixel. In the case of image uncertainty, they summed over all image pixels. According to the computed uncertainty, top 10% most uncertain images were filtered out.</p>

<h1 id="conclusion">Conclusion</h1>

<p><img class="img-fluid" src="/assets/post-images/DatasetGAN/fig6.png" />
<span class="caption text-muted">Figure 6. <b>3D Application</b>.</span></p>

<p>The authors proposed a simple but powerful approach for semi-supervised learning with few labels. They explored the learned latent space of the state-of-the-art generative model StyleGAN, and developed an effective classifier which can be trained on only a few human-annotated images. The DatasetGAN, consists of StyleGAN backbone and a novel Style Interpeter architecture, is able to synthesize large labeled datasets which then can be used for training various computer vision architectures.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘Designing an Encoder for StyleGAN Image Manipulation’</title><link href="https://dvelopery0115.github.io/2021/08/19/e4e.html" rel="alternate" type="text/html" title="Summary of ‘Designing an Encoder for StyleGAN Image Manipulation’" /><published>2021-08-19T00:00:00+00:00</published><updated>2021-08-19T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/19/e4e</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/19/e4e.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig1.png" />
<span class="caption text-muted">Figure 1. <b>Real image editing via StyleGAN inversion using the proposed method in this paper</b>.</span></p>

<ul>
  <li>Applying pretrained unconditional generators (e.g. GANs) on real image editing is still an on-going problem as it requires the inversion of the images (to be edited) into the appropriate latent space. Therefore, a high-quality inversion scheme is necessary for such editing techniques.</li>
  <li>High-quality inversion is characterized by two aspects: (1) <em>reconstruction</em> of input image from its latent code encoded by the encoder, (2) <em>editability</em> achieved by leveraging the editing capabilities of the latent space to obtain meaningful and realistic edits of the input image.</li>
  <li>The quality of reconstruction can be evaluated by measuring two properties: <em>distortion</em> (per-image input-output similarity) and <em>perceptual quality</em> (how realistic the reconstructed image is). Ideally, the lower the distortion and the higher the perceptual quality is the better.</li>
  <li>However, it turns out that the three quantities - distortion, perceptual quality, and editability - are closely related to each other. And the trade-off between these three should be analyzed thoroughly.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Analyzes the complex latent space of StyleGAN and suggests a novel view of its structure.</li>
  <li>Presents the innate tradeoffs among distortion, perception, and editability.</li>
  <li>Characterizes the tradeoffs and design two means for an encoder to control them.</li>
  <li>Presents <em>e4e</em>, a novel encoder specifically designed to allow for the subsequent editing of inverted real images.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This paper proposes a novel approach for GAN inversion by designing an encoder along with training schemes that guide the encoder to embedd images into StyleGAN’s latent space. Also, this paper provides thorough  analysis on the trade-offs among the quantities that are directly related to the quality of decoded images - distortion, perceptual quality, and editability.</p>

<h1 id="methods">Methods</h1>

<h2 id="terminology">Terminology</h2>

<p>Briefly speaking, StyleGAN is a model consists of two key components:</p>

<ol>
  <li>A mapping function that maps a latent code $\textbf{z} \in \mathcal{Z} = \mathcal{N} (\mu, \sigma^{2})$ into a <em>style code $\textbf{w} \in \mathcal{W} \subsetneq \mathbb{R}^{512}$,</em></li>
  <li>A generator (synthesis network) which takes the style code, replicated several times, as input, and generates an image.</li>
</ol>

<p>From now on, the distribution $\mathcal{W}$ will be called the <em>range</em> of the mapping function.</p>

<p>It has been empirically shown that a single latent vector is not powerful enough to represent an image that is inverted into StyleGAN’s latent space. To circumvent such issue, one possible solution for it is to introduce more latent vectors, say $k$ number of them, different from each other. Then the space obtained by extension can be denoted by $\mathcal{W}^{k} \subsetneq \mathbb{R}^{k \times 512}$ where $k$ is the number of style inputs of the generator.</p>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig2.png" />
<span class="caption text-muted">Figure 2. <b>An illustration of different latent spaces in 1D and 2D</b>.</span></p>

<p>We can push the boundary of expressiveness even further by inputting style codes that are not from the true distribution of $\mathcal{W}$. Such extension can be achieved by either taking a single style code and replicate it over the blocks of StyleGAN, or taking $k$ different individual codes. Such codes are denoted by $\mathcal{W}_{*}$ and $\mathcal{W}_{*}^{k}$, respectively.</p>

<p>Finally, the characteristics of different latent spaces can be summarized as follows:</p>

<p><img class="img-fluid" src="/assets/post-images/e4e/table1.png" />
<span class="caption text-muted">Table 1. <b>A concise summary outlining the differenes between various latent spaces</b>.</span></p>

<h2 id="the-gan-inversion-tradeoffs">The GAN Inversion tradeoffs</h2>

<h3 id="preliminaries">Preliminaries</h3>

<p>Most of the research on StyleGAN’s latent code can be categorized into two tasks: <em>GAN inversion</em> and <em>latent space manipulation</em>.</p>

<p>In previous works, the two tasks were considered as follows.</p>

<ol>
  <li><em>GAN inversion</em>: Given an image $x$, infer a latent code $\textbf{w}$ which is then decoded by the generator to reconstruct the input as closely as possible.</li>
  <li><em>Latent space manipulation</em>: Given a latent code $\textbf{w}$, infer a new latent code $\textbf{w}^{\prime}$, such that the image $G(\textbf{w}^{\prime})$ synthesized by decoding the code is an image which can be obtained by applying semantically meaningful modification to $G(\textbf{w})$.</li>
</ol>

<p>In fact, inverting GAN alone has less practical significance compared to that of editing images by controlling their corresponding latent vectors. <strong>What one could do with the network that takes an image and only outputs (ideally) complete replica of the input?</strong></p>

<p>Similarily, image manipulation by exploiting latent space would be useless if edited images are not realistic when perceived by ourselves. <strong>Who wants an image editor that introduces too much distortions (or noises) as you modify certain aspect of images?</strong> Therefore, the desirable way is to find the sweet spot that will enable us to modify images in while keeping them visually plausible.</p>

<p>Therefore, the GAN inversion methods should be evaluated by taking both reconstruction and editability into account.</p>

<p>In this work, the reconstruction is assumed to have two distinct properties - <em>distortion</em> and <em>perceptual quality</em>. Distortion is rigorously defined as $\mathbb{E}_{x \sim p_{X}}[\Delta (x, G(\textbf{w}))]$  where $p_{X}$ is the distribution of the real images, and $\Delta(x, G(\textbf{w}))$ is an image-space difference measure (often $\ell_{1}$ or $\ell_{2}$) between images $x$ and $G(\textbf{w})$. On the other hand, perceptual quality measures how realistic the reconstructed images are.</p>

<p>Speaking of editability, it is desirable to have latent space where one can find latent space directions corresponding to disentangled semantic edits in the image-space. At the same time, modifications introduced by exploring such latent space should not harm the perceptual quality of the output images.</p>

<h3 id="distortion-editability--distortion-perception-trade-offs">Distortion-Editability &amp; Distortion-Perception Trade-offs</h3>

<p>Let us analyze the distortion, perceptual quality, and editability of different regions in StyleGAN’s latent space. As mentioned previously, $\mathcal{W}_{*}^{k}$ differs from $\mathcal{W}$ in two ways. Specifically:</p>

<ol>
  <li>$\mathcal{W}_{*}^{k}$ may contain different style codes at different style-modulation layers.</li>
  <li>Each individual style code is not bound to the true distribution of $\mathcal{W}$, but can take any value from $\mathbb{R}^{512}$.</li>
</ol>

<p>Several studies on the properties of such spaces reveal that $\mathcal{W}_{*}^{k}$ achieves lower (i.e. better) distortion than $\mathcal{W}$. Moreover, the authors found that $\mathcal{W}$ is more editable. The observation is illustrated in the figure below. Evidently, there is inherent trade-off between distortion and editability.</p>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig3.png" />
<span class="caption text-muted">Figure 3. <b>The editability gap between different latent spaces</b>.</span></p>

<p>Since StyleGAN is originally trained in the $\mathcal{W}$ space, it is not a surprise that $\mathcal{W}$ is more well-behaved and has better perceptual quality compared to its $\mathcal{W}_{*}^{k}$ counterpart. In contrast, note that the higher dimensionality of $\mathcal{W}_{*}^{k}$ and the structure of StyleGAN gave $\mathcal{W}_{*}^{k}$ greater expressive power.</p>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig4.png" />
<span class="caption text-muted">Figure 4. <b>An example of the distortion-perception trade-off</b>.</span></p>

<p>The authors claim that the distortion-editability and the distortion-perception trade-offs also exist within the $\mathcal{W}_{*}^{k}$. Furthermore, the trade-offs are controlled by the proximity to $\mathcal{W}$. Precisely, as $\mathcal{W}_{*}^{k}$ gets closer to $\mathcal{W}$, the distortion worsens while the editability and perceptual quality improve. That being said, it is desirable to find means those would allow one to control the proximity of an encoded image to $\mathcal{W}$. From the next section, we will discuss two principles and mechanisms for controlling this proximity.</p>

<h2 id="designing-an-encoder">Designing an encoder</h2>

<p>Learned from the several observations, this paper present principles for designing an encoder and novel training scheme to explicitly address the proximity to $\mathcal{W}$ While there are two mainstreams of GAN inversion methodologies: (i) latent code optimization, (ii) encoder-based methods, this paper focuses on the latter for several reasons:</p>

<ol>
  <li>Encoder-based methods are way more faster as they infer a latent code with a single forward pass. → optimization based methodologies are expensive since they work in per-image basis.</li>
  <li>Due to the piece-wise smoothness of CNNs, the output of an encoder lies in a tight space that is more suitable for editing. → an optimization based inversion may encode an image to an arbitrary point in the latent space.</li>
</ol>

<p>From now on, we will be discussing the two principles for controlling the proximity to $\mathcal{W}$, where each is defined using a dedicated training paradigm to encourage the encoder to map into regions in $\mathcal{W}_{*}^{k}$ that lie close to $\mathcal{W}$. Moreover, these principles were found to be most effective when applied jointly.</p>

<h3 id="minimize-variation-from-mathcalw">Minimize Variation (from $\mathcal{W}$)</h3>

<p>One way to get closer to $\mathcal{W}$ is to encourage the inferred $\mathcal{W}_{*}^{k}$ latent codes to lie closer to $\mathcal{W}_{*}$. In other words, we should minimize the variance between the different style codes. To this end, the authors propose a novel ‘progressive’ training scheme.</p>

<p>Let $E(x) = (\textbf{w}_{0}, \textbf{w}_{1}, \dots, \textbf{w}_{N-1})$ denote the output of the encoder, where $N$ is the number of style-modulation layers. Encoders are normally trained directly into $\mathcal{W}_{*}^{k}$, that is, they learn each $\textbf{w}_{i}$ separately and simultaneously. Apart from such approach, this paper suggests to infer a single latent code, namely $\textbf{w}$, and a set of offsets from $w$. Formally, the encoder will learn to output a collection of latent codes of form $E(x) = (\textbf{w}, \textbf{w} + \Delta_{1}, \dots, \textbf{w} + \Delta_{N-1})$.</p>

<p>Specifically, $\Delta_{i}$’s are initialized to zero at the beginning and the encoder is trained to infer a single $\mathcal{W}_{*}$ code. Then the network is gradually allowed to learn a different $\Delta_{i}$ for each $i$ sequentially. This allows the encoder to gradually expand from $\mathcal{W}_{*}$ towards $\mathcal{W}_{*}^{k}$. Recalling the structure of StyleGAN’s synthesis network and the semantic meaning of each input layer of it, the proposed training scheme allows the encoder to first learn a coarse reconstruction of the input image and then to gradually refine it by optimizing the offset vectors.</p>

<p>The authors observed that low frequency details greatly affect the distortion quality. Therefore, the suggested progressive training scheme can be thought to be doing the following:</p>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig5.png" />
<span class="caption text-muted">Figure 5. <b>Another illustration of latent spaces in 1D and 2D</b>. Moving along the direction of red arrow (toward diagonal line) brings a latent vector closer to $\mathcal{W}_{*}$, while the directions of blue arrows are the paths to get closer to the distribution $\mathcal{W}^{2}$.</span></p>

<blockquote>
  <p>“It first focuses on improving the low frequency distortion by tuning the coarse-level offsets, and then it gradually improves the offsets corresponding to higher frequency details over time.”</p>
</blockquote>

<p>With the visual aid of figure 5, this process is equivalent to starting from a style code lying on the main diagonal and then slightly diverging from it by allowing small perturbations. <strong>Keep in mind that the encoder outputs offset vectors not latents directly.</strong></p>

<p>To explicitly enforce a proximity to $\mathcal{W}_{*}$, the authors added an $L_{2}$ delta-regularization loss:</p>

<p>$$\mathcal{L}_{\text{d-reg}} (w) = \sum_{i=1}^{N-1} \Vert \Delta_{i} \Vert_{2}.$$</p>

<h3 id="minimize-deviation-from-mathcalwk">Minimize Deviation From $\mathcal{W}^{k}$</h3>

<p>Another way to make latents get closer to $\mathcal{W}$ is to encourage the $\mathcal{W}_{*}^{k}$ latent codes obtained by the encoder to lie closer to $\mathcal{W}^{k}$. We can achieve this by enforcing the individual style codes to lie within the actual distribution of $\mathcal{W}$. To this end, the authors use a <em>latent discriminator</em> trained in an adversarial manner to discriminate between real samples from the $\mathcal{W}$ space and the encoder’s learned latent codes.</p>

<p>Guided by such latent discriminator, the encoder will learn to infer latent codes lying in $\mathcal{W}$ as opposed to $\mathcal{W}_{*}$. This paper uses a single latent discriminator, denoted $D_{\mathcal{W}}$, which operates on each latent code entry separately. The discriminator iterates over every inferred latent code $E(x)_{i}$ computing the GAN loss for each of them. We then simply take the average over all $i$-s.</p>

<p>Precisely, the form of used GAN loss (non-saturating GAN loss with $R_{1}$ regularization) is as follows:</p>

<p>$$
\begin{gather}
\mathcal{L}<em>{\text{adv}}^{D} = - \underset{\textbf{w} \sim \mathcal{W}}{\mathbb{E}} [\log D</em>{\mathcal{W}}(\textbf{w})] - \underset{x \sim p_{X}}{\mathbb{E}} [\log (1 - D_{\mathcal{W}} (E(x)<em>{i}))] + \\ \frac{\gamma}{2} \underset{\textbf{w} \sim \mathcal{W}}{\mathbb{E}} \Big[ \Vert \nabla</em>{\textbf{w}} D_{\mathcal{W}} (\textbf{w}) \Vert_{2}^{2} \Big], <br />
\mathcal{L}<em>{\text{adv}}^{E} = - \underset{x \sim p</em>{X}}{\mathbb{E}} [\log D_{\mathcal{W}}(E(x)_{i})].
\end{gather}
$$</p>

<h2 id="e4e-encoder-for-editing">e4e: Encoder for Editing</h2>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig6.png" />
<span class="caption text-muted">Figure 6. <b>The overall structure of e4e network</b>.</span></p>

<p>The high-level design of the encoder is illustrated above. The encoder denoted <em>e4e</em> (<em>“Encoder for Editing”,</em> reflecting its purpose), builds upon the Pixel2Style2Pixel (pSp) encoder. Unlike the original pSp encoder that generates $N$ style codes at the same time, as described earlier, <em>e4e</em> generates a single base style code, denoted by $w$, along with a series of $N-1$ offset vectors. These offsets are then summed up with the base style code $w$ to yield the final $N$ style codes which are now ready to be fed into the fixed, pre-trained StyleGAN2 generator.</p>

<h3 id="losses">Losses</h3>

<p>In the following section, we will go over each loss adopted for specific purpose. Briefly speaking, we need losses that are able to:</p>

<ul>
  <li>lower the distortion, by keeping the input and output images to be similar</li>
  <li>increase the perceptual quality and editability by encouraging the generated style codes to remain close to $\mathcal{W}$</li>
</ul>

<p><strong>&lt; Distortion &gt;</strong></p>

<p>To control the distortion introduced by the encoder, this paper borrows one of the key ideas presented in pSp: the identity loss.</p>

<p>Originally, the identity loss is designed to assist the accurate inversion of real images in the facial domain. The authors push this further and generalize it, and end up introducing a novel $\mathcal{L}_{\text{sim}}$ loss defined as:</p>

<p>$$\mathcal{L}_{\text{sim}} (x) = 1 - \langle C(x), C(G(e4e(x))) \rangle,$$</p>

<p>where $C$ is a ResNet-50 model trained with MOCOv2 dataset and $G$ is the pretrained StyleGAN2 generator. This loss encourages the model to predict latent codes that will be decoded to an image whose feature embedding has high cosine similarity with that of the original image. One powerful aspect of $\mathcal{L}_{\text{sim}}$ is that it can be applied to any image domain thanks to the general nature of the extracted features. <em>(Disclaimer: For facial domain, the authors used the original identity loss as in pSp, and employed a pre-trained ArcFace facial recognition network in the place of ResNet-50.)</em></p>

<p>On top of that, the commonly used $\mathcal{L}_{2}$ and $\mathcal{L}_{\text{LPIPS}}$ losses are appended to learn both pixel-wise and perceptual similarities. Finally, the distortion loss is defined as:</p>

<p>$$\mathcal{L}_{\text{dist}} (x) = \lambda_{l2} \mathcal{L}_{2} (x) + \lambda_{\text{LPIPS}} \mathcal{L}_{\text{LPIPS}} (x) + \lambda_{\text{sim}} \mathcal{L}_{\text{sim}} (x).$$</p>

<p><strong>&lt; Perceptual quality and editability &gt;</strong></p>

<p>Keeping the inferred latent space proximate to StyleGAN’s range $\mathcal{W}$ is the key to increase the perceptual quality and editability. To do so, this paper employs the two losses discussed earlier:</p>

<ol>
  <li>A delta-regularization loss to ensure proximity to $\mathcal{W}_{*}$ when learning the offsets $\Delta_{i}$</li>
  <li>An adversarial loss using the latent discriminator that encourages each learned style code to lie within the distribution $\mathcal{W}$.</li>
</ol>

<p>To summarize, the loss for ensuring high perceptual quality and editability is defined as:</p>

<p>$$\mathcal{L}_{\text{edit}} (x) = \lambda_{\text{d-reg}} \mathcal{L}_{\text{d-reg}} (x) + \lambda_{\text{adv}} \mathcal{L}_{\text{adv}}(x),$$</p>

<p>where $\mathcal{L}_{\text{d-reg}}$ and $\mathcal{L}_{\text{adv}}$ are defined as in the previous section.</p>

<p><strong>&lt; Total loss &gt;</strong></p>

<p>At last, the overall loss is defined as a weighted (linear) combination of the distortion and editability losses:</p>

<p>$$\mathcal{L} (x) = \mathcal{L}_{\text{dist}} (x) + \lambda_{\text{edit}} \mathcal{L}_{\text{edit}} (x).$$</p>

<h1 id="evaluation">Evaluation</h1>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig7.png" />
<span class="caption text-muted">Figure 7. <b>A triplets of a source image, its inversion, and an edit applied on the inverted image across multiple domains</b>.</span></p>

<p>Speaking of evaluation, there is no doubt that this is challenging. The goal is a proper evaluation method which can evaluate a trade-off between distortion and two other qualities - perceptual quality and editability. Every single one of them is hard to evaluate since they are perceptual in essence, thus objectively measuring &amp; numericalizing these is non-trivial task.</p>

<p>It is worth noting that there are several widely used methods for evaluating perceptual quality. These methods measure the discrepancy between the real and generated distributions using algorithms, listing few of them here:</p>

<ul>
  <li>FID (Frechet Inception Distance)</li>
  <li>SWD (Sliced Wasserstein Distance)</li>
  <li>IS (Inception Score)</li>
</ul>

<p>However these methods have some drawbacks:</p>

<ol>
  <li>Do not always agree with human perception</li>
  <li>Affected by distortion</li>
</ol>

<p>In the case of editability, evaluation becomes even more difficult task since the quality of an edited image should be evaluated as a function of the magnitude of the change introduced by editing. Moreover, qualitative measures often tend to be biased.</p>

<p>For evaluation, the following methods were used:</p>

<h2 id="distortion">Distortion</h2>

<ul>
  <li>$L_{2}$ reconstruction loss</li>
  <li>$\text{LPIPS}$</li>
</ul>

<h2 id="perceptual-quality">Perceptual quality</h2>

<p>To quantitatively evaluate the results, the authors measured:</p>

<ul>
  <li>FID</li>
  <li>SWD</li>
</ul>

<p>between the distributions of the real and reconstructed images.</p>

<h2 id="editability">Editability</h2>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig8.png" />
<span class="caption text-muted">Figure 8. <b>Inversion followed by a series of edits</b>. Note that D is the configuration which uses the complete e4e method.</span></p>

<p>We previously defined editability as the ability to perform latent-space editing using any arbitrary technique while maintaining high-visual quality of the image obtained using the edited latent code.</p>

<p><strong>For editing, the authors first used their GAN inversion method followed by several existing editing techniques such as:</strong></p>

<hr />

<ul>
  <li><strong>StyleFlow</strong>: <em>R. Abdal, P. Zhu, N. Mitra, and P. Wonka. Styleflow: Attribute-conditioned exploration of stylegan-generated im- ages using conditional continuous normalizing flows, 2020.</em></li>
  <li><strong>InterFaceGAN</strong>: <em>Y. Shen, J. Gu, X. Tang, and B. Zhou. Interpreting the latent space of gans for semantic face editing.</em></li>
  <li><strong>GANSpace</strong>: <em>E. H¨ark¨onen, A. Hertzmann, J. Lehtinen, and S. Paris. Ganspace: Discovering interpretable gan controls.</em></li>
  <li><strong>SeFa</strong>: <em>Y. Shen and B. Zhou. Closed-form factorization of latent semantics in gans.</em></li>
</ul>

<hr />

<p>After performing inversion, the authors applied the listed techniques to edit the code to introduce semantic modifications such as pose, gender, and age for the human facial domain. This modified code is then passed to StyleGAN to generate images. These images are evaluated using FID and SWD. Note that FID and SWD are applied differently in the previous section:</p>

<ol>
  <li>Perceptual quality: FID (or SWD) measured by computing the discrepancy between real and <em>reconstructed</em> images</li>
  <li>Editability: FID (or SWD) but measured by computing the discrepancy between real (original) and <em>edited</em> images</li>
</ol>

<h2 id="latent-editing-consistency">Latent Editing Consistency</h2>

<p><img class="img-fluid" src="/assets/post-images/e4e/fig9.png" />
<span class="caption text-muted">Figure 9. <b>An illustration of the Latent Editing Consistency.&lt;/span&gt;</b></span></p>

<p>Apart from the existing evaluation metrics, the authors present a new evaluation metric, called <em>latent editing consistency</em> (LEC) which combines two key components of GAN inversion methods meant for latent space editing:</p>

<ol>
  <li>A component for capturing the extent for which the inversion matches the true inverse of the generator.</li>
  <li>Another component for capturing how well-behaved the edits of the inversion outputs are.</li>
</ol>

<p>And the distance measured in the latent space for quantifying LEC can be defined as the following:</p>

<p>$$\text{LEC}(f_{\theta}) = \underset{x}{\mathbb{E}} \Vert E(x) - f^{-1}_{\theta} (E(G(f_{\theta}(E(x)))))\Vert_{2},$$</p>

<p>where $E$ is an encoder and $f_{\theta}(w)$ is an invertible semantic latent editing function parametrized by $\theta$. The purpose of doing the inverse editing is to let LEC capture how the inherent inversion errors translate to errors in the subsequent editing. In the optimal case, a well-behaved encoder should yield a small LEC difference in the latent space.</p>

<h1 id="conclusion">Conclusion</h1>

<p>In this work, the authors analyzed the structure of StyleGAN’s latent space, a very huge and complex space, in the context of GAN inversion literature. Based on several observations, this paper proposes a novel encoder architecture which can transform a natural image into a proper latent vector lying in the latent space of StyleGAN. At the same time, the authors suggest clever training strategies that better encourage the encoder to learn such behavior. On top of that, the authors validate the encoder by conducting several user studies as well as computing evaluation metrics including the novel method introduced by themselves.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation’</title><link href="https://dvelopery0115.github.io/2021/08/16/PointNet.html" rel="alternate" type="text/html" title="Summary of ‘PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation’" /><published>2021-08-16T00:00:00+00:00</published><updated>2021-08-16T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/16/PointNet</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/16/PointNet.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>Point cloud is an important type of geometric data structure which can easily be encountered in real-world problems. However, these data have been transformed to voluminous 3D voxel grids or collections of images due to its irregular format and such converted data came with some issues.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>Applications of PointNet</b>.</span></p>

<ul>
  <li>Designs a novel deep neural network architecture called <em>PointNet</em> suitable for consuming unordered point sets in 3D.</li>
  <li>Demonstrates how such architecture can be trained to perform 3D shape classification, shape part segmentation and scene semantic parsing tasks.</li>
  <li>Provides thorough empirical and theoretical analysis on the stability and efficiency of the proposed method.</li>
  <li>Visualizes the 3D features computed by the selected layers in the network and develops intuitive explanations for its performance.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This work presents PointNet along with the idea for extracting useful features from unordered set of 3D points. This novel architecture is capable of various tasks such as 3D shape classification, shape part segmentation and scene semantic parsing tasks, and achieves both best performance and efficiency when compared to previous methods. Furthermore, it provides detailed justification on the internal behavior of PointNet using visualizations of its layers helping us to understand the reason behind its performance intuitively.</p>

<h1 id="methods">Methods</h1>

<h2 id="problem-statement">Problem Statement</h2>

<p>The primary goal of this work is to design a neural network which can directly consume unordered point sets as inputs.</p>

<p>A point cloud is represented as a set of 3D points $\{ P_{i} \vert i = 1, \dots, n \}$, where each point $P_{i}$ is a vector of its $(x, y, z)$ coordinate and possibly with extra feature channels such as color, normal etc.</p>

<p>For the object classification task, the input point cloud is either directly sampled from a shape or pre-segmented from a scene point cloud. Given such point cloud as input, PointNet outputs $k$ scores for all the $k$ candidate classes.</p>

<p>For semantic segmentation, the input can be a single object for part region segmentation, or a sub-volume from a 3D scene for object region segmentation. Then PointNet will output $n \times m$ scores for each of the $n$ points and each of the $m$ semantic subcategories.</p>

<h2 id="deep-learning-on-point-sets">Deep Learning on Point Sets</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Qualitative results for part segmentation</b>.</span></p>

<p>The structure of PointNet is heavily influenced by the properties of point sets in $\mathbb{R}^{n}$.</p>

<h3 id="properties-of-point-sets-in-mathbbrn">Properties of Point Sets in $\mathbb{R}^{n}$</h3>

<p>A point cloud is in fact a subset of points from an Euclidean space $\mathbb{R}^{3}$. And it has three notable properties:</p>

<ol>
  <li><strong>Unordered.</strong> Unlike pixel arrays in images or voxel arrays in volumetric grids, point cloud is just a collection of points without specific order. That is, a network which takes point clouds as input must be invariant to all possible permutations of them (e.g. if a point cloud contains $N$ number of points, then the network must work consistently across its $N!$ permutations).</li>
  <li><strong>Interaction among points.</strong> The points are from a space with a distance metric. This implies that points are not isolated, and in fact neighboring points form a meaningful subset. Thus, the model should be able to capture local structures from points inside some local region.</li>
  <li><strong>Invariance under transformations.</strong> Since the point clouds of our concern are geometric representations of objects, the learned representation of point sets should be invariant to certain transformations. For example, rotating or translating a point cloud should not alter its predicted category.</li>
</ol>

<h3 id="pointnet-architecture">PointNet Architecture</h3>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 3. <b>PointNet Architecture</b>.</span></p>

<p>The entire PointNet architecture is illustrated above. Note that the classification network and the segmentation network share a great portion of structures.</p>

<p>The network has three key modules:</p>

<ol>
  <li>The max pooling layer as a symmetric function to aggregate information from all points in the input point cloud.</li>
  <li>A local and global information combination structure.</li>
  <li>Two joint alignment networks (denoted “T-Net”s) that align both input points and point features.</li>
</ol>

<p>The intention behind these structure will be discussed in the following sections.</p>

<p>&lt; <strong>Symmetry Function for Unordered Input &gt;</strong></p>

<p>There are three possible strategies to make a model invariant to input permutation:</p>

<ol>
  <li>Sort input into a canonical order</li>
  <li>Treat the input as asequence to train an RNN, but augment the training data by all kinds of permutations</li>
  <li><strong>Use a simple symmetric function to aggregate the information from each point</strong></li>
</ol>

<p>Here, a symmetric function takes $n$ vectors as input and outputs a new vector which is invariant to the input order. + and * operators are examples of such function.</p>

<p>While the idea of using sorting sounds simple, but in fact there does not exist a stable ordering in high dimensional space. Furthermore, one **might hope RNN to be invariant to input order when trained with randomly permuted sequence, but a study shows that it’s still hard to neglect the effect of different ordering. Moreover, RNN is hard to scale to thousands of input elements.</p>

<p>Therefore, it turns out that the reasonable way to cope with unordered set of points is to use a symmetric function.</p>

<p>The idea of the paper is to approximate a general function defined on a point set by applying a symmetric function on transformed elements in the set:</p>

<p>$$f({ x_1, \dots, x_n}) \approx g(h(x_1), \dots, h(x_n)),$$</p>

<p>where $f: 2^{\mathbb{R}^{N}} \rightarrow \mathbb{R}$, $h: \mathbb{R}^{N} \rightarrow \mathbb{R}^{K}$ and $g: \mathbb{R}^{K} \times \cdots \times \mathbb{R}^{K} \rightarrow \mathbb{R}$ is a symmetric funciton. In practice, the basic module of PointNet is very simple. The function $h$ is approximated by a multi-layer perceptron and $g$ is approximated by a composition of a single variable function and a max pooling. Apart from the simplexity of module, it has interesting properties and achieves high performance in a few different applications.</p>

<p>&lt; <strong>Local and Global Information Aggregation &gt;</strong></p>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig4.png" />
</center>
<p><span class="caption text-muted">Figure 4. <b>Qualitative results for semantic segmentation</b>.</span></p>

<p>The output from the above section has a form of a vector $[f_{1}, \dots, f_{K}]$, which is a global signature of the input set. One can simply construct a SVM or multi-layer perceptron classifier using such global features representing shape for classification. However, compared with shape classification, point segmentation requires not only global features but also local features that vary over different regions in the given shape (or scene). This issue can be easily addressed by simply concatenating the obtained global features to local features. After that, one can extract new per point features based on the “combined” point features which now contain both global and local information.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig5.png" />
</center>
<p><span class="caption text-muted">Figure 5. <b>Network architecture for part segmentation</b>.</span></p>

<p>With this modification, PointNet is now able to predict per point quantities that depend on both local geometry and global semantics. For example, a experimental result shows that the network can carry out accurate normal prediction which requires understanding of a point’s local neighborhood.</p>

<p><strong>&lt; Joint Alignment Network &gt;</strong></p>

<p>It’s trivial that the semantic labeling of a point cloud must be invariant if the point cloud is transformed by certain geometric transformations - rotation, translation, etc. Thus, it is desirable that the learnt representation of the point set is invariant to such transformations.</p>

<p>One straightforward solution for this is to align all input set to a canonical space before feature extraction. Then our question would be: “How can we find an appropriate transformation which will bring a point cloud of interest to such well-aligned, canonical space?”</p>

<p>The answer is simple - predict an affine transformation matrix using a mini, PointNet-like network and apply this predicted transformation to the coordinates of input points right away.</p>

<p>This idea - aligning the points in Euclidean space - can be extended to the alignment of feature space as well. Thus, we append additional alignment network on point features and predict a feature transformation matrix to align features from different input point clouds. However, due to the high dimensionality of feature space, finding the optimal transformation matrix is difficult. Therefore, we constrain the feature transformation matrix to be close to orthogonal matrix:</p>

<p>$$L_{\text{reg}} = \Vert I - AA^{T} \Vert^{2}_{F},$$</p>

<p>where $A$ is the feature alignment matrix predicted by T-Net. Thanks to its property of having the determinant of 1, applying such transformation will not cause any information loss.</p>

<h3 id="theoretical-analysis">Theoretical Analysis</h3>

<p><strong>&lt; Universal approximation &gt;</strong></p>

<p>PointNet can well approximate continuous set functions. By the continuity of set functions, a small perturbation to the input point set should not greatly change the function values, such as classification or segmentation scores.</p>

<p>Formally, let $\mathcal{X} = { S: S \subseteq [0, 1]^{m} \,\, \text{and} \,\, \vert S \vert = n}$, $f: \mathcal{X} \rightarrow \mathbb{R}$ be a continuous set function on $\mathcal{X}$ w.r.t to Hausdorff distance $d_{H}(\cdot, \cdot)$, i.e., $\forall \epsilon &gt; 0$, $\exists \delta &gt; 0$, for any $S, S^{\prime} \in \mathcal{X}$, if $d_{H}(S, S^{\prime}) &lt; \delta$, then $\vert f(S) - f(S^{\prime}) \vert &lt; \epsilon$. Then the proposed theorem states that $f$ can be arbitrarily approximated by the neural network given enough neurons at the max pooling layer.</p>

<hr />

<p><strong>Theorem 1.</strong> <em>Suppose $f: \mathcal{X} \rightarrow \mathbb{R}$  is a continuous set function w.r.t Hausdorff distance $d_{H}(\cdot, \cdot)$. That is, $\forall \epsilon &gt; 0$ , $\exists$ a continuous function $h$ and a symmetric function</em> $g(x_1, \dots, x_n) = \gamma \circ \text{MAX}$, <em>such that for any</em> $S \in \mathcal{X}$,</p>

<p>$$ \Big\vert \, f(S) - \gamma \Big(  \underset{x_{i} \in S}{\text{MAX}} { h(x_{i})} \Big) \Big\vert &lt; \epsilon$$</p>

<p><em>where $x_{1}, \dots, x_{n}$ is the full list of elements in $S$ ordered arbitrarily, $\gamma$ is a continuous function, and $\text{MAX}$ is a vector max operator that takes $n$ vectors as input and returns a new vector of the element-wise maximum.</em></p>

<hr />

<p><strong>&lt; Bottleneck dimension and stability &gt;</strong></p>

<p>Both theoretically and experimentally, the authors discovered that the expressiveness of PointNet is greatly affected by the dimension of the max pooling layer. Let’s define $\textbf{u} = \underset{x_{i} \in S}{\text{MAX}} \{ h(x_{i})\}$ to be the sub-network of $f$ which maps a point set in $[0, 1]^{m}$ to a $K$-dimensional vector. Then the following theorem says that small corruptions or extra noise points in the input set are not likely to alter the output of PointNet.</p>

<hr />

<p><strong>Theorem 2.</strong> <em>Suppose $\textbf{u} : \mathcal{X} \rightarrow \mathbb{R}^{K}$ such that $\textbf{u} = \underset{x_{i} \in S}{\text{MAX}} \{ h(x_{i}) \}$ and $f = \gamma \circ \textbf{u}$. Then,</em></p>

<ol>
  <li>$\forall S$,  $\exists \,\mathcal{C}_{S}$,  $\mathcal{N}_{S} \subseteq \mathcal{X}$,  $f(T) = f(S)$  if  $\mathcal{C}_{S} \subseteq T \subseteq \mathcal{N}_{S}$</li>
  <li>$\vert \mathcal{C}_{S} \vert \leq K$</li>
</ol>

<hr />

<p>The first says that $f(S)$ is unchanged up to the input corruption if all points in $\mathcal{C}_{S}$ are preserved. Also, it is also unchanged with extra noise added to the input up to $\mathcal{N}_{S}$ (i.e. less or additional number of points do not change the function value).</p>

<p>The second tells us that $\mathcal{C}_{S}$ contains only a bounded number of points, determined by $K$ which is the dimensionality of the output of the function $h : \mathbb{R}^{N} \rightarrow \mathbb{R}^{K}$. That is, $f(S)$ <strong>is in fact totally determined by a finite subset $\mathcal{C}_{S} \subseteq S$ of less or equal to $K$ elements.</strong> Thus, we call $\mathcal{C}_{S} \subseteq S$ the <em>critical point set</em> of $S$ and $K$ the <em>bottleneck dimension</em> of $f$.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/PointNet/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 6. <b>Critical points and upper bound shape</b>.</span></p>

<p>Conditioned on the continuity of $h$, this theorem explains the robustness of PointNet w.r.t point perturbation, corruption and extra noise points. This can be summarized as the following intuition:</p>

<blockquote>
  <p><strong>“PointNet learns to summarize a shape by a sparse set of key points.”</strong></p>
</blockquote>

<h1 id="conclusion">Conclusion</h1>

<p>This paper proposes a novel deep neural network <em>PointNet</em> which directly consumes unordered set of points, aggregates both local and global features. <em>PointNet</em> paves the way of utilizing unstructured point cloud data by providing a unified approach to a number of 3D recognition tasks including object classification, part segmentation and semantic segmentation. The experimental results show that <em>PointNet</em> achieves on par or better performance than state of the arts on standard benchmarks.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘A Style-Based Generator Architecture for Generative Adversarial Networks’</title><link href="https://dvelopery0115.github.io/2021/08/12/StyleGAN.html" rel="alternate" type="text/html" title="Summary of ‘A Style-Based Generator Architecture for Generative Adversarial Networks’" /><published>2021-08-12T00:00:00+00:00</published><updated>2021-08-12T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/12/StyleGAN</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/12/StyleGAN.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>While the resolution and quality of images generated by GANs greatly improved in recent years, the internal mechanism of generators is still undiscovered despite numerous recent efforts to better understand the way how images are synthesized.</li>
  <li>Besides, the properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations provide no quantitative way to compare different generators against each other.</li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>A novel architecture, inspired by style transfer literature, that automatically learns unsupervised separation of high-level attributes and is able to introduce stochastic variation in the generated images, and enables intuitive, scale-specific control of the synthesis.</li>
  <li>The new generator improves the state-of-the-art in terms of traditional metrics, and also outperforms others when it comes to qualitative comparisons.</li>
  <li>Moreover, the generator better disentangles the latent factors of variation.</li>
  <li>Besides, the authors suggest two new, automated methods - perceptual path length and linear separability - to quantify interpolation quality and disentanglement.</li>
  <li>Last but not least, the authors introduce a new, highly varied and high-quality dataset of human faces.</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This work proposes StyleGAN, a novel generative adversarial network architecture inspired by studies on style transfer which generates images by gradually adjusting ‘style’ of them at each convolution layer thereby automatically learns to separate high-level image attributes without any supervision.</p>

<h1 id="methods">Methods</h1>

<p><strong>This work doesn’t introduce any modifications to the discriminator or the loss function.</strong> Thus, this work has nothing to do with the ongoing discussion about GAN loss functions, regularization, and hyperparameters.</p>

<h2 id="style-based-generator">Style-based generator</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>Overview of Style-based generator</b>.</span></p>

<p>What generators commonly do is to provide the latent code to the input layer in the beginning. The newly proposed generator has different design which differentiate it from typical generator networks. Instead of input layer that takes the latent, <strong>the generator starts from a learned constant instead.</strong></p>

<p>Given a latent code $\textbf{z}$ in the input latent space $\mathcal{Z}$, a non-linear mapping network $f: \mathcal{Z} \to \mathcal{W}$ first maps it to $\textbf{w} \in \mathcal{W}$. This mapping network $f$ is implemented using an 8-layer MLP and the dimensionality of input and output feature vector ($\textbf{z}$ and $\textbf{w}$) are equally set to 512.</p>

<p>Then the learned affine transformations specialize $\textbf{w}$ to <em>styles $\textbf{y} = (\textbf{y}_{s}, \textbf{y}_{b})$</em> that control <em>adaptive instance normalization (AdaIN)</em> operations after each convolution layer of the synthesis network $g$. The AdaIN operation is defined as follows:</p>

<p>$$\text{AdaIN} (\textbf{x}_{i}, \textbf{y}) = \textbf{y}_{s, i} \frac{\textbf{x}_{i} - \mu(\textbf{x}_{i})}{\sigma(\textbf{x}_{i})} + \textbf{y}_{b, i},$$</p>

<p>where each feature map $\textbf{x}_{i}$ is normalized separately, and then scaled and biased using the corresponding scalar components from style $\textbf{y}$. This implies that each $\textbf{y}_{s}$, $\textbf{y}_{b}$ has to have the same dimensionality as $\textbf{x}_{i}$, meaning that the overall dimensionality of $\textbf{y}$ is twice the number of feature maps on that layer.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Uncurated set of images generated by the style-based generator</b>.</span></p>

<p>Apart from fusing style information into an intermediate image (i.e. the image which will eventually become the output of the generator), the generator receives <em>noise inputs</em> before &amp; after each convolution layer to introduce stochastic details. The noises are in fact single-channel images consisting of uncorrelated Gaussian noise, and injected to each layer of the synthesis network. As one can see in the overall architecture these single channel images are first broadcasted to all feature maps using learned per-feature scaling factors and then added to the output of the convolution.</p>

<h3 id="truncation-trick-in-mathcalw-optional">Truncation trick in $\mathcal{W}$ (Optional)</h3>

<p>Poorly represented distribution in regions with low density (i.e. rare data) can be a problem when training the generator. This is still an open problem in all generative models, so the generator in this work is not an exception. However, it’s known that sampling latent vectors from a truncated (or shrunk) sampling space tends to improve the quality of generated image, although the loss of some amount of variation is inevitable trade-off.</p>

<p>That being said, we can use a similar approach. Instead of using $\textbf{w} = f(\textbf{z})$ directly, we can compute the center of mass of $\mathcal{W}$ as $\bar{\textbf{w}} = \mathbb{E}_{\textbf{z} \sim P(\textbf{z})} \big[ f(\textbf{z}) \big]$. Then, we can scale the deviation of a given $\textbf{w}$ from the center as $\textbf{w}^{\prime} = \bar{\textbf{w}} + \psi (\textbf{w} - \bar{\textbf{w}})$, where $\psi &lt; 1$.</p>

<p>While some studies argue that only a subset of networks (even with orthogonal regularization due to unstability!) can benefit from the truncation , truncation in $\mathcal{W}$ seems to work reliably even without changes to the loss function.</p>

<h2 id="properties-of-the-style-based-generator">Properties of the Style-based Generator</h2>

<p>The generator architecture enables us to control the image synthesis via scale-specific modifications to the styles. We can think of the mapping network and each learned affine transform as a way to draw samples for each style from certain distribution. And the role of synthesis network can be thought as appropriately fusing these styles together to generate novel images. We suspect that the effects of each style are localized in the network. That is, <strong>modification on a specific subset of the styles affects only certain aspects of the image.</strong></p>

<p>One possible hint for this localization is in the way how AdaIN operation works. It first normalizes each channel to zero mean and unit variance, and then applies scales and biases based on the style. This changes the per-channel statistics, and eventually modifies the relative importance of features for the subsequent convolution operation. Note that such features do not depend on the original statistics since the normalization has wiped out the original statistics. In other words, <strong>each style controls only one convolution before the next AdaIN operation overrides statistics of feature maps.</strong></p>

<h3 id="style-mixing">Style Mixing</h3>

<p>“<strong>Each style code contributes to different high-level image attribute.”</strong></p>

<p>In order to encourage the localization of each style, authors used <em>mixing regularization</em> which is a method to generate some portion of output images from not only one latent code, but two during training. An operation for such regularization is called <em>style mixing</em>, which simply switches one latent code to another at a randomly selected point in the synthesis network (i.e. the injected code changes at some point in the synthesis process).</p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 3. <b>Examples of style mixing at various scales</b>. Observe how each subset of styles controls meaningful high-level attributes of the image.</span></p>

<p>Specifically, with <em>style mixing</em>, two latent codes $\textbf{z}_{1}$, $\textbf{z}_{2}$ are transformed by the mapping network and turned into $\textbf{w}_{1}$, $\textbf{w}_{2}$, respectively. Until some randomly picked crossover point (say, the fourth block of synthesis network), $\textbf{w}_{1}$ control styles and $\textbf{w}_{2}$ governs the style manipulation after that. This technique prevents the network from assuming that adjacent styles are correlated. <strong>→ 🤔  Does “adjacent style” mean the order of injected style codes?</strong></p>

<h3 id="stochastic-variation">Stochastic variation</h3>

<p>“<strong>Random noises introduced during synthesis affect appearance attributes that are confined to local region and has inherent stochasticity.”</strong></p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig4.png" />
</center>
<p><span class="caption text-muted">Figure 4. <b>Examples of stochastic variation</b>.</span></p>

<p>One important observation from looking at various human portraits is that randomness involves in determining lots of aspects of images from the placement of hairs, stubble, freckles, or skin pores. Choice for such appearance aspects can be randomized without affecting our perception of the image as long as they follow the correct distribution (e.g. putting freckles in the eyes would leads to very weird images). Then what is the correct way to introduce stochasticity to images correctly?</p>

<p>The reason why traditional generators often fail to model plausible randomness is because of the input layer of their architectures. Compared to the style-based generator proposed in this work, the only input to such generators is single latent vector which will eventually be decoded to the (hopefully, photorealistic) image.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig5.png" />
</center>
<p><span class="caption text-muted">Figure 5. <b>Effect of noise inputs at different layers of the generator</b>.</span></p>

<p>However, since that is the only input to the network, it needs to somehow find a way to generate spatially-varying pseudorandom numbers from earlier activations whenever they are needed. Such randomness introduced implicitly by the network itself is not powerful enough to give rise to natural patterns instead leads to repetitive patterns in generated images. <strong>To address this issue, this work suggests to add per-pixel noise after each convolution so the randomness is now explicitly handled at each style level.</strong></p>

<p>Each portrait in figure 4 (middle column) is generated from the same underlying image (same constant initialization, same latents, same affine transforms to obtain styles) but with different noises applied during synthesis. Surprisingly, the introduced noises affect only the stochastic aspects (e.g. different placements of hairs) of images not the global context (e.g. overall composition, identity intact, etc). Furthermore, the figure 5 shows the effect of applying stochastic variation to different subsets of layers.</p>

<h3 id="separation-of-global-effects-from-stochasticity">Separation of Global Effects from Stochasticity</h3>

<p>What we’ve been discussing can be summarized into two simple facts:</p>

<ol>
  <li>Changing style effects the synthesized image globally.</li>
  <li>Noises injected to each convolution layer affect only inconsequential random variation.</li>
</ol>

<p>This discovery is well-aligned with style transfer literature which have been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc) encode the style of an image (global) while spatially varying features encode a specific instance (local). These observations in style transfer literature can explain why changing style code, noise behaves differently.</p>

<p>In the style-based generator, the style can affect the entire image because of AdaIN operation, which shifts the distribution of all feature maps by uniformly scaling and adding biases to them. Such operation is spatially invariant, thus leads to the modification of global attributes like pose, lighting, or background style. In contrast, the noise (per-pixel Gaussian) is added independently to each pixel thus it varies over the spatial domain. Thus, in theory, it’s suitable for controlling stochastic variation.</p>

<h2 id="disentanglement-studies">Disentanglement Studies</h2>

<center>
    <img class="img-fluid" src="/assets/post-images/StyleGAN/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 6. <b>Illustration of latent space of image features</b>. Mapping from initial latent space to image features is nonlinear, thus one cannot guarantee that changing the single axis of latent code will affect only one image attribute. Meanwhile, the mapping between intermediate latent space and image features are (hopefully) linear, so that one can expect modification in single image attribute when exploring the latent space along specific axis.</span></p>

<p>The goal of disentanglement is to <strong>find a latent space that consists of linear subspaces, each of which controls only one factor of variation</strong> so that we can modify only one aspect of images while exploring the space along specific direction.</p>

<p>However, we cannot easily do this in $\mathcal{Z}$ since it needs to match the corresponding density in the training data. This means that each axis of $\mathcal{Z}$ is not well-aligned to perceptually meaningful image features hindering the complete disentanglement that we desire.</p>

<p>Fortunately, the style-based generator circumvents this issue by introducing a mapping network $f$ which maps the latent space $\mathcal{Z}$ to the intermediate latent space $\mathcal{W}$ that doesn’t have to support sampling according to any <em>fixed</em> distribution (e.g. distribution of training data). Rather, it’s the “unwarped” space consisting of vectors $\textbf{w}$ transformed from $\textbf{z}$ by <em>learned</em> piecewise continuous mapping $f$. <strong>This “unwarping” $\mathcal{W}$ makes the factors of variation become more linear making it easier to identify each important feature along each dimension</strong> of the space. And the authors expected $f$ to be trained in a way that it unwarps the space well achieving nice disentanglement of features.</p>

<p>While there has been numerous metrics suggested to quantify the degree of disentanglement, they cannot be directly applied to this work since they require a pipeline with an encoder network which maps input images to latents. As introduced earlier, the overall structure of GAN used as a baseline employed encoder-less architecture making it impossible to compare this method to others. Also, it’s definite waste of time and resources to add an extra encoder network in the architecture that is not directly related to the problem.</p>

<p>Thus, <strong>the authors present two new ways of quantifying disentanglement neither of which requires an encoder or known factors of variation</strong> (i.e. which part of latent contributes to which image attribute).</p>

<h3 id="perceptual-path-length">Perceptual Path Length</h3>

<p>Interpolation of latent vectors lying in latent space sometimes leads to surprising non-linear changes in the image. To be more specific, features that did not appear in either endpoint may appear in the middle of a linear interpolation path. <strong>This is a strong evidence that the latent space is entangled and the factors of variation are not separated.</strong></p>

<p>To quantify this phenomena, one can come up with an idea to measure how the image changes as interpolation is performed in the latent space. One good intuition to start from is that the more curvy the latent space is, the more drastic change will be introduced to the image during the latent space exploration.</p>

<p>That being said, the authors propose a metric which uses a perceptually-based pairwise image distance as a basis. Such distance can be computed as a weighted difference between two VGG16 embeddings, where the weights are designed to match the similarity difference perceived by human.</p>

<p>Then, the length of path drawn by interpolated latent vector can be defined mathematically as the limit of the sum of the length of line segments where the length of each segment goes to infinitesimal. Thus, the average perceptual path length in latent space $\mathcal{Z}$, over all possible endpoints can be defined as follows:</p>

<p>$$l_{\mathcal{Z}} = \mathbb{E} \Big[ \frac{1}{\epsilon^{2}} d \big(  G(\text{slerp}(\textbf{z}_{1}, \textbf{z}_{2}; t)), G(\text{slerp}(\textbf{z}_{1}, \textbf{z}_{2}; t + \epsilon))\big)\Big],$$</p>

<p>where $\epsilon = 10^{-4}$,  $\textbf{z}_{1}$, $\textbf{z}_{2} \sim P(\textbf{z})$, $t \sim U(0, 1)$, $G = g \circ f$ is the generator, and $d(\cdot, \cdot)$ evaluates the perceptual distance between the resulting images. Here, $\text{slerp}$ denotes spherical interpolation used to interpolate between normalized input latent vectors.</p>

<p>Similarily, one can define the average perceptual path length in $\mathcal{W}$ as well.</p>

<p>$$ \begin{gather} l_{\mathcal{W}} = \mathbb{E} \Big[ \frac{1}{\epsilon^{2}} d \big( g (\text{lerp} (f(\textbf{z}<em>{1}), f(\textbf{z}</em>{2}); t)), <br />
g (\text{lerp}(f(\textbf{z}<em>{1}), f (\textbf{z}</em>{2}); t+\epsilon))\big) \Big], \end{gather}$$</p>

<p>where the only difference is that interpolation is carried out in $\mathcal{W}$. Since vectors in $\mathcal{W}$ are not normalized, linear interpolation ($\text{lerp}$) is used instead of $\text{slerp}$.</p>

<h3 id="linear-separability">Linear Separability</h3>

<p>Furthermore, if a latent space is well-disentangled, it should be possible to find direction vectors that consistently correspond to individual factors of variations (e.g. modifying the first and second dimension of $\textbf{w}$ changes camera pose).</p>

<p>Thus, another metric proposed by the authors quantifies this effect. Evaluation of such metric is done by measuring how well the vectors in the latent space can be separated into two distinct sets, say $S$ and $S^{C}$ via a linear hyperplane - For example, suppose that we have found a hyperplane which perfectly divides the latent space into two subspaces, so that all vectors from one set are decoded to portraits of men, while the vectors in its complement are decoded to photographs of women.</p>

<p>To label the generated images, the authors trained auxiliary classification networks for a number of binary attributes (e.g. binary classification on male and female faces). These networks are trained to classify images generated from $\textbf{z} \sim P(\textbf{z})$ with respect to certain predefined attributes. Then the authors utilize SVM to find proper hyperplanes lying in the latent space $\mathcal{Z}$, that best separate the generated images $G(\textbf{z})$ in RGB space with respect to the selected attributes (actually, the images are already labeled, so it’s nothing but a simple classification using ML).</p>

<h1 id="conclusion">Conclusion</h1>

<p>The newly proposed style-based generator outperforms the prior works in qualitative comparisons, as well as in quantitative evaluations including the novel metrics suggested by the authors.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Summary of ‘Generative Adversarial Transformers’</title><link href="https://dvelopery0115.github.io/2021/08/04/GANformer.html" rel="alternate" type="text/html" title="Summary of ‘Generative Adversarial Transformers’" /><published>2021-08-04T00:00:00+00:00</published><updated>2021-08-04T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/04/GANformer</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/04/GANformer.html"><![CDATA[<h1 id="motivations">Motivations</h1>

<ul>
  <li>This work is inspired by two reciprocal mechanisms underlying human perception that are often discussed in the field of cognitive science: the <strong>bottom-up</strong> (proceeding from the retina up to the visual cortex - local elements and salient stimuli hierarchically group together to form the whole) processing, and the <strong>top-down</strong> (surrounding global context, selective attention and prior knowledge inform the interpretation of the particular) processing.</li>
  <li>However, the convolutional neural network, which gained huge success in computer vision over the last decade, does not reflect this bidirectional nature of our visual system. Rather, its feed-forward propagation only mimics the bottom-up processing by building up higher, abstract representations from raw sensory signals.</li>
  <li>The lack of global context understanding leads to multiple problems - (1) <em>model struggling to capture long-range dependencies</em>, (2) <em>develop holistic understanding of global shapes and structures</em>, (3) <em>optimization and stability issues</em> (the difficulty of training GANs is infamous 😵) due to the inherent difficulty in coordinating between fine details across the generated image.</li>
  <li>Thus, all of these bring us to the unanswered question: <strong>“Is convolution alone a complete solution, or do we need more?”</strong></li>
</ul>

<h1 id="key-contributions">Key Contributions</h1>

<ul>
  <li>Introduces the <strong>GANformer</strong>, a novel and efficient type of transformer, a highly-adaptive architecture centered around relational attention and dynamic interaction, and explore it for the task of visual generative modeling.</li>
  <li>This new <em>bipartite</em> structure enables long range interactions across the image, reducing the computational cost from $O(n^{2})$ of conventional transformers to $O(n)$. This hints the new way of applying transformers to high-resolution synthesis which was considered impossible due to the expensiveness of transformers.</li>
  <li>Compared to the classic transformers, this structure utilizes multiplicative integration which allows flexible region-based modulation. → GANformer can be regarded as a generalization of StyleGAN!</li>
</ul>

<hr />

<h1 id="tldr">TL;DR</h1>

<p>This paper introduces GANformer, a combination of two renowned architectures - GAN &amp; Transformer - to overcome the limitations that previous GANs had in the task of synthesizing scenes composed of multiple objects. This novel, bipartite architecture successfully brings the notion of attention to the vision tasks with reasonable, linear time complexity.</p>

<h1 id="methods">Methods</h1>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig1.png" />
</center>
<p><span class="caption text-muted">Figure 1. <b>Sample images generated by the GANformer with a visualization of the model attention maps</b>.</span></p>

<h2 id="the-generative-adversarial-transformer">The Generative Adversarial Transformer</h2>

<p>The Generative Adversarial Transformer (GANformer) is a type of Generative Adversarial Network (GAN) consists of a <em>generator</em> network (G) that maps a sample from the <em>latent</em> space to the output space, and a <em>discriminator</em> network (D) whose goal is to distinguish real and fake samples. During training, the two networks compete with each other through a minimax game until reaching an equilibrium.</p>

<p>While typical generators and discriminators are built by composing multiple convolutional layers, in the case of GANformer, both of them are constructed by using a novel architecture called <strong><em>Bipartite Transformer</em></strong>.</p>

<h3 id="the-bipartite-transformer">The Bipartite Transformer</h3>

<p>Before explaining what the bipartite transformer is, let me briefly review the conventional transformers.</p>

<p>The standard transformer network is composed of alternating multi-head self-attention and feed-forward layers. The authors of GANformer refer to <em>each pair of self-attention and feed-forward layers</em> as <em>a transformer layer</em>, so a transformer can be thought as a stack of several transformer layers. The self-attention layer, the core idea &amp; structure of transformer, numericalizes all pairwise relations among the input elements (e.g. relations between each word in a sentence), thereby updating each single elements by attending to all the others.</p>

<p>Then, what makes the bipartite transformer so stand out from the standard one? In short, the bipartite transformer can be considered as a generalization of self-attention. Instead of taking relations of every element pair into account, <strong>the bipartite transformer formulates a bipartite graph between two groups of variables</strong>. In the GAN case, the set of latents and image features can be thought as such groups.</p>

<p>Moreover, the authors of GANformer suggest two forms of attention that could be computed over the bipartite graph, categorized by the direction in which information propagates - (1) <strong>Simplex attention (one way only)</strong>, and (2) <strong>Duplex attention (both ways)</strong>. Just like other transformers, the GANformer adopts multi-head structures in practice while the following descriptions are all done with one-head version for clarity.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig2.png" />
</center>
<p><span class="caption text-muted">Figure 2. <b>Bipartite Attention</b>.</span></p>

<h3 id="simplex-attention">Simplex Attention</h3>

<p>The <em>simplex attention</em> distributes information in a single direction over the bipartite transformer graph.</p>

<p>Formally, let $X^{n \times d}$ denote an input set of $n$ vectors of dimension $d$ (in particular, $n = W \times H$ for the image case), and $Y^{m \times d}$ denote a set of $m$ aggregator variables (in the generative case, they become latent vectors). Then we can compute attention over the derived bipartite graph between these two groups:</p>

<p>$$
\begin{gather}
Attention (Q, K, V) = \text{softmax} \Big( \frac{QK^{T}}{\sqrt{d}}\Big) V 
<br />
a(X, Y) = Attention(q(X), k(Y), v(Y))
\end{gather}$$</p>

<p>Here, $q(\cdot), k(\cdot), v(\cdot)$ are functions that maps elements into queries, keys, and values, respectively, all maintaining dimensionality $d$. Also, the inputs to these mappings are all <em>positional encoded</em> reflecting the distinct position of each element (e.g. in the image). When $Y=X$, this bipartite attention becomes equivalent to the self-attention.</p>

<p>Then, we integrate the attended information with the input elements $X$. In the case of the standard transformer, it applies an additive update rule of the form:</p>

<p>$$u^{a}(X, Y) = LayerNorm(X + a(X, Y))$$</p>

<p>In contrast, in the case of GANformer, it exploits the retrieved information to control both the <em>scale</em> as well as the <em>bias</em> of the elements in $X$, following the practice proven to be effective by StyleGAN model. Such multiplicative integration improves the model performance by huge margin. Formally:</p>

<p>$$u^{s}(X, Y) = \gamma(a(X, Y)) \,  \odot \, \omega(X) + \beta (a(X, Y))$$</p>

<p>where $\gamma(\cdot), \beta(\cdot)$ are mappings that compute multiplicative and additive styles, maintaining a dimension of $d$. and $\omega(X) = \frac{X - \mu(X)}{\sigma(X)}$ normalizes each element with respect to the other features.</p>

<p>One important implication of this multiiplicative integration is that, by normalizing $X$ (image features in generative model), and then letting $Y$ (latents) control the statistical tendencies of $X$, the information can flow from $Y$ to $X$. <strong>That is, the higher &amp; abstract knowledge on images (latents) can control the visual generation of spatial (local) attended regions (features) within the image.</strong></p>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig3.png" />
</center>
<p><span class="caption text-muted">Figure 3. <b>Attention Maps</b>.</span></p>

<h3 id="duplex-attention">Duplex Attention</h3>

<p>The idea of simplex attention can be extended to the case where the variables $Y$ have a key-value structure of their own, namely $Y = (K^{n \times d}, V^{n \times d})$. In such case, the values $V$ store the <em>content</em> of the $Y$variables as before, while the keys track the <em><strong>centroids</strong> $K$</em> of the attention-based assignments between $Y$ and $X$, which can be computed as $K = a(Y, X)$ - the weighted averages of the $X$ elements using the bipartite attention. As a result, we can define a new update rule:</p>

<p>$$u^{d} (X, Y) = \gamma (A(X, K, V)) \, \odot \, \omega(X) + \beta(A(X, K, V))$$</p>

<p>This new update rule includes two attention operations on top of each other:</p>

<ol>
  <li>Compute soft attention assignments between $X$ and $Y$, by $K = a(Y, X)$,</li>
  <li>Refine the assignments by considering their centroids, by $A(X, K, V)$.</li>
</ol>

<p>Note that this is analogous to the $k$-means algorithm and turned out to be more effective than the simpler update rule $u^{a}$ in the experiments.</p>

<p>To support bidirectional interaction between $X$ and $Y$, the authors chained two reciprocal simplex attentions from $X$ to $Y$ and from $Y$to $X$, obtaining the <strong><em>duplex attention</em></strong>, which is actually the alternating computation of $Y := u^{a}(Y, X)$ and $X := u^{d}(X, Y)$. By doing so, we can simultaneously update each representation through bidirectional interaction - bottom-up and top-down.</p>

<h3 id="overall-architecture-structure">Overall Architecture Structure</h3>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig4.png" />
</center>
<p><span class="caption text-muted">Figure 4. <b>Model Overview</b>. <b>Left</b>: The GANformer layer consists of a bipartite attention operation to propagate information from the latents to the image grid, followed by convolution and upsampling. These layers are stacked multiple times and generates images by starting from a 4 X 4 grid refining them over different layers until reaching a final high-resolution image. <b>Right</b>: The latents and image features attend to each other to capture the scene structure. In contrast to StyleGAN where a single latent uniformly impacts the entire image, different latents in GANformer’s compositional latent space contribute to different (semantic) regions of the final output image.</span></p>

<p><strong>Vision-Specific Adaptations.</strong> In the standard transformer used for NLP, each self-attention layer is followed by a FC layer that processes each element independently (which can be seen as a $1 \times 1$ convolution). Since this work is about images not sentences, the authors instead used <strong>a convolution of which kernel size is</strong> $k=3$ ($3 \times 3$) after each application of the attention. For nonlinearity activation, a <strong>Leaky ReLU</strong> is applied after each convolution - they upsample the image in the generator, while the ones of discriminator downsamples the input image. To make the transformer be aware of the feature location in the image, the authors used a <strong>sinusoidal positional encoding</strong> along the horizontal and vertical dimensions for the visual features $X$, and trained positional embeddings for the set of latent variables $Y$.</p>

<p>This design decision - allowing latents to communicate through image features indirectly - enables adaptive long-range interaction between far away pixels passing through a compact and global <em>latent bottleneck</em> which selectively gathers information from the entire input and distributes it back to the relevant regions.</p>

<p>Fortunately, the bipartite structure make it possible to compute both the simplex and duplex attention operations with bilinear time complexity $O(mn)$. Furthermore, $m$, the number of latents is kept small (in the range of 8 to 32) in practice making the comparison with the standard self-attention, whose cost is $O(n^{2})$, almost meaningless.</p>

<h3 id="the-generator-and-discriminator-networks">The Generator and Discriminator Networks</h3>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig5.png" />
</center>
<p><span class="caption text-muted">Figure 5. <b>Upper-Layer Attention Maps</b>.</span></p>

<p>The design of GANformer starts from that of StyleGAN, a structure that gained huge popularity in research community due to significant quality of generated images.</p>

<p>What makes StyleGAN stand out from the common GANs whose generator networks usually composed of multiple convolutional layers which transforms a randomly sampled latent vector $z$ into an image, is a feed-forward <em>mapping network</em> that outputs an intermediate latent vector $w$. Moreover, this transformed latent vector $w$ then goes through the <em>synthesis network,</em> which globally controls the feature maps’ statistics at every layer.</p>

<p>One surprising outcome of this approach is that StyleGAN can decompose visual properties of images into different layers of it. This allows StyleGAN to control global aspects of the picture such as pose, lighting conditions, camera position, or color schemes in visually plausible ways. <strong>However, StyleGAN also has limit.</strong> While StyleGAN successfully decomposes global properties, it often fails to perform <em>spatial decomposition</em> since it provides no direct means to control the style of a localized regions.</p>

<p>Then the bipartite transformer comes to rescue. Compared to StyleGAN which tried to control the style of all features globally, the two types of attention brought to vision task with reasonable computational cost performs adaptive region-wise modulation. More precisely, the latent vector $z$ is splitted into $k$ components, $z = [z_{1}, \dots, z_{k}]$. These splitted variables are transformed into a set of intermediate latent variables $Y = [y_{1}, \dots, y_{k}]$ by a shared mapping network. Then, during image synthesis, the feature map $X$ and latents $Y$ play the roles of two elements groups, exchanging the information through the <em>bipartite latent bottleneck</em> introduced before, either by the simplex or duplex attention operation.</p>

<p>This setting allows for a <strong>flexible and dynamic style modulation at the region level</strong>. It is because of the tendency of soft attention which groups elements based on their proximity and content similarity.</p>

<p>For the discriminator, the authors applied attention after every convolution, and used trained embeddings to initialize the aggregator variables $Y$, hoping it to represent background knowledge the model learns about the task. At the last layer, they concatenated these variables $Y$ to the final feature map $X$ to make a prediction about the image source (real, fake, or ideally, indistinguishable).</p>

<p>For training, the authors adopted the settings and techniques of StyleGAN2, including the loss function, optimization and training configuration.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/GANformer/fig6.png" />
</center>
<p><span class="caption text-muted">Figure 6. <b>Sample Images and Attention Maps of Different Layers</b>. Samples of images generated by the GANformer on the CLEVR, LSUN-Bedroom and Cityscapes datasets, and visualizations of the produced attention maps.</span></p>

<h1 id="conclusion">Conclusion</h1>

<p>GANformer successfully unifies the GAN and Transformer architectures for the task of scene generation. The key contributions of this work is:</p>

<ul>
  <li><strong>Compositional Latent Space</strong> with multiple variables that coordinate through attention to produce the image cooperatively, while matching the inherent compositionality of natural scenes.</li>
  <li><strong>Bipartite Structure</strong> that balances between expressiveness and efficiency, modeling long-range dependencies while maintaining linear computational costs.</li>
  <li><strong>Bidirectional Interaction</strong> between the latents and the visual (image) features, which allows the transaction of information both in bottom-up, top-bottom manner.</li>
  <li><strong>Multiplicative Integration</strong> rule to impact the features’ visual style more flexibly.</li>
</ul>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Motivations]]></summary></entry><entry><title type="html">Brief Introduction to Logging with Weights &amp;amp; Biases</title><link href="https://dvelopery0115.github.io/2021/08/01/Introduction_to_W&B.html" rel="alternate" type="text/html" title="Brief Introduction to Logging with Weights &amp;amp; Biases" /><published>2021-08-01T00:00:00+00:00</published><updated>2021-08-01T00:00:00+00:00</updated><id>https://dvelopery0115.github.io/2021/08/01/Introduction_to_W&amp;B</id><content type="html" xml:base="https://dvelopery0115.github.io/2021/08/01/Introduction_to_W&amp;B.html"><![CDATA[<h1 id="introduction">Introduction</h1>

<p>This post is written to serve as the minimal starting point, including basic concepts and ready-to-run code snippets, for you when integrating Weights &amp; Biases (I will call it W&amp;B after all) into your machine learning workflow.</p>

<p>As you can notice from the word “minimal”, this post is just a very condensed summary covering only a small portion of the entire W&amp;B documentation. Therefore, I highly encourage you to check out <a href="https://docs.wandb.ai/">the full documentation</a> to learn more after finishing this.</p>

<p>In a nutshell, W&amp;B is <strong>a tool for experiment tracking, dataset versioning, and model management</strong> which we, ML researchers / engineers, do twenty-four-seven. If you are already familiar with TensorBoard, I bet you can easily get most of the contents and start writing codes immediately. Even if you aren’t, W&amp;B is quite straightforward to use yet incredibly powerful, so I strongly recommend you to take some time and go over this material (it won’t take that long!).</p>

<p>For explanation, and to give you a sense of adopting W&amp;B in real-world ML projects, I will use my implementation of <em>PointNet (PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, Charles R. Qi et al., CVPR 2017)</em> as an exempler. Note that all code snippets in this post are from my <a href="https://github.com/DveloperY0115/torch-pointnet"><em>torch-pointnet</em> repository</a>.</p>

<h1 id="initializing-wb">Initializing W&amp;B</h1>

<h2 id="sign-up--sign-in">Sign Up &amp; Sign In</h2>

<p>Don’t have an account for Weights &amp; Biases? You can easily make one, or directly link your Github account to it.</p>

<p>After signing up, you need to install <em>wandb</em> module to your Python environment. Everything we wil do after all assumes that you’ve installed this module successfully. And in fact, this module is all you need. Installation is surprisingly simple - just run the following command in your shell.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>pip <span class="nb">install </span>wandb
wandb login
</code></pre></div></div>

<h2 id="starting-a-new-run">Starting a New Run</h2>

<p>If you have experience with TensorBoard, you would remember that we need to initialize a <em>SummaryWriter</em> object which is then used for logging throughout the rest of your program. W&amp;B has no difference, but is more elegant in some sense compared to TensorBoard. Running a single line of code will do everything for us.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># imports
# import torch, numpy, etc
</span><span class="kn">import</span> <span class="n">wandb</span>
<span class="c1"># ...
</span>
<span class="c1"># def main():
# bla bla bla
</span>
<span class="c1"># and many other functions
# bla bla bla
</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>

    <span class="c1"># initialize W&amp;B
</span>    <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"torch-pointnet"</span><span class="p">)</span>

    <span class="c1"># run main function
</span>    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>

<p>The above code snippet shows the overall structure of my <em>train.py</em> file, which defines various control flows involving the training of PointNet. As you can see, <em>wandb</em> is initialized before running the main function which contains a number of <em>wandb.log</em> (will be explained soon) calls for tracking important quantities (loss, accuracy, etc) during the experiment.</p>

<h1 id="logging--model-tracking">Logging &amp; Model Tracking</h1>

<h2 id="log-almost-everything-you-want">Log (Almost) Everything You Want</h2>

<p><em>wandb.log</em> is a function that we use for logging. You can log almost everything you can imagine in various ML workflows - numbers, images, audio, video, HTML, Histogram, 3D data, and much more. You can check the supported types <a href="https://docs.wandb.ai/guides/track/log">here</a>.</p>

<p>In my case, I use this for tracking:</p>

<ul>
  <li>Training loss</li>
  <li>Test loss</li>
  <li>Test accuracy</li>
  <li>Visualization of test result (for qualitative analysis)</li>
</ul>

<p>For instance, in the function <em>train_one_epoch</em> that carries out optimization for a single epoch, I simply write:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>

    <span class="c1"># forward propagation
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># back propagation
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># update
</span>    <span class="c1"># bla bla bla
</span>
    <span class="c1"># log data
</span>    <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="s">"Train/Loss"</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">},</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
</code></pre></div></div>

<p>The quantity of interest is packed into a Python dictionary as a <em>(key, value)</em> pair where <strong>the key becomes the title of a plot</strong>, and <strong>the value represents the actual metric</strong> to be recorded. Notice how simply the logging works.</p>

<p>Of course, I can put as many kinds of data into a dictionary and log them at once.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">run_test</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>

    <span class="c1"># forward propagation
</span>
    <span class="c1"># compute metrics used for testing
</span>
    <span class="c1"># log data
</span>    <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span>
        <span class="p">{</span><span class="s">"Test/Loss"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">"Test/Accuracy"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span> <span class="s">"Test/Result"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">fig</span><span class="p">)},</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>(<em>Disclaimer: Logging image data isn’t working as intended at the time I write this post. I’ll try to resolve this issue ASAP.)</em></p>

<p>As a result, you can see this beautiful plot of training loss decreasing nicely over time. What makes this even cooler is that <strong>you can check the results in real time at the control panel through your web browser</strong> (both PC and mobile) without logging into your remote machine.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig1.png" />
</center>

<h2 id="anatomy-of-your-model-with-wb">Anatomy of Your Model with W&amp;B</h2>

<p>Having problem with exploding (or vanishing) gradients and don’t know which part<del>(s)</del> of your model is <del>(are)</del> in trouble? Using W&amp;B, you can easily diagnose the problem. To make W&amp;B pay attention to the internal of your model, simply wrap the model after initializing it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code>		
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># check GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

		<span class="c1"># model &amp; optimizer, schedulers
</span>    <span class="n">network</span> <span class="o">=</span> <span class="nc">PointNetCls</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># make W&amp;B track model's gradient and topology
</span>    <span class="n">wandb</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">log_freq</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">log_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

		<span class="c1"># and the rest of main..
</span></code></pre></div></div>

<p>W&amp;B gives you the freedom of choosing which aspects of your network are going to be tracked. In the snippet above, I decided to record the gradients at all layers of my model every 100 steps and track the computational graph formed during training.</p>

<p>Just like logged metrics, this information will show up in your project dashboard.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig2.png" />
</center>

<h1 id="check-system-utilization">Check System Utilization</h1>

<p>You might want to check whether you’re pushing your machine to its limit. Thankfully, W&amp;B automatically tracks various statistics of the system throughout a session - GPU memory usage, power consumption, number of CPU threads in use, etc.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig3.png" />
</center>

<h1 id="hyperparameter-tuning-with-sweep">Hyperparameter Tuning with Sweep</h1>

<p>Personally, I think this is <strong>the coolest feature of W&amp;B</strong>. W&amp;B provides a powerful yet easy-to-use tool for hyperparameters tuning with fancy visualizations that helps users intuitively compare different combinations of them.</p>

<p>In this example, although my model has only few hyperparameters associated with its structure and training, I’ll try to find the sweet spot for learning rate and batch size.</p>

<p>In order to find the optimal combination of hyperparameters, we need to tell W&amp;B,</p>

<ul>
  <li><strong>which hyperparameters to adjust</strong></li>
  <li><strong>what are the possible values (i.e. explorable space) for such hyperparameters</strong></li>
  <li><strong>which metric we want to opimize further through hyperparameter tuning</strong></li>
</ul>

<h2 id="tell-wb-a-list-of-adjustable-variables">Tell W&amp;B a list of adjustable variables</h2>

<p>In my case, I personally prefer using <em>argparse</em> for tweak variables for training. As you can see at the beginning of my <em>train.py,</em> I store the set of arguments in variable <em>args</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="nc">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Parsing argument"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--beta1"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Beta 1 of Adam optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--beta2"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Beta 2 of Adam optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--lr"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Learning rate for optimizer"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--step_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Step size of StepLR"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--gamma"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Gamma of StepLR"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--num_epoch"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of epochs"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--num_iter"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of iteration in one epoch"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--batch_size"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Size of a batch"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--num_worker"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of workers for data loader"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="s">"--out_dir"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"out"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Name of the output directory"</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span>
    <span class="s">"--save_period"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of epochs between checkpoints"</span>
<span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse_args</span><span class="p">()</span>
</code></pre></div></div>

<p>Then I need to inform W&amp;B what the controllable variables are. All I need to do this is simply passing the set of parsed arguments at the time of initialization. Note that this is one way of doing this, and you can see other methods <a href="https://docs.wandb.ai/guides/sweeps/quickstart">here</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>

    <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"torch-pointnet"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>

    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div>

<p>And in function <em>main</em>, I replace all occurences of <em>args.** to *config[</em>]<em>. Here, the object *config</em> holds the variables that we informed to W&amp;B at initialization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    
    <span class="n">config</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">config</span>

    <span class="c1"># ...
</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">"beta1"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s">"beta2"</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"lr"</span><span class="p">])</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="nc">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"step_size"</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"gamma"</span><span class="p">])</span>

		<span class="c1"># ...
</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"batch_size"</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"num_worker"</span><span class="p">])</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s">"batch_size"</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

		<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">"num_epoch"</span><span class="p">]),</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

				<span class="c1"># rest of main..
</span></code></pre></div></div>

<h2 id="configure-sweep">Configure Sweep</h2>

<p>Next, I wrote a YAML file to specify the hyperparameters that I want to sweep over, the way how to explore the hyperparameter space, and the set of possible values for each hyperparameter.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="na">program</span><span class="pi">:</span> <span class="s">train.py</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">grid</span>
<span class="na">metric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">Test/Loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">lr</span><span class="pi">:</span> 
    <span class="na">values</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="m">0.1</span>
    <span class="pi">-</span> <span class="m">0.01</span>
    <span class="pi">-</span> <span class="m">0.001</span>
    <span class="pi">-</span> <span class="m">0.0001</span>
  <span class="na">batch_size</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="m">32</span>
    <span class="pi">-</span> <span class="m">64</span>
    <span class="pi">-</span> <span class="m">128</span>
    <span class="pi">-</span> <span class="m">256</span>
</code></pre></div></div>

<p>In particular, I’m telling W&amp;B:</p>

<ul>
  <li>that the training routine is defined at the file <em>train.py</em></li>
  <li>to use “grid” method (examine all possible combinations of hyperparameter values) for exploration</li>
  <li>to “minimize” the metric “Test/Loss”. <strong>One important note is that you should log a quantity with name “Test/Loss” somewhere in the training routine.</strong> And I did it previously in the function <em>run_test</em>.</li>
  <li>variables <em>“lr”</em> and <em>“batch_size”</em> are the ones that can be modified for each different run. And each of them has a set of possible values (e.g. learning rate (<em>lr</em>) can be one of 0.1, 0.01, 0.001, 0.0001)</li>
</ul>

<h2 id="initialize--run-sweep">Initialize &amp; Run Sweep</h2>

<p>After setting up a sweep configuration in <em>*.yaml</em> file, run the following command to initialize the sweep:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="syntax"><code>wandb sweep <span class="k">*</span>.yaml
</code></pre></div></div>

<p>W&amp;B will automatically set up things for you and give you a <em>sweep ID</em> which specifies that exact sweep to be run. <strong>Copy that and use it in the next step.</strong></p>

<h2 id="launch-agents">Launch Agent(s)</h2>

<p>What makes sweep feature more powerful is that multiple machines or processes can contribute to the same sweep. This means, you can concurrently test different combinations of hyperparameters across your devices or processes in a single machine. As long as you share the same sweep ID, W&amp;B will distribute tasks to agents participating in the sweep and all you have to do is just waiting for the result.</p>

<p>The below image shows the result of each trial in the middle of sweeping (i.e. it was still on going). As soon as you see the plot at the bottom, you will immediately notice that the cases using large learning rate tend to give higher test losses while batch size seems to have no effect on test loss.</p>

<center>
    <img class="img-fluid" src="/assets/post-images/Introduction_to_W&amp;B/fig4.png" />
</center>

<h1 id="summary">Summary</h1>

<p>In this post I briefly introduced only a few of, yet fundamental functionalities of W&amp;B. These include:</p>

<ul>
  <li>How to set up W&amp;B for your project</li>
  <li>Logging metrics and various kinds of data using <em>wandb.log</em></li>
  <li>Analyzing model with <em>wandb.watch</em></li>
  <li>Checking statistics of system usages collected by W&amp;B</li>
  <li>Sweep - a powerful way of tuning hyperparameters &amp; visualizing results to gain insights</li>
</ul>

<p>I hope this post was helpful &amp; comprehensible to you, would be glad if you can benefit from it, work in more productive way. Furthermore, as I mentioned in the introduction, be sure to check out the official documentation for more details and things that were not covered here.</p>]]></content><author><name>Seungwoo Yoo</name></author><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>